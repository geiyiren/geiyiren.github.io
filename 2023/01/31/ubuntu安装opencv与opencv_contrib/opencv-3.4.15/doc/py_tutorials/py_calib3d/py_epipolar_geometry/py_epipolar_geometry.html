<h1 id="Epipolar-Geometry-tutorial-py-epipolar-geometry"><a href="#Epipolar-Geometry-tutorial-py-epipolar-geometry" class="headerlink" title="Epipolar Geometry {#tutorial_py_epipolar_geometry}"></a>Epipolar Geometry {#tutorial_py_epipolar_geometry}</h1><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>In this section,</p>
<ul>
<li>We will learn about the basics of multiview geometry</li>
<li>We will see what is epipole, epipolar lines, epipolar constraint etc.</li>
</ul>
<h2 id="Basic-Concepts"><a href="#Basic-Concepts" class="headerlink" title="Basic Concepts"></a>Basic Concepts</h2><p>When we take an image using pin-hole camera, we loose an important information, ie depth of the<br>image. Or how far is each point in the image from the camera because it is a 3D-to-2D conversion. So<br>it is an important question whether we can find the depth information using these cameras. And the<br>answer is to use more than one camera. Our eyes works in similar way where we use two cameras (two<br>eyes) which is called stereo vision. So let’s see what OpenCV provides in this field.</p>
<p>(<em>Learning OpenCV</em> by Gary Bradsky has a lot of information in this field.)</p>
<p>Before going to depth images, let’s first understand some basic concepts in multiview geometry. In<br>this section we will deal with epipolar geometry. See the image below which shows a basic setup with<br>two cameras taking the image of same scene.</p>
<p><img src="/images/epipolar.jpg" alt="image"></p>
<p>If we are using only the left camera, we can’t find the 3D point corresponding to the point \f$x\f$ in<br>image because every point on the line \f$OX\f$ projects to the same point on the image plane. But<br>consider the right image also. Now different points on the line \f$OX\f$ projects to different points<br>(\f$x’\f$) in right plane. So with these two images, we can triangulate the correct 3D point. This is<br>the whole idea.</p>
<p>The projection of the different points on \f$OX\f$ form a line on right plane (line \f$l’\f$). We call it<br><strong>epiline</strong> corresponding to the point \f$x\f$. It means, to find the point \f$x\f$ on the right image,<br>search along this epiline. It should be somewhere on this line (Think of it this way, to find the<br>matching point in other image, you need not search the whole image, just search along the epiline.<br>So it provides better performance and accuracy). This is called <strong>Epipolar Constraint</strong>. Similarly<br>all points will have its corresponding epilines in the other image. The plane \f$XOO’\f$ is called<br><strong>Epipolar Plane</strong>.</p>
<p>\f$O\f$ and \f$O’\f$ are the camera centers. From the setup given above, you can see that projection of<br>right camera \f$O’\f$ is seen on the left image at the point, \f$e\f$. It is called the <strong>epipole</strong>. Epipole<br>is the point of intersection of line through camera centers and the image planes. Similarly \f$e’\f$ is<br>the epipole of the left camera. In some cases, you won’t be able to locate the epipole in the image,<br>they may be outside the image (which means, one camera doesn’t see the other).</p>
<p>All the epilines pass through its epipole. So to find the location of epipole, we can find many<br>epilines and find their intersection point.</p>
<p>So in this session, we focus on finding epipolar lines and epipoles. But to find them, we need two<br>more ingredients, <strong>Fundamental Matrix (F)</strong> and <strong>Essential Matrix (E)</strong>. Essential Matrix contains<br>the information about translation and rotation, which describe the location of the second camera<br>relative to the first in global coordinates. See the image below (Image courtesy: Learning OpenCV by<br>Gary Bradsky):</p>
<p><img src="/images/essential_matrix.jpg" alt="image"></p>
<p>But we prefer measurements to be done in pixel coordinates, right? Fundamental Matrix contains the<br>same information as Essential Matrix in addition to the information about the intrinsics of both<br>cameras so that we can relate the two cameras in pixel coordinates. (If we are using rectified<br>images and normalize the point by dividing by the focal lengths, \f$F&#x3D;E\f$). In simple words,<br>Fundamental Matrix F, maps a point in one image to a line (epiline) in the other image. This is<br>calculated from matching points from both the images. A minimum of 8 such points are required to<br>find the fundamental matrix (while using 8-point algorithm). More points are preferred and use<br>RANSAC to get a more robust result.</p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>So first we need to find as many possible matches between two images to find the fundamental matrix.<br>For this, we use SIFT descriptors with FLANN based matcher and ratio test.<br>@code{.py}<br>import numpy as np<br>import cv2 as cv<br>from matplotlib import pyplot as plt</p>
<p>img1 &#x3D; cv.imread(‘myleft.jpg’,0)  #queryimage # left image<br>img2 &#x3D; cv.imread(‘myright.jpg’,0) #trainimage # right image</p>
<p>sift &#x3D; cv.SIFT_create()</p>
<h1 id="find-the-keypoints-and-descriptors-with-SIFT"><a href="#find-the-keypoints-and-descriptors-with-SIFT" class="headerlink" title="find the keypoints and descriptors with SIFT"></a>find the keypoints and descriptors with SIFT</h1><p>kp1, des1 &#x3D; sift.detectAndCompute(img1,None)<br>kp2, des2 &#x3D; sift.detectAndCompute(img2,None)</p>
<h1 id="FLANN-parameters"><a href="#FLANN-parameters" class="headerlink" title="FLANN parameters"></a>FLANN parameters</h1><p>FLANN_INDEX_KDTREE &#x3D; 1<br>index_params &#x3D; dict(algorithm &#x3D; FLANN_INDEX_KDTREE, trees &#x3D; 5)<br>search_params &#x3D; dict(checks&#x3D;50)</p>
<p>flann &#x3D; cv.FlannBasedMatcher(index_params,search_params)<br>matches &#x3D; flann.knnMatch(des1,des2,k&#x3D;2)</p>
<p>pts1 &#x3D; []<br>pts2 &#x3D; []</p>
<h1 id="ratio-test-as-per-Lowe’s-paper"><a href="#ratio-test-as-per-Lowe’s-paper" class="headerlink" title="ratio test as per Lowe’s paper"></a>ratio test as per Lowe’s paper</h1><p>for i,(m,n) in enumerate(matches):<br>    if m.distance &lt; 0.8*n.distance:<br>        pts2.append(kp2[m.trainIdx].pt)<br>        pts1.append(kp1[m.queryIdx].pt)<br>@endcode<br>Now we have the list of best matches from both the images. Let’s find the Fundamental Matrix.<br>@code{.py}<br>pts1 &#x3D; np.int32(pts1)<br>pts2 &#x3D; np.int32(pts2)<br>F, mask &#x3D; cv.findFundamentalMat(pts1,pts2,cv.FM_LMEDS)</p>
<h1 id="We-select-only-inlier-points"><a href="#We-select-only-inlier-points" class="headerlink" title="We select only inlier points"></a>We select only inlier points</h1><p>pts1 &#x3D; pts1[mask.ravel()&#x3D;&#x3D;1]<br>pts2 &#x3D; pts2[mask.ravel()&#x3D;&#x3D;1]<br>@endcode<br>Next we find the epilines. Epilines corresponding to the points in first image is drawn on second<br>image. So mentioning of correct images are important here. We get an array of lines. So we define a<br>new function to draw these lines on the images.<br>@code{.py}<br>def drawlines(img1,img2,lines,pts1,pts2):<br>    ‘’’ img1 - image on which we draw the epilines for the points in img2<br>        lines - corresponding epilines ‘’’<br>    r,c &#x3D; img1.shape<br>    img1 &#x3D; cv.cvtColor(img1,cv.COLOR_GRAY2BGR)<br>    img2 &#x3D; cv.cvtColor(img2,cv.COLOR_GRAY2BGR)<br>    for r,pt1,pt2 in zip(lines,pts1,pts2):<br>        color &#x3D; tuple(np.random.randint(0,255,3).tolist())<br>        x0,y0 &#x3D; map(int, [0, -r[2]&#x2F;r[1] ])<br>        x1,y1 &#x3D; map(int, [c, -(r[2]+r[0]*c)&#x2F;r[1] ])<br>        img1 &#x3D; cv.line(img1, (x0,y0), (x1,y1), color,1)<br>        img1 &#x3D; cv.circle(img1,tuple(pt1),5,color,-1)<br>        img2 &#x3D; cv.circle(img2,tuple(pt2),5,color,-1)<br>    return img1,img2<br>@endcode<br>Now we find the epilines in both the images and draw them.<br>@code{.py}</p>
<h1 id="Find-epilines-corresponding-to-points-in-right-image-second-image-and"><a href="#Find-epilines-corresponding-to-points-in-right-image-second-image-and" class="headerlink" title="Find epilines corresponding to points in right image (second image) and"></a>Find epilines corresponding to points in right image (second image) and</h1><h1 id="drawing-its-lines-on-left-image"><a href="#drawing-its-lines-on-left-image" class="headerlink" title="drawing its lines on left image"></a>drawing its lines on left image</h1><p>lines1 &#x3D; cv.computeCorrespondEpilines(pts2.reshape(-1,1,2), 2,F)<br>lines1 &#x3D; lines1.reshape(-1,3)<br>img5,img6 &#x3D; drawlines(img1,img2,lines1,pts1,pts2)</p>
<h1 id="Find-epilines-corresponding-to-points-in-left-image-first-image-and"><a href="#Find-epilines-corresponding-to-points-in-left-image-first-image-and" class="headerlink" title="Find epilines corresponding to points in left image (first image) and"></a>Find epilines corresponding to points in left image (first image) and</h1><h1 id="drawing-its-lines-on-right-image"><a href="#drawing-its-lines-on-right-image" class="headerlink" title="drawing its lines on right image"></a>drawing its lines on right image</h1><p>lines2 &#x3D; cv.computeCorrespondEpilines(pts1.reshape(-1,1,2), 1,F)<br>lines2 &#x3D; lines2.reshape(-1,3)<br>img3,img4 &#x3D; drawlines(img2,img1,lines2,pts2,pts1)</p>
<p>plt.subplot(121),plt.imshow(img5)<br>plt.subplot(122),plt.imshow(img3)<br>plt.show()<br>@endcode<br>Below is the result we get:</p>
<p><img src="/images/epiresult.jpg" alt="image"></p>
<p>You can see in the left image that all epilines are converging at a point outside the image at right<br>side. That meeting point is the epipole.</p>
<p>For better results, images with good resolution and many non-planar points should be used.</p>
<h2 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h2><h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2><p>-#  One important topic is the forward movement of camera. Then epipoles will be seen at the same<br>    locations in both with epilines emerging from a fixed point. <a href="http://answers.opencv.org/question/17912/location-of-epipole/">See this<br>    discussion</a>.<br>2.  Fundamental Matrix estimation is sensitive to quality of matches, outliers etc. It becomes worse<br>    when all selected matches lie on the same plane. <a href="http://answers.opencv.org/question/18125/epilines-not-correct/">Check this<br>    discussion</a>.</p>
