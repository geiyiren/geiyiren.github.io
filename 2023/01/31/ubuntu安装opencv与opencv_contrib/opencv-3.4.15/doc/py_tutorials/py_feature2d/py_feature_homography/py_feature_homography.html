<h1 id="Feature-Matching-Homography-to-find-Objects-tutorial-py-feature-homography"><a href="#Feature-Matching-Homography-to-find-Objects-tutorial-py-feature-homography" class="headerlink" title="Feature Matching + Homography to find Objects {#tutorial_py_feature_homography}"></a>Feature Matching + Homography to find Objects {#tutorial_py_feature_homography}</h1><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>In this chapter,<br>    -   We will mix up the feature matching and findHomography from calib3d module to find known<br>        objects in a complex image.</p>
<h2 id="Basics"><a href="#Basics" class="headerlink" title="Basics"></a>Basics</h2><p>So what we did in last session? We used a queryImage, found some feature points in it, we took<br>another trainImage, found the features in that image too and we found the best matches among them.<br>In short, we found locations of some parts of an object in another cluttered image. This information<br>is sufficient to find the object exactly on the trainImage.</p>
<p>For that, we can use a function from calib3d module, ie <strong>cv.findHomography()</strong>. If we pass the set<br>of points from both the images, it will find the perspective transformation of that object. Then we<br>can use <strong>cv.perspectiveTransform()</strong> to find the object. It needs atleast four correct points to<br>find the transformation.</p>
<p>We have seen that there can be some possible errors while matching which may affect the result. To<br>solve this problem, algorithm uses RANSAC or LEAST_MEDIAN (which can be decided by the flags). So<br>good matches which provide correct estimation are called inliers and remaining are called outliers.<br><strong>cv.findHomography()</strong> returns a mask which specifies the inlier and outlier points.</p>
<p>So let’s do it !!!</p>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>First, as usual, let’s find SIFT features in images and apply the ratio test to find the best<br>matches.<br>@code{.py}<br>import numpy as np<br>import cv2 as cv<br>from matplotlib import pyplot as plt</p>
<p>MIN_MATCH_COUNT &#x3D; 10</p>
<p>img1 &#x3D; cv.imread(‘box.png’,0)          # queryImage<br>img2 &#x3D; cv.imread(‘box_in_scene.png’,0) # trainImage</p>
<h1 id="Initiate-SIFT-detector"><a href="#Initiate-SIFT-detector" class="headerlink" title="Initiate SIFT detector"></a>Initiate SIFT detector</h1><p>sift &#x3D; cv.SIFT_create()</p>
<h1 id="find-the-keypoints-and-descriptors-with-SIFT"><a href="#find-the-keypoints-and-descriptors-with-SIFT" class="headerlink" title="find the keypoints and descriptors with SIFT"></a>find the keypoints and descriptors with SIFT</h1><p>kp1, des1 &#x3D; sift.detectAndCompute(img1,None)<br>kp2, des2 &#x3D; sift.detectAndCompute(img2,None)</p>
<p>FLANN_INDEX_KDTREE &#x3D; 1<br>index_params &#x3D; dict(algorithm &#x3D; FLANN_INDEX_KDTREE, trees &#x3D; 5)<br>search_params &#x3D; dict(checks &#x3D; 50)</p>
<p>flann &#x3D; cv.FlannBasedMatcher(index_params, search_params)</p>
<p>matches &#x3D; flann.knnMatch(des1,des2,k&#x3D;2)</p>
<h1 id="store-all-the-good-matches-as-per-Lowe’s-ratio-test"><a href="#store-all-the-good-matches-as-per-Lowe’s-ratio-test" class="headerlink" title="store all the good matches as per Lowe’s ratio test."></a>store all the good matches as per Lowe’s ratio test.</h1><p>good &#x3D; []<br>for m,n in matches:<br>    if m.distance &lt; 0.7*n.distance:<br>        good.append(m)<br>@endcode<br>Now we set a condition that atleast 10 matches (defined by MIN_MATCH_COUNT) are to be there to<br>find the object. Otherwise simply show a message saying not enough matches are present.</p>
<p>If enough matches are found, we extract the locations of matched keypoints in both the images. They<br>are passed to find the perspective transformation. Once we get this 3x3 transformation matrix, we use<br>it to transform the corners of queryImage to corresponding points in trainImage. Then we draw it.<br>@code{.py}<br>if len(good)&gt;MIN_MATCH_COUNT:<br>    src_pts &#x3D; np.float32([ kp1[m.queryIdx].pt for m in good ]).reshape(-1,1,2)<br>    dst_pts &#x3D; np.float32([ kp2[m.trainIdx].pt for m in good ]).reshape(-1,1,2)</p>
<pre><code>M, mask = cv.findHomography(src_pts, dst_pts, cv.RANSAC,5.0)
matchesMask = mask.ravel().tolist()

h,w,d = img1.shape
pts = np.float32([ [0,0],[0,h-1],[w-1,h-1],[w-1,0] ]).reshape(-1,1,2)
dst = cv.perspectiveTransform(pts,M)

img2 = cv.polylines(img2,[np.int32(dst)],True,255,3, cv.LINE_AA)
</code></pre>
<p>else:<br>    print( “Not enough matches are found - {}&#x2F;{}”.format(len(good), MIN_MATCH_COUNT) )<br>    matchesMask &#x3D; None<br>@endcode<br>Finally we draw our inliers (if successfully found the object) or matching keypoints (if failed).<br>@code{.py}<br>draw_params &#x3D; dict(matchColor &#x3D; (0,255,0), # draw matches in green color<br>                   singlePointColor &#x3D; None,<br>                   matchesMask &#x3D; matchesMask, # draw only inliers<br>                   flags &#x3D; 2)</p>
<p>img3 &#x3D; cv.drawMatches(img1,kp1,img2,kp2,good,None,**draw_params)</p>
<p>plt.imshow(img3, ‘gray’),plt.show()<br>@endcode<br>See the result below. Object is marked in white color in cluttered image:</p>
<p><img src="/images/homography_findobj.jpg" alt="image"></p>
<h2 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h2><h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2>