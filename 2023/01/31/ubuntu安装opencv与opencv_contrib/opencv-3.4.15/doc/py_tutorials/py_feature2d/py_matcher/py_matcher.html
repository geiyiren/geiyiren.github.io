<h1 id="Feature-Matching-tutorial-py-matcher"><a href="#Feature-Matching-tutorial-py-matcher" class="headerlink" title="Feature Matching {#tutorial_py_matcher}"></a>Feature Matching {#tutorial_py_matcher}</h1><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>In this chapter<br>    -   We will see how to match features in one image with others.<br>    -   We will use the Brute-Force matcher and FLANN Matcher in OpenCV</p>
<h2 id="Basics-of-Brute-Force-Matcher"><a href="#Basics-of-Brute-Force-Matcher" class="headerlink" title="Basics of Brute-Force Matcher"></a>Basics of Brute-Force Matcher</h2><p>Brute-Force matcher is simple. It takes the descriptor of one feature in first set and is matched<br>with all other features in second set using some distance calculation. And the closest one is<br>returned.</p>
<p>For BF matcher, first we have to create the BFMatcher object using <strong>cv.BFMatcher()</strong>. It takes two<br>optional params. First one is normType. It specifies the distance measurement to be used. By<br>default, it is cv.NORM_L2. It is good for SIFT, SURF etc (cv.NORM_L1 is also there). For binary<br>string based descriptors like ORB, BRIEF, BRISK etc, cv.NORM_HAMMING should be used, which used<br>Hamming distance as measurement. If ORB is using WTA_K &#x3D;&#x3D; 3 or 4, cv.NORM_HAMMING2 should be<br>used.</p>
<p>Second param is boolean variable, crossCheck which is false by default. If it is true, Matcher<br>returns only those matches with value (i,j) such that i-th descriptor in set A has j-th descriptor<br>in set B as the best match and vice-versa. That is, the two features in both sets should match each<br>other. It provides consistent result, and is a good alternative to ratio test proposed by D.Lowe in<br>SIFT paper.</p>
<p>Once it is created, two important methods are <em>BFMatcher.match()</em> and <em>BFMatcher.knnMatch()</em>. First<br>one returns the best match. Second method returns k best matches where k is specified by the user.<br>It may be useful when we need to do additional work on that.</p>
<p>Like we used cv.drawKeypoints() to draw keypoints, <strong>cv.drawMatches()</strong> helps us to draw the<br>matches. It stacks two images horizontally and draw lines from first image to second image showing<br>best matches. There is also <strong>cv.drawMatchesKnn</strong> which draws all the k best matches. If k&#x3D;2, it<br>will draw two match-lines for each keypoint. So we have to pass a mask if we want to selectively<br>draw it.</p>
<p>Let’s see one example for each of SIFT and ORB (Both use different distance measurements).</p>
<h3 id="Brute-Force-Matching-with-ORB-Descriptors"><a href="#Brute-Force-Matching-with-ORB-Descriptors" class="headerlink" title="Brute-Force Matching with ORB Descriptors"></a>Brute-Force Matching with ORB Descriptors</h3><p>Here, we will see a simple example on how to match features between two images. In this case, I have<br>a queryImage and a trainImage. We will try to find the queryImage in trainImage using feature<br>matching. ( The images are &#x2F;samples&#x2F;data&#x2F;box.png and &#x2F;samples&#x2F;data&#x2F;box_in_scene.png)</p>
<p>We are using ORB descriptors to match features. So let’s start with loading images, finding<br>descriptors etc.<br>@code{.py}<br>import numpy as np<br>import cv2 as cv<br>import matplotlib.pyplot as plt</p>
<p>img1 &#x3D; cv.imread(‘box.png’,cv.IMREAD_GRAYSCALE)          # queryImage<br>img2 &#x3D; cv.imread(‘box_in_scene.png’,cv.IMREAD_GRAYSCALE) # trainImage</p>
<h1 id="Initiate-ORB-detector"><a href="#Initiate-ORB-detector" class="headerlink" title="Initiate ORB detector"></a>Initiate ORB detector</h1><p>orb &#x3D; cv.ORB_create()</p>
<h1 id="find-the-keypoints-and-descriptors-with-ORB"><a href="#find-the-keypoints-and-descriptors-with-ORB" class="headerlink" title="find the keypoints and descriptors with ORB"></a>find the keypoints and descriptors with ORB</h1><p>kp1, des1 &#x3D; orb.detectAndCompute(img1,None)<br>kp2, des2 &#x3D; orb.detectAndCompute(img2,None)<br>@endcode<br>Next we create a BFMatcher object with distance measurement cv.NORM_HAMMING (since we are using<br>ORB) and crossCheck is switched on for better results. Then we use Matcher.match() method to get the<br>best matches in two images. We sort them in ascending order of their distances so that best matches<br>(with low distance) come to front. Then we draw only first 10 matches (Just for sake of visibility.<br>You can increase it as you like)<br>@code{.py}</p>
<h1 id="create-BFMatcher-object"><a href="#create-BFMatcher-object" class="headerlink" title="create BFMatcher object"></a>create BFMatcher object</h1><p>bf &#x3D; cv.BFMatcher(cv.NORM_HAMMING, crossCheck&#x3D;True)</p>
<h1 id="Match-descriptors"><a href="#Match-descriptors" class="headerlink" title="Match descriptors."></a>Match descriptors.</h1><p>matches &#x3D; bf.match(des1,des2)</p>
<h1 id="Sort-them-in-the-order-of-their-distance"><a href="#Sort-them-in-the-order-of-their-distance" class="headerlink" title="Sort them in the order of their distance."></a>Sort them in the order of their distance.</h1><p>matches &#x3D; sorted(matches, key &#x3D; lambda x:x.distance)</p>
<h1 id="Draw-first-10-matches"><a href="#Draw-first-10-matches" class="headerlink" title="Draw first 10 matches."></a>Draw first 10 matches.</h1><p>img3 &#x3D; cv.drawMatches(img1,kp1,img2,kp2,matches[:10],None,flags&#x3D;cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)</p>
<p>plt.imshow(img3),plt.show()<br>@endcode<br>Below is the result I got:</p>
<p><img src="/images/matcher_result1.jpg" alt="image"></p>
<h3 id="What-is-this-Matcher-Object"><a href="#What-is-this-Matcher-Object" class="headerlink" title="What is this Matcher Object?"></a>What is this Matcher Object?</h3><p>The result of matches &#x3D; bf.match(des1,des2) line is a list of DMatch objects. This DMatch object has<br>following attributes:</p>
<ul>
<li>DMatch.distance - Distance between descriptors. The lower, the better it is.</li>
<li>DMatch.trainIdx - Index of the descriptor in train descriptors</li>
<li>DMatch.queryIdx - Index of the descriptor in query descriptors</li>
<li>DMatch.imgIdx - Index of the train image.</li>
</ul>
<h3 id="Brute-Force-Matching-with-SIFT-Descriptors-and-Ratio-Test"><a href="#Brute-Force-Matching-with-SIFT-Descriptors-and-Ratio-Test" class="headerlink" title="Brute-Force Matching with SIFT Descriptors and Ratio Test"></a>Brute-Force Matching with SIFT Descriptors and Ratio Test</h3><p>This time, we will use BFMatcher.knnMatch() to get k best matches. In this example, we will take k&#x3D;2<br>so that we can apply ratio test explained by D.Lowe in his paper.<br>@code{.py}<br>import numpy as np<br>import cv2 as cv<br>import matplotlib.pyplot as plt</p>
<p>img1 &#x3D; cv.imread(‘box.png’,cv.IMREAD_GRAYSCALE)          # queryImage<br>img2 &#x3D; cv.imread(‘box_in_scene.png’,cv.IMREAD_GRAYSCALE) # trainImage</p>
<h1 id="Initiate-SIFT-detector"><a href="#Initiate-SIFT-detector" class="headerlink" title="Initiate SIFT detector"></a>Initiate SIFT detector</h1><p>sift &#x3D; cv.SIFT_create()</p>
<h1 id="find-the-keypoints-and-descriptors-with-SIFT"><a href="#find-the-keypoints-and-descriptors-with-SIFT" class="headerlink" title="find the keypoints and descriptors with SIFT"></a>find the keypoints and descriptors with SIFT</h1><p>kp1, des1 &#x3D; sift.detectAndCompute(img1,None)<br>kp2, des2 &#x3D; sift.detectAndCompute(img2,None)</p>
<h1 id="BFMatcher-with-default-params"><a href="#BFMatcher-with-default-params" class="headerlink" title="BFMatcher with default params"></a>BFMatcher with default params</h1><p>bf &#x3D; cv.BFMatcher()<br>matches &#x3D; bf.knnMatch(des1,des2,k&#x3D;2)</p>
<h1 id="Apply-ratio-test"><a href="#Apply-ratio-test" class="headerlink" title="Apply ratio test"></a>Apply ratio test</h1><p>good &#x3D; []<br>for m,n in matches:<br>    if m.distance &lt; 0.75*n.distance:<br>        good.append([m])</p>
<h1 id="cv-drawMatchesKnn-expects-list-of-lists-as-matches"><a href="#cv-drawMatchesKnn-expects-list-of-lists-as-matches" class="headerlink" title="cv.drawMatchesKnn expects list of lists as matches."></a>cv.drawMatchesKnn expects list of lists as matches.</h1><p>img3 &#x3D; cv.drawMatchesKnn(img1,kp1,img2,kp2,good,None,flags&#x3D;cv.DrawMatchesFlags_NOT_DRAW_SINGLE_POINTS)</p>
<p>plt.imshow(img3),plt.show()<br>@endcode<br>See the result below:</p>
<p><img src="/images/matcher_result2.jpg" alt="image"></p>
<h2 id="FLANN-based-Matcher"><a href="#FLANN-based-Matcher" class="headerlink" title="FLANN based Matcher"></a>FLANN based Matcher</h2><p>FLANN stands for Fast Library for Approximate Nearest Neighbors. It contains a collection of<br>algorithms optimized for fast nearest neighbor search in large datasets and for high dimensional<br>features. It works faster than BFMatcher for large datasets. We will see the second example<br>with FLANN based matcher.</p>
<p>For FLANN based matcher, we need to pass two dictionaries which specifies the algorithm to be used,<br>its related parameters etc. First one is IndexParams. For various algorithms, the information to be<br>passed is explained in FLANN docs. As a summary, for algorithms like SIFT, SURF etc. you can pass<br>following:<br>@code{.py}<br>FLANN_INDEX_KDTREE &#x3D; 1<br>index_params &#x3D; dict(algorithm &#x3D; FLANN_INDEX_KDTREE, trees &#x3D; 5)<br>@endcode<br>While using ORB, you can pass the following. The commented values are recommended as per the docs,<br>but it didn’t provide required results in some cases. Other values worked fine.:<br>@code{.py}<br>FLANN_INDEX_LSH &#x3D; 6<br>index_params&#x3D; dict(algorithm &#x3D; FLANN_INDEX_LSH,<br>                   table_number &#x3D; 6, # 12<br>                   key_size &#x3D; 12,     # 20<br>                   multi_probe_level &#x3D; 1) #2<br>@endcode<br>Second dictionary is the SearchParams. It specifies the number of times the trees in the index<br>should be recursively traversed. Higher values gives better precision, but also takes more time. If<br>you want to change the value, pass search_params &#x3D; dict(checks&#x3D;100).</p>
<p>With this information, we are good to go.<br>@code{.py}<br>import numpy as np<br>import cv2 as cv<br>import matplotlib.pyplot as plt</p>
<p>img1 &#x3D; cv.imread(‘box.png’,cv.IMREAD_GRAYSCALE)          # queryImage<br>img2 &#x3D; cv.imread(‘box_in_scene.png’,cv.IMREAD_GRAYSCALE) # trainImage</p>
<h1 id="Initiate-SIFT-detector-1"><a href="#Initiate-SIFT-detector-1" class="headerlink" title="Initiate SIFT detector"></a>Initiate SIFT detector</h1><p>sift &#x3D; cv.SIFT_create()</p>
<h1 id="find-the-keypoints-and-descriptors-with-SIFT-1"><a href="#find-the-keypoints-and-descriptors-with-SIFT-1" class="headerlink" title="find the keypoints and descriptors with SIFT"></a>find the keypoints and descriptors with SIFT</h1><p>kp1, des1 &#x3D; sift.detectAndCompute(img1,None)<br>kp2, des2 &#x3D; sift.detectAndCompute(img2,None)</p>
<h1 id="FLANN-parameters"><a href="#FLANN-parameters" class="headerlink" title="FLANN parameters"></a>FLANN parameters</h1><p>FLANN_INDEX_KDTREE &#x3D; 1<br>index_params &#x3D; dict(algorithm &#x3D; FLANN_INDEX_KDTREE, trees &#x3D; 5)<br>search_params &#x3D; dict(checks&#x3D;50)   # or pass empty dictionary</p>
<p>flann &#x3D; cv.FlannBasedMatcher(index_params,search_params)</p>
<p>matches &#x3D; flann.knnMatch(des1,des2,k&#x3D;2)</p>
<h1 id="Need-to-draw-only-good-matches-so-create-a-mask"><a href="#Need-to-draw-only-good-matches-so-create-a-mask" class="headerlink" title="Need to draw only good matches, so create a mask"></a>Need to draw only good matches, so create a mask</h1><p>matchesMask &#x3D; [[0,0] for i in range(len(matches))]</p>
<h1 id="ratio-test-as-per-Lowe’s-paper"><a href="#ratio-test-as-per-Lowe’s-paper" class="headerlink" title="ratio test as per Lowe’s paper"></a>ratio test as per Lowe’s paper</h1><p>for i,(m,n) in enumerate(matches):<br>    if m.distance &lt; 0.7*n.distance:<br>        matchesMask[i]&#x3D;[1,0]</p>
<p>draw_params &#x3D; dict(matchColor &#x3D; (0,255,0),<br>                   singlePointColor &#x3D; (255,0,0),<br>                   matchesMask &#x3D; matchesMask,<br>                   flags &#x3D; cv.DrawMatchesFlags_DEFAULT)</p>
<p>img3 &#x3D; cv.drawMatchesKnn(img1,kp1,img2,kp2,matches,None,**draw_params)</p>
<p>plt.imshow(img3,),plt.show()<br>@endcode<br>See the result below:</p>
<p><img src="/images/matcher_flann.jpg" alt="image"></p>
<h2 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h2><h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2>