<h1 id="Introduction-to-SIFT-Scale-Invariant-Feature-Transform-tutorial-py-sift-intro"><a href="#Introduction-to-SIFT-Scale-Invariant-Feature-Transform-tutorial-py-sift-intro" class="headerlink" title="Introduction to SIFT (Scale-Invariant Feature Transform) {#tutorial_py_sift_intro}"></a>Introduction to SIFT (Scale-Invariant Feature Transform) {#tutorial_py_sift_intro}</h1><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>In this chapter,<br>    -   We will learn about the concepts of SIFT algorithm<br>    -   We will learn to find SIFT Keypoints and Descriptors.</p>
<h2 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h2><p>In last couple of chapters, we saw some corner detectors like Harris etc. They are<br>rotation-invariant, which means, even if the image is rotated, we can find the same corners. It is<br>obvious because corners remain corners in rotated image also. But what about scaling? A corner may<br>not be a corner if the image is scaled. For example, check a simple image below. A corner in a small<br>image within a small window is flat when it is zoomed in the same window. So Harris corner is not<br>scale invariant.</p>
<p><img src="/images/sift_scale_invariant.jpg" alt="image"></p>
<p>In 2004, <strong>D.Lowe</strong>, University of British Columbia, came up with a new algorithm, Scale<br>Invariant Feature Transform (SIFT) in his paper, <strong>Distinctive Image Features from Scale-Invariant<br>Keypoints</strong>, which extract keypoints and compute its descriptors. <em>(This paper is easy to understand<br>and considered to be best material available on SIFT. This explanation is just a short summary of<br>this paper)</em>.</p>
<p>There are mainly four steps involved in SIFT algorithm. We will see them one-by-one.</p>
<h3 id="1-Scale-space-Extrema-Detection"><a href="#1-Scale-space-Extrema-Detection" class="headerlink" title="1. Scale-space Extrema Detection"></a>1. Scale-space Extrema Detection</h3><p>From the image above, it is obvious that we can’t use the same window to detect keypoints with<br>different scale. It is OK with small corner. But to detect larger corners we need larger windows.<br>For this, scale-space filtering is used. In it, Laplacian of Gaussian is found for the image with<br>various \f$\sigma\f$ values. LoG acts as a blob detector which detects blobs in various sizes due to<br>change in \f$\sigma\f$. In short, \f$\sigma\f$ acts as a scaling parameter. For eg, in the above image,<br>gaussian kernel with low \f$\sigma\f$ gives high value for small corner while gaussian kernel with high<br>\f$\sigma\f$ fits well for larger corner. So, we can find the local maxima across the scale and space<br>which gives us a list of \f$(x,y,\sigma)\f$ values which means there is a potential keypoint at (x,y) at<br>\f$\sigma\f$ scale.</p>
<p>But this LoG is a little costly, so SIFT algorithm uses Difference of Gaussians which is an<br>approximation of LoG. Difference of Gaussian is obtained as the difference of Gaussian blurring of<br>an image with two different \f$\sigma\f$, let it be \f$\sigma\f$ and \f$k\sigma\f$. This process is done for<br>different octaves of the image in Gaussian Pyramid. It is represented in below image:</p>
<p><img src="/images/sift_dog.jpg" alt="image"></p>
<p>Once this DoG are found, images are searched for local extrema over scale and space. For eg, one<br>pixel in an image is compared with its 8 neighbours as well as 9 pixels in next scale and 9 pixels<br>in previous scales. If it is a local extrema, it is a potential keypoint. It basically means that<br>keypoint is best represented in that scale. It is shown in below image:</p>
<p><img src="/images/sift_local_extrema.jpg" alt="image"></p>
<p>Regarding different parameters, the paper gives some empirical data which can be summarized as,<br>number of octaves &#x3D; 4, number of scale levels &#x3D; 5, initial \f$\sigma&#x3D;1.6\f$, \f$k&#x3D;\sqrt{2}\f$ etc as optimal<br>values.</p>
<h3 id="2-Keypoint-Localization"><a href="#2-Keypoint-Localization" class="headerlink" title="2. Keypoint Localization"></a>2. Keypoint Localization</h3><p>Once potential keypoints locations are found, they have to be refined to get more accurate results.<br>They used Taylor series expansion of scale space to get more accurate location of extrema, and if<br>the intensity at this extrema is less than a threshold value (0.03 as per the paper), it is<br>rejected. This threshold is called <strong>contrastThreshold</strong> in OpenCV</p>
<p>DoG has higher response for edges, so edges also need to be removed. For this, a concept similar to<br>Harris corner detector is used. They used a 2x2 Hessian matrix (H) to compute the principal<br>curvature. We know from Harris corner detector that for edges, one eigen value is larger than the<br>other. So here they used a simple function,</p>
<p>If this ratio is greater than a threshold, called <strong>edgeThreshold</strong> in OpenCV, that keypoint is<br>discarded. It is given as 10 in paper.</p>
<p>So it eliminates any low-contrast keypoints and edge keypoints and what remains is strong interest<br>points.</p>
<h3 id="3-Orientation-Assignment"><a href="#3-Orientation-Assignment" class="headerlink" title="3. Orientation Assignment"></a>3. Orientation Assignment</h3><p>Now an orientation is assigned to each keypoint to achieve invariance to image rotation. A<br>neighbourhood is taken around the keypoint location depending on the scale, and the gradient<br>magnitude and direction is calculated in that region. An orientation histogram with 36 bins covering<br>360 degrees is created (It is weighted by gradient magnitude and gaussian-weighted circular window<br>with \f$\sigma\f$ equal to 1.5 times the scale of keypoint). The highest peak in the histogram is taken<br>and any peak above 80% of it is also considered to calculate the orientation. It creates keypoints<br>with same location and scale, but different directions. It contribute to stability of matching.</p>
<h3 id="4-Keypoint-Descriptor"><a href="#4-Keypoint-Descriptor" class="headerlink" title="4. Keypoint Descriptor"></a>4. Keypoint Descriptor</h3><p>Now keypoint descriptor is created. A 16x16 neighbourhood around the keypoint is taken. It is<br>divided into 16 sub-blocks of 4x4 size. For each sub-block, 8 bin orientation histogram is created.<br>So a total of 128 bin values are available. It is represented as a vector to form keypoint<br>descriptor. In addition to this, several measures are taken to achieve robustness against<br>illumination changes, rotation etc.</p>
<h3 id="5-Keypoint-Matching"><a href="#5-Keypoint-Matching" class="headerlink" title="5. Keypoint Matching"></a>5. Keypoint Matching</h3><p>Keypoints between two images are matched by identifying their nearest neighbours. But in some cases,<br>the second closest-match may be very near to the first. It may happen due to noise or some other<br>reasons. In that case, ratio of closest-distance to second-closest distance is taken. If it is<br>greater than 0.8, they are rejected. It eliminates around 90% of false matches while discards only<br>5% correct matches, as per the paper.</p>
<p>This is a summary of SIFT algorithm. For more details and understanding, reading the original<br>paper is highly recommended.</p>
<h2 id="SIFT-in-OpenCV"><a href="#SIFT-in-OpenCV" class="headerlink" title="SIFT in OpenCV"></a>SIFT in OpenCV</h2><p>Now let’s see SIFT functionalities available in OpenCV. Note that these were previously only<br>available in <a href="https://github.com/opencv/opencv_contrib">the opencv contrib repo</a>, but the patent<br>expired in the year 2020. So they are now included in the main repo. Let’s start with keypoint<br>detection and draw them. First we have to construct a SIFT object. We can pass different<br>parameters to it which are optional and they are well explained in docs.<br>@code{.py}<br>import numpy as np<br>import cv2 as cv</p>
<p>img &#x3D; cv.imread(‘home.jpg’)<br>gray&#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</p>
<p>sift &#x3D; cv.SIFT_create()<br>kp &#x3D; sift.detect(gray,None)</p>
<p>img&#x3D;cv.drawKeypoints(gray,kp,img)</p>
<p>cv.imwrite(‘sift_keypoints.jpg’,img)<br>@endcode<br><strong>sift.detect()</strong> function finds the keypoint in the images. You can pass a mask if you want to<br>search only a part of image. Each keypoint is a special structure which has many attributes like its<br>(x,y) coordinates, size of the meaningful neighbourhood, angle which specifies its orientation,<br>response that specifies strength of keypoints etc.</p>
<p>OpenCV also provides <strong>cv.drawKeyPoints()</strong> function which draws the small circles on the locations<br>of keypoints. If you pass a flag, <strong>cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS</strong> to it, it will<br>draw a circle with size of keypoint and it will even show its orientation. See below example.<br>@code{.py}<br>img&#x3D;cv.drawKeypoints(gray,kp,img,flags&#x3D;cv.DRAW_MATCHES_FLAGS_DRAW_RICH_KEYPOINTS)<br>cv.imwrite(‘sift_keypoints.jpg’,img)<br>@endcode<br>See the two results below:</p>
<p><img src="/images/sift_keypoints.jpg" alt="image"></p>
<p>Now to calculate the descriptor, OpenCV provides two methods.</p>
<p>-#  Since you already found keypoints, you can call <strong>sift.compute()</strong> which computes the<br>    descriptors from the keypoints we have found. Eg: kp,des &#x3D; sift.compute(gray,kp)<br>2.  If you didn’t find keypoints, directly find keypoints and descriptors in a single step with the<br>    function, <strong>sift.detectAndCompute()</strong>.</p>
<p>We will see the second method:<br>@code{.py}<br>sift &#x3D; cv.SIFT_create()<br>kp, des &#x3D; sift.detectAndCompute(gray,None)<br>@endcode<br>Here kp will be a list of keypoints and des is a numpy array of shape<br>\f$\text{(Number of Keypoints)} \times 128\f$.</p>
<p>So we got keypoints, descriptors etc. Now we want to see how to match keypoints in different images.<br>That we will learn in coming chapters.</p>
<h2 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h2><h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2>