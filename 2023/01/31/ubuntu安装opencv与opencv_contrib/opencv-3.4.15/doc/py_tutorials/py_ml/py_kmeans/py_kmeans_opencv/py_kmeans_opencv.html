<h1 id="K-Means-Clustering-in-OpenCV-tutorial-py-kmeans-opencv"><a href="#K-Means-Clustering-in-OpenCV-tutorial-py-kmeans-opencv" class="headerlink" title="K-Means Clustering in OpenCV {#tutorial_py_kmeans_opencv}"></a>K-Means Clustering in OpenCV {#tutorial_py_kmeans_opencv}</h1><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><ul>
<li>Learn to use <strong>cv.kmeans()</strong> function in OpenCV for data clustering</li>
</ul>
<h2 id="Understanding-Parameters"><a href="#Understanding-Parameters" class="headerlink" title="Understanding Parameters"></a>Understanding Parameters</h2><h3 id="Input-parameters"><a href="#Input-parameters" class="headerlink" title="Input parameters"></a>Input parameters</h3><p>-#  <strong>samples</strong> : It should be of <strong>np.float32</strong> data type, and each feature should be put in a<br>    single column.<br>-#  <strong>nclusters(K)</strong> : Number of clusters required at end<br>-#  <strong>criteria</strong> : It is the iteration termination criteria. When this criteria is satisfied, algorithm iteration stops. Actually, it should be a tuple of 3 parameters. They are `( type, max_iter, epsilon )`:<br>        -#  type of termination criteria. It has 3 flags as below:<br>            - <strong>cv.TERM_CRITERIA_EPS</strong> - stop the algorithm iteration if specified accuracy, <em>epsilon</em>, is reached.<br>            - <strong>cv.TERM_CRITERIA_MAX_ITER</strong> - stop the algorithm after the specified number of iterations, <em>max_iter</em>.<br>            - <strong>cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER</strong> - stop the iteration when any of the above condition is met.<br>        -#  max_iter - An integer specifying maximum number of iterations.<br>        -#  epsilon - Required accuracy</p>
<p>-#  <strong>attempts</strong> : Flag to specify the number of times the algorithm is executed using different<br>    initial labellings. The algorithm returns the labels that yield the best compactness. This<br>    compactness is returned as output.<br>-#  <strong>flags</strong> : This flag is used to specify how initial centers are taken. Normally two flags are<br>    used for this : <strong>cv.KMEANS_PP_CENTERS</strong> and <strong>cv.KMEANS_RANDOM_CENTERS</strong>.</p>
<h3 id="Output-parameters"><a href="#Output-parameters" class="headerlink" title="Output parameters"></a>Output parameters</h3><p>-#  <strong>compactness</strong> : It is the sum of squared distance from each point to their corresponding<br>    centers.<br>-#  <strong>labels</strong> : This is the label array (same as ‘code’ in previous article) where each element<br>    marked ‘0’, ‘1’…..<br>-#  <strong>centers</strong> : This is array of centers of clusters.</p>
<p>Now we will see how to apply K-Means algorithm with three examples.</p>
<ol>
<li>Data with Only One Feature</li>
</ol>
<hr>
<p>Consider, you have a set of data with only one feature, ie one-dimensional. For eg, we can take our<br>t-shirt problem where you use only height of people to decide the size of t-shirt.</p>
<p>So we start by creating data and plot it in Matplotlib<br>@code{.py}<br>import numpy as np<br>import cv2 as cv<br>from matplotlib import pyplot as plt</p>
<p>x &#x3D; np.random.randint(25,100,25)<br>y &#x3D; np.random.randint(175,255,25)<br>z &#x3D; np.hstack((x,y))<br>z &#x3D; z.reshape((50,1))<br>z &#x3D; np.float32(z)<br>plt.hist(z,256,[0,256]),plt.show()<br>@endcode<br>So we have ‘z’ which is an array of size 50, and values ranging from 0 to 255. I have reshaped ‘z’<br>to a column vector. It will be more useful when more than one features are present. Then I made data<br>of np.float32 type.</p>
<p>We get following image :</p>
<p><img src="/images/oc_1d_testdata.png" alt="image"></p>
<p>Now we apply the KMeans function. Before that we need to specify the criteria. My criteria is such<br>that, whenever 10 iterations of algorithm is ran, or an accuracy of epsilon &#x3D; 1.0 is reached, stop<br>the algorithm and return the answer.<br>@code{.py}</p>
<h1 id="Define-criteria-x3D-type-max-iter-x3D-10-epsilon-x3D-1-0"><a href="#Define-criteria-x3D-type-max-iter-x3D-10-epsilon-x3D-1-0" class="headerlink" title="Define criteria &#x3D; ( type, max_iter &#x3D; 10 , epsilon &#x3D; 1.0 )"></a>Define criteria &#x3D; ( type, max_iter &#x3D; 10 , epsilon &#x3D; 1.0 )</h1><p>criteria &#x3D; (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)</p>
<h1 id="Set-flags-Just-to-avoid-line-break-in-the-code"><a href="#Set-flags-Just-to-avoid-line-break-in-the-code" class="headerlink" title="Set flags (Just to avoid line break in the code)"></a>Set flags (Just to avoid line break in the code)</h1><p>flags &#x3D; cv.KMEANS_RANDOM_CENTERS</p>
<h1 id="Apply-KMeans"><a href="#Apply-KMeans" class="headerlink" title="Apply KMeans"></a>Apply KMeans</h1><p>compactness,labels,centers &#x3D; cv.kmeans(z,2,None,criteria,10,flags)<br>@endcode<br>This gives us the compactness, labels and centers. In this case, I got centers as 60 and 207. Labels<br>will have the same size as that of test data where each data will be labelled as ‘0’,’1’,’2’ etc.<br>depending on their centroids. Now we split the data to different clusters depending on their labels.<br>@code{.py}<br>A &#x3D; z[labels&#x3D;&#x3D;0]<br>B &#x3D; z[labels&#x3D;&#x3D;1]<br>@endcode<br>Now we plot A in Red color and B in Blue color and their centroids in Yellow color.<br>@code{.py}</p>
<h1 id="Now-plot-‘A’-in-red-‘B’-in-blue-‘centers’-in-yellow"><a href="#Now-plot-‘A’-in-red-‘B’-in-blue-‘centers’-in-yellow" class="headerlink" title="Now plot ‘A’ in red, ‘B’ in blue, ‘centers’ in yellow"></a>Now plot ‘A’ in red, ‘B’ in blue, ‘centers’ in yellow</h1><p>plt.hist(A,256,[0,256],color &#x3D; ‘r’)<br>plt.hist(B,256,[0,256],color &#x3D; ‘b’)<br>plt.hist(centers,32,[0,256],color &#x3D; ‘y’)<br>plt.show()<br>@endcode<br>Below is the output we got:</p>
<p><img src="/images/oc_1d_clustered.png" alt="image"></p>
<ol start="2">
<li>Data with Multiple Features</li>
</ol>
<hr>
<p>In previous example, we took only height for t-shirt problem. Here, we will take both height and<br>weight, ie two features.</p>
<p>Remember, in previous case, we made our data to a single column vector. Each feature is arranged in<br>a column, while each row corresponds to an input test sample.</p>
<p>For example, in this case, we set a test data of size 50x2, which are heights and weights of 50<br>people. First column corresponds to height of all the 50 people and second column corresponds to<br>their weights. First row contains two elements where first one is the height of first person and<br>second one his weight. Similarly remaining rows corresponds to heights and weights of other people.<br>Check image below:</p>
<p><img src="/images/oc_feature_representation.jpg" alt="image"></p>
<p>Now I am directly moving to the code:<br>@code{.py}<br>import numpy as np<br>import cv2 as cv<br>from matplotlib import pyplot as plt</p>
<p>X &#x3D; np.random.randint(25,50,(25,2))<br>Y &#x3D; np.random.randint(60,85,(25,2))<br>Z &#x3D; np.vstack((X,Y))</p>
<h1 id="convert-to-np-float32"><a href="#convert-to-np-float32" class="headerlink" title="convert to np.float32"></a>convert to np.float32</h1><p>Z &#x3D; np.float32(Z)</p>
<h1 id="define-criteria-and-apply-kmeans"><a href="#define-criteria-and-apply-kmeans" class="headerlink" title="define criteria and apply kmeans()"></a>define criteria and apply kmeans()</h1><p>criteria &#x3D; (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)<br>ret,label,center&#x3D;cv.kmeans(Z,2,None,criteria,10,cv.KMEANS_RANDOM_CENTERS)</p>
<h1 id="Now-separate-the-data-Note-the-flatten"><a href="#Now-separate-the-data-Note-the-flatten" class="headerlink" title="Now separate the data, Note the flatten()"></a>Now separate the data, Note the flatten()</h1><p>A &#x3D; Z[label.ravel()&#x3D;&#x3D;0]<br>B &#x3D; Z[label.ravel()&#x3D;&#x3D;1]</p>
<h1 id="Plot-the-data"><a href="#Plot-the-data" class="headerlink" title="Plot the data"></a>Plot the data</h1><p>plt.scatter(A[:,0],A[:,1])<br>plt.scatter(B[:,0],B[:,1],c &#x3D; ‘r’)<br>plt.scatter(center[:,0],center[:,1],s &#x3D; 80,c &#x3D; ‘y’, marker &#x3D; ‘s’)<br>plt.xlabel(‘Height’),plt.ylabel(‘Weight’)<br>plt.show()<br>@endcode<br>Below is the output we get:</p>
<p><img src="/images/oc_2d_clustered.jpg" alt="image"></p>
<ol start="3">
<li>Color Quantization</li>
</ol>
<hr>
<p>Color Quantization is the process of reducing number of colors in an image. One reason to do so is<br>to reduce the memory. Sometimes, some devices may have limitation such that it can produce only<br>limited number of colors. In those cases also, color quantization is performed. Here we use k-means<br>clustering for color quantization.</p>
<p>There is nothing new to be explained here. There are 3 features, say, R,G,B. So we need to reshape<br>the image to an array of Mx3 size (M is number of pixels in image). And after the clustering, we<br>apply centroid values (it is also R,G,B) to all pixels, such that resulting image will have<br>specified number of colors. And again we need to reshape it back to the shape of original image.<br>Below is the code:<br>@code{.py}<br>import numpy as np<br>import cv2 as cv</p>
<p>img &#x3D; cv.imread(‘home.jpg’)<br>Z &#x3D; img.reshape((-1,3))</p>
<h1 id="convert-to-np-float32-1"><a href="#convert-to-np-float32-1" class="headerlink" title="convert to np.float32"></a>convert to np.float32</h1><p>Z &#x3D; np.float32(Z)</p>
<h1 id="define-criteria-number-of-clusters-K-and-apply-kmeans"><a href="#define-criteria-number-of-clusters-K-and-apply-kmeans" class="headerlink" title="define criteria, number of clusters(K) and apply kmeans()"></a>define criteria, number of clusters(K) and apply kmeans()</h1><p>criteria &#x3D; (cv.TERM_CRITERIA_EPS + cv.TERM_CRITERIA_MAX_ITER, 10, 1.0)<br>K &#x3D; 8<br>ret,label,center&#x3D;cv.kmeans(Z,K,None,criteria,10,cv.KMEANS_RANDOM_CENTERS)</p>
<h1 id="Now-convert-back-into-uint8-and-make-original-image"><a href="#Now-convert-back-into-uint8-and-make-original-image" class="headerlink" title="Now convert back into uint8, and make original image"></a>Now convert back into uint8, and make original image</h1><p>center &#x3D; np.uint8(center)<br>res &#x3D; center[label.flatten()]<br>res2 &#x3D; res.reshape((img.shape))</p>
<p>cv.imshow(‘res2’,res2)<br>cv.waitKey(0)<br>cv.destroyAllWindows()<br>@endcode<br>See the result below for K&#x3D;8:</p>
<p><img src="/images/oc_color_quantization.jpg" alt="image"></p>
<h2 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h2><h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2>