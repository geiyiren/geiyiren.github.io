<h1 id="OCR-of-Hand-written-Data-using-kNN-tutorial-py-knn-opencv"><a href="#OCR-of-Hand-written-Data-using-kNN-tutorial-py-knn-opencv" class="headerlink" title="OCR of Hand-written Data using kNN {#tutorial_py_knn_opencv}"></a>OCR of Hand-written Data using kNN {#tutorial_py_knn_opencv}</h1><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>In this chapter:<br>    -   We will use our knowledge on kNN to build a basic OCR (Optical Character Recognition) application.<br>    -   We will try our application on Digits and Alphabets data that comes with OpenCV.</p>
<h2 id="OCR-of-Hand-written-Digits"><a href="#OCR-of-Hand-written-Digits" class="headerlink" title="OCR of Hand-written Digits"></a>OCR of Hand-written Digits</h2><p>Our goal is to build an application which can read handwritten digits. For this we need some<br>training data and some test data. OpenCV comes with an image digits.png (in the folder<br>opencv&#x2F;samples&#x2F;data&#x2F;) which has 5000 handwritten digits (500 for each digit). Each digit is<br>a 20x20 image. So our first step is to split this image into 5000 different digit images. Then for each digit (20x20 image),<br>we flatten it into a single row with 400 pixels. That is our feature set, i.e. intensity values of all<br>pixels. It is the simplest feature set we can create. We use the first 250 samples of each digit as<br>training data, and the other 250 samples as test data. So let’s prepare them first.<br>@code{.py}<br>import numpy as np<br>import cv2 as cv</p>
<p>img &#x3D; cv.imread(‘digits.png’)<br>gray &#x3D; cv.cvtColor(img,cv.COLOR_BGR2GRAY)</p>
<h1 id="Now-we-split-the-image-to-5000-cells-each-20x20-size"><a href="#Now-we-split-the-image-to-5000-cells-each-20x20-size" class="headerlink" title="Now we split the image to 5000 cells, each 20x20 size"></a>Now we split the image to 5000 cells, each 20x20 size</h1><p>cells &#x3D; [np.hsplit(row,100) for row in np.vsplit(gray,50)]</p>
<h1 id="Make-it-into-a-Numpy-array-its-size-will-be-50-100-20-20"><a href="#Make-it-into-a-Numpy-array-its-size-will-be-50-100-20-20" class="headerlink" title="Make it into a Numpy array: its size will be (50,100,20,20)"></a>Make it into a Numpy array: its size will be (50,100,20,20)</h1><p>x &#x3D; np.array(cells)</p>
<h1 id="Now-we-prepare-the-training-data-and-test-data"><a href="#Now-we-prepare-the-training-data-and-test-data" class="headerlink" title="Now we prepare the training data and test data"></a>Now we prepare the training data and test data</h1><p>train &#x3D; x[:,:50].reshape(-1,400).astype(np.float32) # Size &#x3D; (2500,400)<br>test &#x3D; x[:,50:100].reshape(-1,400).astype(np.float32) # Size &#x3D; (2500,400)</p>
<h1 id="Create-labels-for-train-and-test-data"><a href="#Create-labels-for-train-and-test-data" class="headerlink" title="Create labels for train and test data"></a>Create labels for train and test data</h1><p>k &#x3D; np.arange(10)<br>train_labels &#x3D; np.repeat(k,250)[:,np.newaxis]<br>test_labels &#x3D; train_labels.copy()</p>
<h1 id="Initiate-kNN-train-it-on-the-training-data-then-test-it-with-the-test-data-with-k-x3D-1"><a href="#Initiate-kNN-train-it-on-the-training-data-then-test-it-with-the-test-data-with-k-x3D-1" class="headerlink" title="Initiate kNN, train it on the training data, then test it with the test data with k&#x3D;1"></a>Initiate kNN, train it on the training data, then test it with the test data with k&#x3D;1</h1><p>knn &#x3D; cv.ml.KNearest_create()<br>knn.train(train, cv.ml.ROW_SAMPLE, train_labels)<br>ret,result,neighbours,dist &#x3D; knn.findNearest(test,k&#x3D;5)</p>
<h1 id="Now-we-check-the-accuracy-of-classification"><a href="#Now-we-check-the-accuracy-of-classification" class="headerlink" title="Now we check the accuracy of classification"></a>Now we check the accuracy of classification</h1><h1 id="For-that-compare-the-result-with-test-labels-and-check-which-are-wrong"><a href="#For-that-compare-the-result-with-test-labels-and-check-which-are-wrong" class="headerlink" title="For that, compare the result with test_labels and check which are wrong"></a>For that, compare the result with test_labels and check which are wrong</h1><p>matches &#x3D; result&#x3D;&#x3D;test_labels<br>correct &#x3D; np.count_nonzero(matches)<br>accuracy &#x3D; correct*100.0&#x2F;result.size<br>print( accuracy )<br>@endcode<br>So our basic OCR app is ready. This particular example gave me an accuracy of 91%. One option to<br>improve accuracy is to add more data for training, especially for the digits where we had more errors.</p>
<p>Instead of finding<br>this training data every time I start the application, I better save it, so that the next time, I can directly<br>read this data from a file and start classification. This can be done with the help of some Numpy<br>functions like np.savetxt, np.savez, np.load, etc. Please check the NumPy docs for more details.<br>@code{.py}</p>
<h1 id="Save-the-data"><a href="#Save-the-data" class="headerlink" title="Save the data"></a>Save the data</h1><p>np.savez(‘knn_data.npz’,train&#x3D;train, train_labels&#x3D;train_labels)</p>
<h1 id="Now-load-the-data"><a href="#Now-load-the-data" class="headerlink" title="Now load the data"></a>Now load the data</h1><p>with np.load(‘knn_data.npz’) as data:<br>    print( data.files )<br>    train &#x3D; data[‘train’]<br>    train_labels &#x3D; data[‘train_labels’]<br>@endcode<br>In my system, it takes around 4.4 MB of memory. Since we are using intensity values (uint8 data) as<br>features, it would be better to convert the data to np.uint8 first and then save it. It takes only<br>1.1 MB in this case. Then while loading, you can convert back into float32.</p>
<h2 id="OCR-of-the-English-Alphabet"><a href="#OCR-of-the-English-Alphabet" class="headerlink" title="OCR of the English Alphabet"></a>OCR of the English Alphabet</h2><p>Next we will do the same for the English alphabet, but there is a slight change in data and feature<br>set. Here, instead of images, OpenCV comes with a data file, letter-recognition.data in<br>opencv&#x2F;samples&#x2F;cpp&#x2F; folder. If you open it, you will see 20000 lines which may, on first sight, look<br>like garbage. Actually, in each row, the first column is a letter which is our label. The next 16 numbers<br>following it are the different features. These features are obtained from the <a href="http://archive.ics.uci.edu/ml/">UCI Machine Learning<br>Repository</a>. You can find the details of these features in <a href="http://archive.ics.uci.edu/ml/datasets/Letter+Recognition">this<br>page</a>.</p>
<p>There are 20000 samples available, so we take the first 10000 as training samples and the remaining<br>10000 as test samples. We should change the letters to ascii characters because we can’t work with<br>letters directly.<br>@code{.py}<br>import cv2 as cv<br>import numpy as np</p>
<h1 id="Load-the-data-and-convert-the-letters-to-numbers"><a href="#Load-the-data-and-convert-the-letters-to-numbers" class="headerlink" title="Load the data and convert the letters to numbers"></a>Load the data and convert the letters to numbers</h1><p>data&#x3D; np.loadtxt(‘letter-recognition.data’, dtype&#x3D; ‘float32’, delimiter &#x3D; ‘,’,<br>                    converters&#x3D; {0: lambda ch: ord(ch)-ord(‘A’)})</p>
<h1 id="Split-the-dataset-in-two-with-10000-samples-each-for-training-and-test-sets"><a href="#Split-the-dataset-in-two-with-10000-samples-each-for-training-and-test-sets" class="headerlink" title="Split the dataset in two, with 10000 samples each for training and test sets"></a>Split the dataset in two, with 10000 samples each for training and test sets</h1><p>train, test &#x3D; np.vsplit(data,2)</p>
<h1 id="Split-trainData-and-testData-into-features-and-responses"><a href="#Split-trainData-and-testData-into-features-and-responses" class="headerlink" title="Split trainData and testData into features and responses"></a>Split trainData and testData into features and responses</h1><p>responses, trainData &#x3D; np.hsplit(train,[1])<br>labels, testData &#x3D; np.hsplit(test,[1])</p>
<h1 id="Initiate-the-kNN-classify-measure-accuracy"><a href="#Initiate-the-kNN-classify-measure-accuracy" class="headerlink" title="Initiate the kNN, classify, measure accuracy"></a>Initiate the kNN, classify, measure accuracy</h1><p>knn &#x3D; cv.ml.KNearest_create()<br>knn.train(trainData, cv.ml.ROW_SAMPLE, responses)<br>ret, result, neighbours, dist &#x3D; knn.findNearest(testData, k&#x3D;5)</p>
<p>correct &#x3D; np.count_nonzero(result &#x3D;&#x3D; labels)<br>accuracy &#x3D; correct*100.0&#x2F;10000<br>print( accuracy )<br>@endcode<br>It gives me an accuracy of 93.22%. Again, if you want to increase accuracy, you can iteratively add<br>more data.</p>
<h2 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h2><ol>
<li><a href="https://en.wikipedia.org/wiki/Optical_character_recognition">Wikipedia article on Optical character recognition</a></li>
</ol>
<h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2><ol>
<li>Here we used k&#x3D;5. What happens if you try other values of k? Can you find a value that maximizes accuracy (minimizes the number of errors)?</li>
</ol>
