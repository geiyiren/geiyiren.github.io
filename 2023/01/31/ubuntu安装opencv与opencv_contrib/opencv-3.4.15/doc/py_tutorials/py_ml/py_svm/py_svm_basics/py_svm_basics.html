<h1 id="Understanding-SVM-tutorial-py-svm-basics"><a href="#Understanding-SVM-tutorial-py-svm-basics" class="headerlink" title="Understanding SVM {#tutorial_py_svm_basics}"></a>Understanding SVM {#tutorial_py_svm_basics}</h1><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>In this chapter<br>    -   We will see an intuitive understanding of SVM</p>
<h2 id="Theory"><a href="#Theory" class="headerlink" title="Theory"></a>Theory</h2><h3 id="Linearly-Separable-Data"><a href="#Linearly-Separable-Data" class="headerlink" title="Linearly Separable Data"></a>Linearly Separable Data</h3><p>Consider the image below which has two types of data, red and blue. In kNN, for a test data, we used<br>to measure its distance to all the training samples and take the one with minimum distance. It takes<br>plenty of time to measure all the distances and plenty of memory to store all the training-samples.<br>But considering the data given in image, should we need that much?</p>
<p><img src="/images/svm_basics1.png" alt="image"></p>
<p>Consider another idea. We find a line, \f$f(x)&#x3D;ax_1+bx_2+c\f$ which divides both the data to two<br>regions. When we get a new test_data \f$X\f$, just substitute it in \f$f(x)\f$. If \f$f(X) &gt; 0\f$, it belongs<br>to blue group, else it belongs to red group. We can call this line as <strong>Decision Boundary</strong>. It is<br>very simple and memory-efficient. Such data which can be divided into two with a straight line (or<br>hyperplanes in higher dimensions) is called <strong>Linear Separable</strong>.</p>
<p>So in above image, you can see plenty of such lines are possible. Which one we will take? Very<br>intuitively we can say that the line should be passing as far as possible from all the points. Why?<br>Because there can be noise in the incoming data. This data should not affect the classification<br>accuracy. So taking a farthest line will provide more immunity against noise. So what SVM does is to<br>find a straight line (or hyperplane) with largest minimum distance to the training samples. See the<br>bold line in below image passing through the center.</p>
<p><img src="/images/svm_basics2.png" alt="image"></p>
<p>So to find this Decision Boundary, you need training data. Do you need all? NO. Just the ones which<br>are close to the opposite group are sufficient. In our image, they are the one blue filled circle<br>and two red filled squares. We can call them <strong>Support Vectors</strong> and the lines passing through them<br>are called <strong>Support Planes</strong>. They are adequate for finding our decision boundary. We need not<br>worry about all the data. It helps in data reduction.</p>
<p>What happened is, first two hyperplanes are found which best represents the data. For eg, blue data<br>is represented by \f$w^Tx+b_0 &gt; 1\f$ while red data is represented by \f$w^Tx+b_0 &lt; -1\f$ where \f$w\f$ is<br><strong>weight vector</strong> ( \f$w&#x3D;[w_1, w_2,…, w_n]\f$) and \f$x\f$ is the feature vector<br>(\f$x &#x3D; [x_1,x_2,…, x_n]\f$). \f$b_0\f$ is the <strong>bias</strong>. Weight vector decides the orientation of decision<br>boundary while bias point decides its location. Now decision boundary is defined to be midway<br>between these hyperplanes, so expressed as \f$w^Tx+b_0 &#x3D; 0\f$. The minimum distance from support vector<br>to the decision boundary is given by, \f$distance_{support , vectors}&#x3D;\frac{1}{||w||}\f$. Margin is<br>twice this distance, and we need to maximize this margin. i.e. we need to minimize a new function<br>\f$L(w, b_0)\f$ with some constraints which can expressed below:</p>
<p>\f[\min_{w, b_0} L(w, b_0) &#x3D; \frac{1}{2}||w||^2 ; \text{subject to} ; t_i(w^Tx+b_0) \geq 1 ; \forall i\f]</p>
<p>where \f$t_i\f$ is the label of each class, \f$t_i \in [-1,1]\f$.</p>
<h3 id="Non-Linearly-Separable-Data"><a href="#Non-Linearly-Separable-Data" class="headerlink" title="Non-Linearly Separable Data"></a>Non-Linearly Separable Data</h3><p>Consider some data which can’t be divided into two with a straight line. For example, consider an<br>one-dimensional data where ‘X’ is at -3 &amp; +3 and ‘O’ is at -1 &amp; +1. Clearly it is not linearly<br>separable. But there are methods to solve these kinds of problems. If we can map this data set with<br>a function, \f$f(x) &#x3D; x^2\f$, we get ‘X’ at 9 and ‘O’ at 1 which are linear separable.</p>
<p>Otherwise we can convert this one-dimensional to two-dimensional data. We can use \f$f(x)&#x3D;(x,x^2)\f$<br>function to map this data. Then ‘X’ becomes (-3,9) and (3,9) while ‘O’ becomes (-1,1) and (1,1).<br>This is also linear separable. In short, chance is more for a non-linear separable data in<br>lower-dimensional space to become linear separable in higher-dimensional space.</p>
<p>In general, it is possible to map points in a d-dimensional space to some D-dimensional space<br>\f$(D&gt;d)\f$ to check the possibility of linear separability. There is an idea which helps to compute the<br>dot product in the high-dimensional (kernel) space by performing computations in the low-dimensional<br>input (feature) space. We can illustrate with following example.</p>
<p>Consider two points in two-dimensional space, \f$p&#x3D;(p_1,p_2)\f$ and \f$q&#x3D;(q_1,q_2)\f$. Let \f$\phi\f$ be a<br>mapping function which maps a two-dimensional point to three-dimensional space as follows:</p>
<p>\f[\phi (p) &#x3D; (p_{1}^2,p_{2}^2,\sqrt{2} p_1 p_2)<br>\phi (q) &#x3D; (q_{1}^2,q_{2}^2,\sqrt{2} q_1 q_2)\f]</p>
<p>Let us define a kernel function \f$K(p,q)\f$ which does a dot product between two points, shown below:</p>
<p>\f[<br>\begin{aligned}<br>K(p,q)  &#x3D; \phi(p).\phi(q) &amp;&#x3D; \phi(p)^T \phi(q) \<br>                          &amp;&#x3D; (p_{1}^2,p_{2}^2,\sqrt{2} p_1 p_2).(q_{1}^2,q_{2}^2,\sqrt{2} q_1 q_2) \<br>                          &amp;&#x3D; p_{1}^2 q_{1}^2 + p_{2}^2 q_{2}^2 + 2 p_1 q_1 p_2 q_2 \<br>                          &amp;&#x3D; (p_1 q_1 + p_2 q_2)^2 \<br>          \phi(p).\phi(q) &amp;&#x3D; (p.q)^2<br>\end{aligned}<br>\f]</p>
<p>It means, a dot product in three-dimensional space can be achieved using squared dot product in<br>two-dimensional space. This can be applied to higher dimensional space. So we can calculate higher<br>dimensional features from lower dimensions itself. Once we map them, we get a higher dimensional<br>space.</p>
<p>In addition to all these concepts, there comes the problem of misclassification. So just finding<br>decision boundary with maximum margin is not sufficient. We need to consider the problem of<br>misclassification errors also. Sometimes, it may be possible to find a decision boundary with less<br>margin, but with reduced misclassification. Anyway we need to modify our model such that it should<br>find decision boundary with maximum margin, but with less misclassification. The minimization<br>criteria is modified as:</p>
<p>\f[min ; ||w||^2 + C(distance ; of ; misclassified ; samples ; to ; their ; correct ; regions)\f]</p>
<p>Below image shows this concept. For each sample of the training data a new parameter \f$\xi_i\f$ is<br>defined. It is the distance from its corresponding training sample to their correct decision region.<br>For those who are not misclassified, they fall on their corresponding support planes, so their<br>distance is zero.</p>
<p><img src="/images/svm_basics3.png" alt="image"></p>
<p>So the new optimization problem is :</p>
<p>\f[\min_{w, b_{0}} L(w,b_0) &#x3D; ||w||^{2} + C \sum_{i} {\xi_{i}} \text{ subject to } y_{i}(w^{T} x_{i} + b_{0}) \geq 1 - \xi_{i} \text{ and } \xi_{i} \geq 0 \text{ } \forall i\f]</p>
<p>How should the parameter C be chosen? It is obvious that the answer to this question depends on how<br>the training data is distributed. Although there is no general answer, it is useful to take into<br>account these rules:</p>
<ul>
<li>Large values of C give solutions with less misclassification errors but a smaller margin.<br>Consider that in this case it is expensive to make misclassification errors. Since the aim of<br>the optimization is to minimize the argument, few misclassifications errors are allowed.</li>
<li>Small values of C give solutions with bigger margin and more classification errors. In this<br>case the minimization does not consider that much the term of the sum so it focuses more on<br>finding a hyperplane with big margin.</li>
</ul>
<h2 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h2><p>-#  <a href="http://www.nptel.ac.in/courses/106108057/26">NPTEL notes on Statistical Pattern Recognition, Chapters<br>    25-29</a>.</p>
<h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2>