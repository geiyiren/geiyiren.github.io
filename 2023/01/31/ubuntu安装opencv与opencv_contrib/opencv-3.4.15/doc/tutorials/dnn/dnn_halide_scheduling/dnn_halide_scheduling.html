<h1 id="How-to-schedule-your-network-for-Halide-backend-tutorial-dnn-halide-scheduling"><a href="#How-to-schedule-your-network-for-Halide-backend-tutorial-dnn-halide-scheduling" class="headerlink" title="How to schedule your network for Halide backend {#tutorial_dnn_halide_scheduling}"></a>How to schedule your network for Halide backend {#tutorial_dnn_halide_scheduling}</h1><p>@prev_tutorial{tutorial_dnn_halide}<br>@next_tutorial{tutorial_dnn_android}</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>Halide code is the same for every device we use. But for achieving the satisfied<br>efficiency we should schedule computations properly. In this tutorial we describe<br>the ways to schedule your networks using Halide backend in OpenCV deep learning module.</p>
<p>For better understanding of Halide scheduling you might want to read tutorials @ <a href="http://halide-lang.org/tutorials">http://halide-lang.org/tutorials</a>.</p>
<p>If it’s your first meeting with Halide in OpenCV, we recommend to start from @ref tutorial_dnn_halide.</p>
<h2 id="Configuration-files"><a href="#Configuration-files" class="headerlink" title="Configuration files"></a>Configuration files</h2><p>You can schedule computations of Halide pipeline by writing textual configuration files.<br>It means that you can easily vectorize, parallelize and manage loops order of<br>layers computation. Pass path to file with scheduling directives for specific<br>device into <code>cv::dnn::Net::setHalideScheduler</code> before the first <code>cv::dnn::Net::forward</code> call.</p>
<p>Scheduling configuration files represented as YAML files where each node is a<br>scheduled function or a scheduling directive.<br>@code<br>relu1:<br>  reorder: [x, c, y]<br>  split: { y: 2, c: 8 }<br>  parallel: [yo, co]<br>  unroll: yi<br>  vectorize: { x: 4 }<br>conv1_constant_exterior:<br>  compute_at: { relu1: yi }<br>@endcode</p>
<p>Considered use variables <code>n</code> for batch dimension, <code>c</code> for channels,<br><code>y</code> for rows and <code>x</code> for columns. For variables after split are used names<br>with the same prefix but <code>o</code> and <code>i</code> suffixes for outer and inner variables<br>correspondingly. In example, for variable <code>x</code> in range <code>[0, 10)</code> directive<br><code>split: { x: 2 }</code> gives new ones <code>xo</code> in range <code>[0, 5)</code> and <code>xi</code> in range <code>[0, 2)</code>.<br>Variable name <code>x</code> is no longer available in the same scheduling node.</p>
<p>You can find scheduling examples at <a href="https://github.com/opencv/opencv_extra/tree/master/testdata/dnn">opencv_extra&#x2F;testdata&#x2F;dnn</a><br>and use it for schedule your networks.</p>
<h2 id="Layers-fusing"><a href="#Layers-fusing" class="headerlink" title="Layers fusing"></a>Layers fusing</h2><p>Thanks to layers fusing we can schedule only the top layers of fused sets.<br>Because for every output value we use the fused formula.<br>In example, if you have three layers Convolution + Scale + ReLU one by one,<br>@code<br>conv(x, y, c, n) &#x3D; sum(…) + bias(c);<br>scale(x, y, c, n) &#x3D; conv(x, y, c, n) * weights(c);<br>relu(x, y, c, n) &#x3D; max(scale(x, y, c, n), 0);<br>@endcode</p>
<p>fused function is something like<br>@code<br>relu(x, y, c, n) &#x3D; max((sum(…) + bias(c)) * weights(c), 0);<br>@endcode</p>
<p>So only function called <code>relu</code> require scheduling.</p>
<h2 id="Scheduling-patterns"><a href="#Scheduling-patterns" class="headerlink" title="Scheduling patterns"></a>Scheduling patterns</h2><p>Sometimes networks built using blocked structure that means some layer are<br>identical or quite similar. If you want to apply the same scheduling for<br>different layers accurate to tiling or vectorization factors, define scheduling<br>patterns in section <code>patterns</code> at the beginning of scheduling file.<br>Also, your patters may use some parametric variables.<br>@code</p>
<h1 id="At-the-beginning-of-the-file"><a href="#At-the-beginning-of-the-file" class="headerlink" title="At the beginning of the file"></a>At the beginning of the file</h1><p>patterns:<br>  fully_connected:<br>    split: { c: c_split }<br>    fuse: { src: [x, y, co], dst: block }<br>    parallel: block<br>    vectorize: { ci: c_split }</p>
<h1 id="Somewhere-below"><a href="#Somewhere-below" class="headerlink" title="Somewhere below"></a>Somewhere below</h1><p>fc8:<br>  pattern: fully_connected<br>  params: { c_split: 8 }<br>@endcode</p>
<h2 id="Automatic-scheduling"><a href="#Automatic-scheduling" class="headerlink" title="Automatic scheduling"></a>Automatic scheduling</h2><p>You can let DNN to schedule layers automatically. Just skip call of <code>cv::dnn::Net::setHalideScheduler</code>. Sometimes it might be even more efficient than manual scheduling.<br>But if specific layers require be scheduled manually, you would be able to<br>mix both manual and automatic scheduling ways. Write scheduling file<br>and skip layers that you want to be scheduled automatically.</p>
