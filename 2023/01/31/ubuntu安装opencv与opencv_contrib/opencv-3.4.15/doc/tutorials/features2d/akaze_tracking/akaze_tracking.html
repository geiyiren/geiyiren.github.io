<h1 id="AKAZE-and-ORB-planar-tracking-tutorial-akaze-tracking"><a href="#AKAZE-and-ORB-planar-tracking-tutorial-akaze-tracking" class="headerlink" title="AKAZE and ORB planar tracking {#tutorial_akaze_tracking}"></a>AKAZE and ORB planar tracking {#tutorial_akaze_tracking}</h1><p>@prev_tutorial{tutorial_akaze_matching}<br>@next_tutorial{tutorial_homography}</p>
<h2 id="Introduction"><a href="#Introduction" class="headerlink" title="Introduction"></a>Introduction</h2><p>In this tutorial we will compare <em>AKAZE</em> and <em>ORB</em> local features using them to find matches between<br>video frames and track object movements.</p>
<p>The algorithm is as follows:</p>
<ul>
<li>Detect and describe keypoints on the first frame, manually set object boundaries</li>
<li>For every next frame:<br>-#  Detect and describe keypoints<br>-#  Match them using bruteforce matcher<br>-#  Estimate homography transformation using RANSAC<br>-#  Filter inliers from all the matches<br>-#  Apply homography transformation to the bounding box to find the object<br>-#  Draw bounding box and inliers, compute inlier ratio as evaluation metric</li>
</ul>
<p><img src="/images/frame.png"></p>
<h2 id="Data"><a href="#Data" class="headerlink" title="Data"></a>Data</h2><p>To do the tracking we need a video and object position on the first frame.</p>
<p>You can download our example video and data from<br><a href="https://docs.google.com/file/d/0B72G7D4snftJandBb0taLVJHMFk">here</a>.</p>
<p>To run the code you have to specify input (camera id or video_file). Then, select a bounding box with the mouse, and press any key to start tracking<br>@code{.none}<br>.&#x2F;planar_tracking blais.mp4<br>@endcode</p>
<h2 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a>Source Code</h2><p>@include cpp&#x2F;tutorial_code&#x2F;features2D&#x2F;AKAZE_tracking&#x2F;planar_tracking.cpp</p>
<h2 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h2><h3 id="Tracker-class"><a href="#Tracker-class" class="headerlink" title="Tracker class"></a>Tracker class</h3><p>This class implements algorithm described abobve using given feature detector and descriptor<br>matcher.</p>
<ul>
<li><p><strong>Setting up the first frame</strong><br>@code{.cpp}<br>void Tracker::setFirstFrame(const Mat frame, vector<Point2f> bb, string title, Stats&amp; stats)<br>{<br>first_frame &#x3D; frame.clone();<br>(*detector)(first_frame, noArray(), first_kp, first_desc);<br>stats.keypoints &#x3D; (int)first_kp.size();<br>drawBoundingBox(first_frame, bb);<br>putText(first_frame, title, Point(0, 60), FONT_HERSHEY_PLAIN, 5, Scalar::all(0), 4);<br>object_bb &#x3D; bb;<br>}<br>@endcode<br>We compute and store keypoints and descriptors from the first frame and prepare it for the<br>output.</p>
<p>We need to save number of detected keypoints to make sure both detectors locate roughly the same<br>number of those.</p>
</li>
<li><p><strong>Processing frames</strong></p>
<p>-#  Locate keypoints and compute descriptors<br>@code{.cpp}<br>(*detector)(frame, noArray(), kp, desc);<br>@endcode<br><br>To find matches between frames we have to locate the keypoints first.<br><br>In this tutorial detectors are set up to find about 1000 keypoints on each frame.<br>-#  Use 2-nn matcher to find correspondences<br>@code{.cpp}<br>matcher-&gt;knnMatch(first_desc, desc, matches, 2);<br>for(unsigned i &#x3D; 0; i &lt; matches.size(); i++) {<br>    if(matches[i][0].distance &lt; nn_match_ratio * matches[i][1].distance) {<br>        matched1.push_back(first_kp[matches[i][0].queryIdx]);<br>        matched2.push_back(      kp[matches[i][0].trainIdx]);<br>    }<br>}<br>@endcode<br>If the closest match is *nn_match_ratio* closer than the second closest one, then itâ€™s a<br>match.<br>-#  Use *RANSAC* to estimate homography transformation<br>@code{.cpp}<br>homography &#x3D; findHomography(Points(matched1), Points(matched2),<br>                            RANSAC, ransac_thresh, inlier_mask);<br>@endcode<br>If there are at least 4 matches we can use random sample consensus to estimate image<br>transformation.<br>-#  Save the inliers<br>@code{.cpp}<br>for(unsigned i &#x3D; 0; i &lt; matched1.size(); i++) {<br>    if(inlier_mask.at<uchar>(i)) {<br>        int new_i &#x3D; static_cast<int>(inliers1.size());<br>        inliers1.push_back(matched1[i]);<br>        inliers2.push_back(matched2[i]);<br>        inlier_matches.push_back(DMatch(new_i, new_i, 0));<br>    }<br>}<br>@endcode<br>Since <em>findHomography</em> computes the inliers we only have to save the chosen points and<br>matches.<br>-#  Project object bounding box<br>@code{.cpp}<br>perspectiveTransform(object_bb, new_bb, homography);<br>@endcode<br><br>If there is a reasonable number of inliers we can use estimated transformation to locate the<br>object.</p>
</li>
</ul>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><p>You can watch the resulting <a href="http://www.youtube.com/watch?v=LWY-w8AGGhE">video on youtube</a>.</p>
<p><em>AKAZE</em> statistics:<br>@code{.none}<br>Matches      626<br>Inliers      410<br>Inlier ratio 0.58<br>Keypoints    1117<br>@endcode</p>
<p><em>ORB</em> statistics:<br>@code{.none}<br>Matches      504<br>Inliers      319<br>Inlier ratio 0.56<br>Keypoints    1112<br>@endcode</p>
