<h1 id="OpenCV-iOS-Video-Processing-tutorial-video-processing"><a href="#OpenCV-iOS-Video-Processing-tutorial-video-processing" class="headerlink" title="OpenCV iOS - Video Processing {#tutorial_video_processing}"></a>OpenCV iOS - Video Processing {#tutorial_video_processing}</h1><p>@prev_tutorial{tutorial_image_manipulation}</p>
<p>This tutorial explains how to process video frames using the iPhone’s camera and OpenCV.</p>
<h2 id="Prerequisites"><a href="#Prerequisites" class="headerlink" title="Prerequisites:"></a>Prerequisites:</h2><ul>
<li>Xcode 4.3 or higher</li>
<li>Basic knowledge of iOS programming (Objective-C, Interface Builder)</li>
</ul>
<h2 id="Including-OpenCV-library-in-your-iOS-project"><a href="#Including-OpenCV-library-in-your-iOS-project" class="headerlink" title="Including OpenCV library in your iOS project"></a>Including OpenCV library in your iOS project</h2><p>The OpenCV library comes as a so-called framework, which you can directly drag-and-drop into your<br>XCode project. Download the latest binary from<br><a href="http://sourceforge.net/projects/opencvlibrary/files/opencv-ios/">http://sourceforge.net/projects/opencvlibrary/files/opencv-ios/</a>. Alternatively follow this<br>guide @ref tutorial_ios_install to compile the framework manually. Once you have the framework, just<br>drag-and-drop into XCode:</p>
<p><img src="/images/xcode_hello_ios_framework_drag_and_drop.png"></p>
<p>Also you have to locate the prefix header that is used for all header files in the project. The file<br>is typically located at “ProjectName&#x2F;Supporting Files&#x2F;ProjectName-Prefix.pch”. There, you have add<br>an include statement to import the opencv library. However, make sure you include opencv before you<br>include UIKit and Foundation, because else you will get some weird compile errors that some macros<br>like min and max are defined multiple times. For example the prefix header could look like the<br>following:<br>@code{.objc}<br>&#x2F;&#x2F;<br>&#x2F;&#x2F; Prefix header for all source files of the ‘VideoFilters’ target in the ‘VideoFilters’ project<br>&#x2F;&#x2F;</p>
<p>#import &lt;Availability.h&gt;</p>
<p>#ifndef __IPHONE_4_0<br>#warning “This project uses features only available in iOS SDK 4.0 and later.”<br>#endif</p>
<p>#ifdef __cplusplus<br>#import &lt;opencv2&#x2F;opencv.hpp&gt;<br>#endif</p>
<p>#ifdef <strong>OBJC</strong><br>    #import &lt;UIKit&#x2F;UIKit.h&gt;<br>    #import &lt;Foundation&#x2F;Foundation.h&gt;<br>#endif<br>@endcode</p>
<h3 id="Example-video-frame-processing-project"><a href="#Example-video-frame-processing-project" class="headerlink" title="Example video frame processing project"></a>Example video frame processing project</h3><h4 id="User-Interface"><a href="#User-Interface" class="headerlink" title="User Interface"></a>User Interface</h4><p>First, we create a simple iOS project, for example Single View Application. Then, we create and add<br>an UIImageView and UIButton to start the camera and display the video frames. The storyboard could<br>look like that:</p>
<p><img src="/images/xcode_hello_ios_viewcontroller_layout.png"></p>
<p>Make sure to add and connect the IBOutlets and IBActions to the corresponding ViewController:<br>@code{.objc}<br>@interface ViewController : UIViewController<br>{<br>    IBOutlet UIImageView* imageView;<br>    IBOutlet UIButton* button;<br>}</p>
<ul>
<li>(IBAction)actionStart:(id)sender;</li>
</ul>
<p>@end<br>@endcode</p>
<h4 id="Adding-the-Camera"><a href="#Adding-the-Camera" class="headerlink" title="Adding the Camera"></a>Adding the Camera</h4><p>We add a camera controller to the view controller and initialize it when the view has loaded:<br>@code{.objc}<br>#import &lt;opencv2&#x2F;videoio&#x2F;cap_ios.h&gt;<br>using namespace cv;</p>
<p>@interface ViewController : UIViewController<br>{<br>    …<br>    CvVideoCamera* videoCamera;<br>}<br>…<br>@property (nonatomic, retain) CvVideoCamera* videoCamera;</p>
<p>@end<br>@endcode<br>@code{.objc}</p>
<ul>
<li><p>(void)viewDidLoad<br>{<br>  [super viewDidLoad];<br>  &#x2F;&#x2F; Do any additional setup after loading the view, typically from a nib.</p>
<p>  self.videoCamera &#x3D; [[CvVideoCamera alloc] initWithParentView:imageView];<br>  self.videoCamera.defaultAVCaptureDevicePosition &#x3D; AVCaptureDevicePositionFront;<br>  self.videoCamera.defaultAVCaptureSessionPreset &#x3D; AVCaptureSessionPreset352x288;<br>  self.videoCamera.defaultAVCaptureVideoOrientation &#x3D; AVCaptureVideoOrientationPortrait;<br>  self.videoCamera.defaultFPS &#x3D; 30;<br>  self.videoCamera.grayscale &#x3D; NO;</p>
</li>
</ul>
<p>}<br>@endcode<br>In this case, we initialize the camera and provide the imageView as a target for rendering each<br>frame. CvVideoCamera is basically a wrapper around AVFoundation, so we provide as properties some of<br>the AVFoundation camera options. For example we want to use the front camera, set the video size to<br>352x288 and a video orientation (the video camera normally outputs in landscape mode, which results<br>in transposed data when you design a portrait application).</p>
<p>The property defaultFPS sets the FPS of the camera. If the processing is less fast than the desired<br>FPS, frames are automatically dropped.</p>
<p>The property grayscale&#x3D;YES results in a different colorspace, namely “YUV (YpCbCr 4:2:0)”, while<br>grayscale&#x3D;NO will output 32 bit BGRA.</p>
<p>Additionally, we have to manually add framework dependencies of the opencv framework. Finally, you<br>should have at least the following frameworks in your project:</p>
<ul>
<li><p>opencv2</p>
</li>
<li><p>Accelerate</p>
</li>
<li><p>AssetsLibrary</p>
</li>
<li><p>AVFoundation</p>
</li>
<li><p>CoreGraphics</p>
</li>
<li><p>CoreImage</p>
</li>
<li><p>CoreMedia</p>
</li>
<li><p>CoreVideo</p>
</li>
<li><p>QuartzCore</p>
</li>
<li><p>UIKit</p>
</li>
<li><p>Foundation</p>
<p><img src="/images/xcode_hello_ios_frameworks_add_dependencies.png"></p>
</li>
</ul>
<h4 id="Processing-frames"><a href="#Processing-frames" class="headerlink" title="Processing frames"></a>Processing frames</h4><p>We follow the delegation pattern, which is very common in iOS, to provide access to each camera<br>frame. Basically, the View Controller has to implement the CvVideoCameraDelegate protocol and has to<br>be set as delegate to the video camera:<br>@code{.objc}<br>@interface ViewController : UIViewController<CvVideoCameraDelegate><br>@endcode<br>@code{.objc}</p>
<ul>
<li>(void)viewDidLoad<br>{<br>  …<br>  self.videoCamera &#x3D; [[CvVideoCamera alloc] initWithParentView:imageView];<br>  self.videoCamera.delegate &#x3D; self;<br>  …<br>}<br>@endcode<br>@code{.objc}</li>
</ul>
<p>#pragma mark - Protocol CvVideoCameraDelegate</p>
<p>#ifdef __cplusplus</p>
<ul>
<li>(void)processImage:(Mat&amp;)image;<br>{<br>  &#x2F;&#x2F; Do some OpenCV stuff with the image<br>}</li>
</ul>
<p>#endif<br>@endcode<br>Note that we are using C++ here (cv::Mat). Important: You have to rename the view controller’s<br>extension .m into .mm, so that the compiler compiles it under the assumption of Objective-C++<br>(Objective-C and C++ mixed). Then, __cplusplus is defined when the compiler is processing the file<br>for C++ code. Therefore, we put our code within a block where __cplusplus is defined.</p>
<h4 id="Basic-video-processing"><a href="#Basic-video-processing" class="headerlink" title="Basic video processing"></a>Basic video processing</h4><p>From here you can start processing video frames. For example the following snippet color-inverts the<br>image:<br>@code{.objc}</p>
<ul>
<li><p>(void)processImage:(Mat&amp;)image;<br>{<br>  &#x2F;&#x2F; Do some OpenCV stuff with the image<br>  Mat image_copy;<br>  cvtColor(image, image_copy, COLOR_BGR2GRAY);</p>
<p>  &#x2F;&#x2F; invert image<br>  bitwise_not(image_copy, image_copy);</p>
<p>  &#x2F;&#x2F;Convert BGR to BGRA (three channel to four channel)<br>  Mat bgr;<br>  cvtColor(image_copy, bgr, COLOR_GRAY2BGR);</p>
<p>  cvtColor(bgr, image, COLOR_BGR2BGRA);</p>
</li>
</ul>
<p>}<br>@endcode</p>
<h4 id="Start"><a href="#Start" class="headerlink" title="Start!"></a>Start!</h4><p>Finally, we have to tell the camera to actually start&#x2F;stop working. The following code will start<br>the camera when you press the button, assuming you connected the UI properly:<br>@code{.objc}<br>#pragma mark - UI Actions</p>
<ul>
<li>(IBAction)actionStart:(id)sender;<br>{<br>  [self.videoCamera start];<br>}<br>@endcode</li>
</ul>
<h4 id="Hints"><a href="#Hints" class="headerlink" title="Hints"></a>Hints</h4><p>Try to avoid costly matrix copy operations as much as you can, especially if you are aiming for<br>real-time. As the image data is passed as reference, work in-place, if possible.</p>
<p>When you are working on grayscale data, turn set grayscale &#x3D; YES as the YUV colorspace gives you<br>directly access the luminance plane.</p>
<p>The Accelerate framework provides some CPU-accelerated DSP filters, which come handy in your case.</p>
