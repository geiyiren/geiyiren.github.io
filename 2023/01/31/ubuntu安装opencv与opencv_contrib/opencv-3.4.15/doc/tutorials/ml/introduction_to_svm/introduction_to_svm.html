<h1 id="Introduction-to-Support-Vector-Machines-tutorial-introduction-to-svm"><a href="#Introduction-to-Support-Vector-Machines-tutorial-introduction-to-svm" class="headerlink" title="Introduction to Support Vector Machines {#tutorial_introduction_to_svm}"></a>Introduction to Support Vector Machines {#tutorial_introduction_to_svm}</h1><p>@next_tutorial{tutorial_non_linear_svms}</p>
<h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>In this tutorial you will learn how to:</p>
<ul>
<li>Use the OpenCV functions @ref cv::ml::SVM::train to build a classifier based on SVMs and @ref<br>cv::ml::SVM::predict to test its performance.</li>
</ul>
<h2 id="What-is-a-SVM"><a href="#What-is-a-SVM" class="headerlink" title="What is a SVM?"></a>What is a SVM?</h2><p>A Support Vector Machine (SVM) is a discriminative classifier formally defined by a separating<br>hyperplane. In other words, given labeled training data (<em>supervised learning</em>), the algorithm<br>outputs an optimal hyperplane which categorizes new examples.</p>
<p>In which sense is the hyperplane obtained optimal? Let’s consider the following simple problem:</p>
<p>For a linearly separable set of 2D-points which belong to one of two classes, find a separating<br>straight line.</p>
<p><img src="/images/separating-lines.png"></p>
<p>@note In this example we deal with lines and points in the Cartesian plane instead of hyperplanes<br>and vectors in a high dimensional space. This is a simplification of the problem.It is important to<br>understand that this is done only because our intuition is better built from examples that are easy<br>to imagine. However, the same concepts apply to tasks where the examples to classify lie in a space<br>whose dimension is higher than two.</p>
<p>In the above picture you can see that there exists multiple lines that offer a solution to the<br>problem. Is any of them better than the others? We can intuitively define a criterion to estimate<br>the worth of the lines: <em> A line is bad if it passes too close to the points because it will be<br>noise sensitive and it will not generalize correctly. </em> Therefore, our goal should be to find<br>the line passing as far as possible from all points.</p>
<p>Then, the operation of the SVM algorithm is based on finding the hyperplane that gives the largest<br>minimum distance to the training examples. Twice, this distance receives the important name of<br><strong>margin</strong> within SVM’s theory. Therefore, the optimal separating hyperplane <em>maximizes</em> the margin<br>of the training data.</p>
<p><img src="/images/optimal-hyperplane.png"></p>
<h2 id="How-is-the-optimal-hyperplane-computed"><a href="#How-is-the-optimal-hyperplane-computed" class="headerlink" title="How is the optimal hyperplane computed?"></a>How is the optimal hyperplane computed?</h2><p>Let’s introduce the notation used to define formally a hyperplane:</p>
<p>\f[f(x) &#x3D; \beta_{0} + \beta^{T} x,\f]</p>
<p>where \f$\beta\f$ is known as the <em>weight vector</em> and \f$\beta_{0}\f$ as the <em>bias</em>.</p>
<p>@note A more in depth description of this and hyperplanes you can find in the section 4.5 (<em>Separating<br>Hyperplanes</em>) of the book: <em>Elements of Statistical Learning</em> by T. Hastie, R. Tibshirani and J. H.<br>Friedman (@cite HTF01).</p>
<p>The optimal hyperplane can be represented in an infinite number of different ways by<br>scaling of \f$\beta\f$ and \f$\beta_{0}\f$. As a matter of convention, among all the possible<br>representations of the hyperplane, the one chosen is</p>
<p>\f[|\beta_{0} + \beta^{T} x| &#x3D; 1\f]</p>
<p>where \f$x\f$ symbolizes the training examples closest to the hyperplane. In general, the training<br>examples that are closest to the hyperplane are called <strong>support vectors</strong>. This representation is<br>known as the <strong>canonical hyperplane</strong>.</p>
<p>Now, we use the result of geometry that gives the distance between a point \f$x\f$ and a hyperplane<br>\f$(\beta, \beta_{0})\f$:</p>
<p>\f[\mathrm{distance} &#x3D; \frac{|\beta_{0} + \beta^{T} x|}{||\beta||}.\f]</p>
<p>In particular, for the canonical hyperplane, the numerator is equal to one and the distance to the<br>support vectors is</p>
<p>\f[\mathrm{distance}<em>{\text{ support vectors}} &#x3D; \frac{|\beta</em>{0} + \beta^{T} x|}{||\beta||} &#x3D; \frac{1}{||\beta||}.\f]</p>
<p>Recall that the margin introduced in the previous section, here denoted as \f$M\f$, is twice the<br>distance to the closest examples:</p>
<p>\f[M &#x3D; \frac{2}{||\beta||}\f]</p>
<p>Finally, the problem of maximizing \f$M\f$ is equivalent to the problem of minimizing a function<br>\f$L(\beta)\f$ subject to some constraints. The constraints model the requirement for the hyperplane to<br>classify correctly all the training examples \f$x_{i}\f$. Formally,</p>
<p>\f[\min_{\beta, \beta_{0}} L(\beta) &#x3D; \frac{1}{2}||\beta||^{2} \text{ subject to } y_{i}(\beta^{T} x_{i} + \beta_{0}) \geq 1 \text{ } \forall i,\f]</p>
<p>where \f$y_{i}\f$ represents each of the labels of the training examples.</p>
<p>This is a problem of Lagrangian optimization that can be solved using Lagrange multipliers to obtain<br>the weight vector \f$\beta\f$ and the bias \f$\beta_{0}\f$ of the optimal hyperplane.</p>
<h2 id="Source-Code"><a href="#Source-Code" class="headerlink" title="Source Code"></a>Source Code</h2><p>@add_toggle_cpp</p>
<ul>
<li><p><strong>Downloadable code</strong>: Click<br><a href="https://github.com/opencv/opencv/tree/3.4/samples/cpp/tutorial_code/ml/introduction_to_svm/introduction_to_svm.cpp">here</a></p>
</li>
<li><p><strong>Code at glance:</strong><br>@include samples&#x2F;cpp&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.cpp<br>@end_toggle</p>
</li>
</ul>
<p>@add_toggle_java</p>
<ul>
<li><p><strong>Downloadable code</strong>: Click<br><a href="https://github.com/opencv/opencv/tree/3.4/samples/java/tutorial_code/ml/introduction_to_svm/IntroductionToSVMDemo.java">here</a></p>
</li>
<li><p><strong>Code at glance:</strong><br>@include samples&#x2F;java&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;IntroductionToSVMDemo.java<br>@end_toggle</p>
</li>
</ul>
<p>@add_toggle_python</p>
<ul>
<li><p><strong>Downloadable code</strong>: Click<br><a href="https://github.com/opencv/opencv/tree/3.4/samples/python/tutorial_code/ml/introduction_to_svm/introduction_to_svm.py">here</a></p>
</li>
<li><p><strong>Code at glance:</strong><br>@include samples&#x2F;python&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.py<br>@end_toggle</p>
</li>
</ul>
<h2 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h2><ul>
<li><strong>Set up the training data</strong></li>
</ul>
<p>The training data of this exercise is formed by a set of labeled 2D-points that belong to one of<br>two different classes; one of the classes consists of one point and the other of three points.</p>
<p>@add_toggle_cpp<br>@snippet samples&#x2F;cpp&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.cpp setup1<br>@end_toggle</p>
<p>@add_toggle_java<br>@snippet samples&#x2F;java&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;IntroductionToSVMDemo.java setup1<br>@end_toggle</p>
<p>@add_toggle_python<br>@snippet samples&#x2F;python&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.py setup1<br>@end_toggle</p>
<p>The function @ref cv::ml::SVM::train that will be used afterwards requires the training data to be<br>stored as @ref cv::Mat objects of floats. Therefore, we create these objects from the arrays<br>defined above:</p>
<p>@add_toggle_cpp<br>@snippet samples&#x2F;cpp&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.cpp setup2<br>@end_toggle</p>
<p>@add_toggle_java<br>@snippet samples&#x2F;java&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;IntroductionToSVMDemo.java setup2<br>@end_toggle</p>
<p>@add_toggle_python<br>@snippet samples&#x2F;python&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.py setup1<br>@end_toggle</p>
<ul>
<li><p><strong>Set up SVM’s parameters</strong></p>
<p>In this tutorial we have introduced the theory of SVMs in the most simple case, when the<br>training examples are spread into two classes that are linearly separable. However, SVMs can be<br>used in a wide variety of problems (e.g. problems with non-linearly separable data, a SVM using<br>a kernel function to raise the dimensionality of the examples, etc). As a consequence of this,<br>we have to define some parameters before training the SVM. These parameters are stored in an<br>object of the class @ref cv::ml::SVM.</p>
</li>
</ul>
<p>@add_toggle_cpp<br>@snippet samples&#x2F;cpp&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.cpp init<br>@end_toggle</p>
<p>@add_toggle_java<br>@snippet samples&#x2F;java&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;IntroductionToSVMDemo.java init<br>@end_toggle</p>
<p>@add_toggle_python<br>@snippet samples&#x2F;python&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.py init<br>@end_toggle</p>
<p>Here:</p>
<ul>
<li><p><em>Type of SVM</em>. We choose here the type @ref cv::ml::SVM::C_SVC “C_SVC” that can be used for<br>n-class classification (n \f$\geq\f$ 2). The important feature of this type is that it deals<br>with imperfect separation of classes (i.e. when the training data is non-linearly separable).<br>This feature is not important here since the data is linearly separable and we chose this SVM<br>type only for being the most commonly used.</p>
</li>
<li><p><em>Type of SVM kernel</em>. We have not talked about kernel functions since they are not<br>interesting for the training data we are dealing with. Nevertheless, let’s explain briefly now<br>the main idea behind a kernel function. It is a mapping done to the training data to improve<br>its resemblance to a linearly separable set of data. This mapping consists of increasing the<br>dimensionality of the data and is done efficiently using a kernel function. We choose here the<br>type @ref cv::ml::SVM::LINEAR “LINEAR” which means that no mapping is done. This parameter is<br>defined using cv::ml::SVM::setKernel.</p>
</li>
<li><p><em>Termination criteria of the algorithm</em>. The SVM training procedure is implemented solving a<br>constrained quadratic optimization problem in an <strong>iterative</strong> fashion. Here we specify a<br>maximum number of iterations and a tolerance error so we allow the algorithm to finish in<br>less number of steps even if the optimal hyperplane has not been computed yet. This<br>parameter is defined in a structure @ref cv::TermCriteria .</p>
</li>
<li><p><strong>Train the SVM</strong><br>We call the method @ref cv::ml::SVM::train to build the SVM model.</p>
</li>
</ul>
<p>@add_toggle_cpp<br>@snippet samples&#x2F;cpp&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.cpp train<br>@end_toggle</p>
<p>@add_toggle_java<br>@snippet samples&#x2F;java&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;IntroductionToSVMDemo.java train<br>@end_toggle</p>
<p>@add_toggle_python<br>@snippet samples&#x2F;python&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.py train<br>@end_toggle</p>
<ul>
<li><p><strong>Regions classified by the SVM</strong></p>
<p>The method @ref cv::ml::SVM::predict is used to classify an input sample using a trained SVM. In<br>this example we have used this method in order to color the space depending on the prediction done<br>by the SVM. In other words, an image is traversed interpreting its pixels as points of the<br>Cartesian plane. Each of the points is colored depending on the class predicted by the SVM; in<br>green if it is the class with label 1 and in blue if it is the class with label -1.</p>
</li>
</ul>
<p>@add_toggle_cpp<br>@snippet samples&#x2F;cpp&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.cpp show<br>@end_toggle</p>
<p>@add_toggle_java<br>@snippet samples&#x2F;java&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;IntroductionToSVMDemo.java show<br>@end_toggle</p>
<p>@add_toggle_python<br>@snippet samples&#x2F;python&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.py show<br>@end_toggle</p>
<ul>
<li><p><strong>Support vectors</strong></p>
<p>We use here a couple of methods to obtain information about the support vectors.<br>The method @ref cv::ml::SVM::getSupportVectors obtain all of the support<br>vectors. We have used this methods here to find the training examples that are<br>support vectors and highlight them.</p>
</li>
</ul>
<p>@add_toggle_cpp<br>@snippet samples&#x2F;cpp&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.cpp show_vectors<br>@end_toggle</p>
<p>@add_toggle_java<br>@snippet samples&#x2F;java&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;IntroductionToSVMDemo.java show_vectors<br>@end_toggle</p>
<p>@add_toggle_python<br>@snippet samples&#x2F;python&#x2F;tutorial_code&#x2F;ml&#x2F;introduction_to_svm&#x2F;introduction_to_svm.py show_vectors<br>@end_toggle</p>
<h2 id="Results"><a href="#Results" class="headerlink" title="Results"></a>Results</h2><ul>
<li>The code opens an image and shows the training examples of both classes. The points of one class<br>are represented with white circles and black ones are used for the other class.</li>
<li>The SVM is trained and used to classify all the pixels of the image. This results in a division<br>of the image in a blue region and a green region. The boundary between both regions is the<br>optimal separating hyperplane.</li>
<li>Finally the support vectors are shown using gray rings around the training examples.</li>
</ul>
<p><img src="/images/svm_intro_result.png"></p>
