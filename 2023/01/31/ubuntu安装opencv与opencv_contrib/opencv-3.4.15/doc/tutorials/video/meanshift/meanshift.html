<h1 id="Meanshift-and-Camshift-tutorial-meanshift"><a href="#Meanshift-and-Camshift-tutorial-meanshift" class="headerlink" title="Meanshift and Camshift {#tutorial_meanshift}"></a>Meanshift and Camshift {#tutorial_meanshift}</h1><p>@prev_tutorial{tutorial_background_subtraction}<br>@next_tutorial{tutorial_optical_flow}</p>
<h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>In this chapter,</p>
<ul>
<li>We will learn about the Meanshift and Camshift algorithms to track objects in videos.</li>
</ul>
<h2 id="Meanshift"><a href="#Meanshift" class="headerlink" title="Meanshift"></a>Meanshift</h2><p>The intuition behind the meanshift is simple. Consider you have a set of points. (It can be a pixel<br>distribution like histogram backprojection). You are given a small window (may be a circle) and you<br>have to move that window to the area of maximum pixel density (or maximum number of points). It is<br>illustrated in the simple image given below:</p>
<p><img src="/images/meanshift_basics.jpg" alt="image"></p>
<p>The initial window is shown in blue circle with the name “C1”. Its original center is marked in blue<br>rectangle, named “C1_o”. But if you find the centroid of the points inside that window, you will<br>get the point “C1_r” (marked in small blue circle) which is the real centroid of the window. Surely<br>they don’t match. So move your window such that the circle of the new window matches with the previous<br>centroid. Again find the new centroid. Most probably, it won’t match. So move it again, and continue<br>the iterations such that the center of window and its centroid falls on the same location (or within a<br>small desired error). So finally what you obtain is a window with maximum pixel distribution. It is<br>marked with a green circle, named “C2”. As you can see in the image, it has maximum number of points. The<br>whole process is demonstrated on a static image below:</p>
<p><img src="/images/meanshift_face.gif" alt="image"></p>
<p>So we normally pass the histogram backprojected image and initial target location. When the object<br>moves, obviously the movement is reflected in the histogram backprojected image. As a result, the meanshift<br>algorithm moves our window to the new location with maximum density.</p>
<h3 id="Meanshift-in-OpenCV"><a href="#Meanshift-in-OpenCV" class="headerlink" title="Meanshift in OpenCV"></a>Meanshift in OpenCV</h3><p>To use meanshift in OpenCV, first we need to setup the target, find its histogram so that we can<br>backproject the target on each frame for calculation of meanshift. We also need to provide an initial<br>location of window. For histogram, only Hue is considered here. Also, to avoid false values due to<br>low light, low light values are discarded using <strong>cv.inRange()</strong> function.</p>
<p>@add_toggle_cpp</p>
<ul>
<li><p><strong>Downloadable code</strong>: Click<br><a href="https://github.com/opencv/opencv/tree/3.4/samples/cpp/tutorial_code/video/meanshift/meanshift.cpp">here</a></p>
</li>
<li><p><strong>Code at glance:</strong><br>@include samples&#x2F;cpp&#x2F;tutorial_code&#x2F;video&#x2F;meanshift&#x2F;meanshift.cpp<br>@end_toggle</p>
</li>
</ul>
<p>@add_toggle_python</p>
<ul>
<li><p><strong>Downloadable code</strong>: Click<br><a href="https://github.com/opencv/opencv/tree/3.4/samples/python/tutorial_code/video/meanshift/meanshift.py">here</a></p>
</li>
<li><p><strong>Code at glance:</strong><br>@include samples&#x2F;python&#x2F;tutorial_code&#x2F;video&#x2F;meanshift&#x2F;meanshift.py<br>@end_toggle</p>
</li>
</ul>
<p>@add_toggle_java</p>
<ul>
<li><p><strong>Downloadable code</strong>: Click<br><a href="https://github.com/opencv/opencv/tree/3.4/samples/java/tutorial_code/video/meanshift/MeanshiftDemo.java">here</a></p>
</li>
<li><p><strong>Code at glance:</strong><br>@include  samples&#x2F;java&#x2F;tutorial_code&#x2F;video&#x2F;meanshift&#x2F;MeanshiftDemo.java<br>@end_toggle</p>
</li>
</ul>
<p>Three frames in a video I used is given below:</p>
<p><img src="/images/meanshift_result.jpg" alt="image"></p>
<h2 id="Camshift"><a href="#Camshift" class="headerlink" title="Camshift"></a>Camshift</h2><p>Did you closely watch the last result? There is a problem. Our window always has the same size whether<br>the car is very far or very close to the camera. That is not good. We need to adapt the window<br>size with size and rotation of the target. Once again, the solution came from “OpenCV Labs” and it<br>is called CAMshift (Continuously Adaptive Meanshift) published by Gary Bradsky in his paper<br>“Computer Vision Face Tracking for Use in a Perceptual User Interface” in 1998 @cite Bradski98 .</p>
<p>It applies meanshift first. Once meanshift converges, it updates the size of the window as,<br>\f$s &#x3D; 2 \times \sqrt{\frac{M_{00}}{256}}\f$. It also calculates the orientation of the best fitting ellipse<br>to it. Again it applies the meanshift with new scaled search window and previous window location.<br>The process continues until the required accuracy is met.</p>
<p><img src="/images/camshift_face.gif" alt="image"></p>
<h3 id="Camshift-in-OpenCV"><a href="#Camshift-in-OpenCV" class="headerlink" title="Camshift in OpenCV"></a>Camshift in OpenCV</h3><p>It is similar to meanshift, but returns a rotated rectangle (that is our result) and box<br>parameters (used to be passed as search window in next iteration). See the code below:</p>
<p>@add_toggle_cpp</p>
<ul>
<li><p><strong>Downloadable code</strong>: Click<br><a href="https://github.com/opencv/opencv/tree/3.4/samples/cpp/tutorial_code/video/meanshift/camshift.cpp">here</a></p>
</li>
<li><p><strong>Code at glance:</strong><br>@include samples&#x2F;cpp&#x2F;tutorial_code&#x2F;video&#x2F;meanshift&#x2F;camshift.cpp<br>@end_toggle</p>
</li>
</ul>
<p>@add_toggle_python</p>
<ul>
<li><p><strong>Downloadable code</strong>: Click<br><a href="https://github.com/opencv/opencv/tree/3.4/samples/python/tutorial_code/video/meanshift/camshift.py">here</a></p>
</li>
<li><p><strong>Code at glance:</strong><br>@include samples&#x2F;python&#x2F;tutorial_code&#x2F;video&#x2F;meanshift&#x2F;camshift.py<br>@end_toggle</p>
</li>
</ul>
<p>@add_toggle_java</p>
<ul>
<li><p><strong>Downloadable code</strong>: Click<br><a href="https://github.com/opencv/opencv/tree/3.4/samples/java/tutorial_code/video/meanshift/CamshiftDemo.java">here</a></p>
</li>
<li><p><strong>Code at glance:</strong><br>@include  samples&#x2F;java&#x2F;tutorial_code&#x2F;video&#x2F;meanshift&#x2F;CamshiftDemo.java<br>@end_toggle</p>
</li>
</ul>
<p>Three frames of the result is shown below:</p>
<p><img src="/images/camshift_result.jpg" alt="image"></p>
<h2 id="Additional-Resources"><a href="#Additional-Resources" class="headerlink" title="Additional Resources"></a>Additional Resources</h2><p>-#  French Wikipedia page on <a href="http://fr.wikipedia.org/wiki/Camshift">Camshift</a>. (The two animations<br>    are taken from there)<br>2.  Bradski, G.R., “Real time face and object tracking as a component of a perceptual user<br>    interface,” Applications of Computer Vision, 1998. WACV ‘98. Proceedings., Fourth IEEE Workshop<br>    on , vol., no., pp.214,219, 19-21 Oct 1998</p>
<h2 id="Exercises"><a href="#Exercises" class="headerlink" title="Exercises"></a>Exercises</h2><p>-#  OpenCV comes with a Python <a href="https://github.com/opencv/opencv/blob/3.4/samples/python/camshift.py">sample</a> for an interactive demo of camshift. Use it, hack it, understand<br>    it.</p>
