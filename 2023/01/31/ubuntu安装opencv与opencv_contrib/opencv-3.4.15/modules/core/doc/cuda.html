<h1 id="CUDA-Module-Introduction-cuda-intro"><a href="#CUDA-Module-Introduction-cuda-intro" class="headerlink" title="CUDA Module Introduction {#cuda_intro}"></a>CUDA Module Introduction {#cuda_intro}</h1><h2 id="General-Information"><a href="#General-Information" class="headerlink" title="General Information"></a>General Information</h2><p>The OpenCV CUDA module is a set of classes and functions to utilize CUDA computational capabilities.<br>It is implemented using NVIDIA* CUDA* Runtime API and supports only NVIDIA GPUs. The OpenCV CUDA<br>module includes utility functions, low-level vision primitives, and high-level algorithms. The<br>utility functions and low-level primitives provide a powerful infrastructure for developing fast<br>vision algorithms taking advantage of CUDA whereas the high-level functionality includes some<br>state-of-the-art algorithms (such as stereo correspondence, face and people detectors, and others)<br>ready to be used by the application developers.</p>
<p>The CUDA module is designed as a host-level API. This means that if you have pre-compiled OpenCV<br>CUDA binaries, you are not required to have the CUDA Toolkit installed or write any extra code to<br>make use of the CUDA.</p>
<p>The OpenCV CUDA module is designed for ease of use and does not require any knowledge of CUDA.<br>Though, such a knowledge will certainly be useful to handle non-trivial cases or achieve the highest<br>performance. It is helpful to understand the cost of various operations, what the GPU does, what the<br>preferred data formats are, and so on. The CUDA module is an effective instrument for quick<br>implementation of CUDA-accelerated computer vision algorithms. However, if your algorithm involves<br>many simple operations, then, for the best possible performance, you may still need to write your<br>own kernels to avoid extra write and read operations on the intermediate results.</p>
<p>To enable CUDA support, configure OpenCV using CMake with WITH_CUDA&#x3D;ON . When the flag is set and<br>if CUDA is installed, the full-featured OpenCV CUDA module is built. Otherwise, the module is still<br>built but at runtime all functions from the module throw Exception with CV_GpuNotSupported error<br>code, except for cuda::getCudaEnabledDeviceCount(). The latter function returns zero GPU count in<br>this case. Building OpenCV without CUDA support does not perform device code compilation, so it does<br>not require the CUDA Toolkit installed. Therefore, using the cuda::getCudaEnabledDeviceCount()<br>function, you can implement a high-level algorithm that will detect GPU presence at runtime and<br>choose an appropriate implementation (CPU or GPU) accordingly.</p>
<h2 id="Compilation-for-Different-NVIDIA-Platforms"><a href="#Compilation-for-Different-NVIDIA-Platforms" class="headerlink" title="Compilation for Different NVIDIA* Platforms"></a>Compilation for Different NVIDIA* Platforms</h2><p>NVIDIA* compiler enables generating binary code (cubin and fatbin) and intermediate code (PTX).<br>Binary code often implies a specific GPU architecture and generation, so the compatibility with<br>other GPUs is not guaranteed. PTX is targeted for a virtual platform that is defined entirely by the<br>set of capabilities or features. Depending on the selected virtual platform, some of the<br>instructions are emulated or disabled, even if the real hardware supports all the features.</p>
<p>At the first call, the PTX code is compiled to binary code for the particular GPU using a JIT<br>compiler. When the target GPU has a compute capability (CC) lower than the PTX code, JIT fails. By<br>default, the OpenCV CUDA module includes:</p>
<p>*<br>   Binaries for compute capabilities 1.3 and 2.0 (controlled by CUDA_ARCH_BIN in CMake)</p>
<p>*<br>   PTX code for compute capabilities 1.1 and 1.3 (controlled by CUDA_ARCH_PTX in CMake)</p>
<p>This means that for devices with CC 1.3 and 2.0 binary images are ready to run. For all newer<br>platforms, the PTX code for 1.3 is JIT’ed to a binary image. For devices with CC 1.1 and 1.2, the<br>PTX for 1.1 is JIT’ed. For devices with CC 1.0, no code is available and the functions throw<br>Exception. For platforms where JIT compilation is performed first, the run is slow.</p>
<p>On a GPU with CC 1.0, you can still compile the CUDA module and most of the functions will run<br>flawlessly. To achieve this, add “1.0” to the list of binaries, for example,<br>CUDA_ARCH_BIN&#x3D;”1.0 1.3 2.0” . The functions that cannot be run on CC 1.0 GPUs throw an exception.</p>
<p>You can always determine at runtime whether the OpenCV GPU-built binaries (or PTX code) are<br>compatible with your GPU. The function cuda::DeviceInfo::isCompatible returns the compatibility<br>status (true&#x2F;false).</p>
<h2 id="Utilizing-Multiple-GPUs"><a href="#Utilizing-Multiple-GPUs" class="headerlink" title="Utilizing Multiple GPUs"></a>Utilizing Multiple GPUs</h2><p>In the current version, each of the OpenCV CUDA algorithms can use only a single GPU. So, to utilize<br>multiple GPUs, you have to manually distribute the work between GPUs. Switching active device can be<br>done using cuda::setDevice() function. For more details please read Cuda C Programming Guide.</p>
<p>While developing algorithms for multiple GPUs, note a data passing overhead. For primitive functions<br>and small images, it can be significant, which may eliminate all the advantages of having multiple<br>GPUs. But for high-level algorithms, consider using multi-GPU acceleration. For example, the Stereo<br>Block Matching algorithm has been successfully parallelized using the following algorithm:</p>
<ol>
<li>Split each image of the stereo pair into two horizontal overlapping stripes.</li>
<li>Process each pair of stripes (from the left and right images) on a separate Fermi* GPU.</li>
<li>Merge the results into a single disparity map.</li>
</ol>
<p>With this algorithm, a dual GPU gave a 180% performance increase comparing to the single Fermi GPU.<br>For a source code example, see <a href="https://github.com/opencv/opencv/tree/3.4/samples/gpu/">https://github.com/opencv/opencv/tree/3.4/samples/gpu/</a>.</p>
