<h1 id="Machine-Learning-Overview-ml-intro"><a href="#Machine-Learning-Overview-ml-intro" class="headerlink" title="Machine Learning Overview {#ml_intro}"></a>Machine Learning Overview {#ml_intro}</h1><p>[TOC]</p>
<h1 id="Training-Data-ml-intro-data"><a href="#Training-Data-ml-intro-data" class="headerlink" title="Training Data {#ml_intro_data}"></a>Training Data {#ml_intro_data}</h1><p>In machine learning algorithms there is notion of training data. Training data includes several<br>components:</p>
<ul>
<li>A set of training samples. Each training sample is a vector of values (in Computer Vision it’s<br>sometimes referred to as feature vector). Usually all the vectors have the same number of<br>components (features); OpenCV ml module assumes that. Each feature can be ordered (i.e. its<br>values are floating-point numbers that can be compared with each other and strictly ordered,<br>i.e. sorted) or categorical (i.e. its value belongs to a fixed set of values that can be<br>integers, strings etc.).</li>
<li>Optional set of responses corresponding to the samples. Training data with no responses is used<br>in unsupervised learning algorithms that learn structure of the supplied data based on distances<br>between different samples. Training data with responses is used in supervised learning<br>algorithms, which learn the function mapping samples to responses. Usually the responses are<br>scalar values, ordered (when we deal with regression problem) or categorical (when we deal with<br>classification problem; in this case the responses are often called “labels”). Some algorithms,<br>most noticeably Neural networks, can handle not only scalar, but also multi-dimensional or<br>vector responses.</li>
<li>Another optional component is the mask of missing measurements. Most algorithms require all the<br>components in all the training samples be valid, but some other algorithms, such as decision<br>trees, can handle the cases of missing measurements.</li>
<li>In the case of classification problem user may want to give different weights to different<br>classes. This is useful, for example, when:<ul>
<li>user wants to shift prediction accuracy towards lower false-alarm rate or higher hit-rate.</li>
<li>user wants to compensate for significantly different amounts of training samples from<br>different classes.</li>
</ul>
</li>
<li>In addition to that, each training sample may be given a weight, if user wants the algorithm to<br>pay special attention to certain training samples and adjust the training model accordingly.</li>
<li>Also, user may wish not to use the whole training data at once, but rather use parts of it, e.g.<br>to do parameter optimization via cross-validation procedure.</li>
</ul>
<p>As you can see, training data can have rather complex structure; besides, it may be very big and&#x2F;or<br>not entirely available, so there is need to make abstraction for this concept. In OpenCV ml there is<br>cv::ml::TrainData class for that.</p>
<p>@sa cv::ml::TrainData</p>
<h1 id="Normal-Bayes-Classifier-ml-intro-bayes"><a href="#Normal-Bayes-Classifier-ml-intro-bayes" class="headerlink" title="Normal Bayes Classifier {#ml_intro_bayes}"></a>Normal Bayes Classifier {#ml_intro_bayes}</h1><p>This simple classification model assumes that feature vectors from each class are normally<br>distributed (though, not necessarily independently distributed). So, the whole data distribution<br>function is assumed to be a Gaussian mixture, one component per class. Using the training data the<br>algorithm estimates mean vectors and covariance matrices for every class, and then it uses them for<br>prediction.</p>
<p>@sa cv::ml::NormalBayesClassifier</p>
<h1 id="K-Nearest-Neighbors-ml-intro-knn"><a href="#K-Nearest-Neighbors-ml-intro-knn" class="headerlink" title="K-Nearest Neighbors {#ml_intro_knn}"></a>K-Nearest Neighbors {#ml_intro_knn}</h1><p>The algorithm caches all training samples and predicts the response for a new sample by analyzing a<br>certain number (<strong>K</strong>) of the nearest neighbors of the sample using voting, calculating weighted<br>sum, and so on. The method is sometimes referred to as “learning by example” because for prediction<br>it looks for the feature vector with a known response that is closest to the given vector.</p>
<p>@sa cv::ml::KNearest</p>
<h1 id="Support-Vector-Machines-ml-intro-svm"><a href="#Support-Vector-Machines-ml-intro-svm" class="headerlink" title="Support Vector Machines {#ml_intro_svm}"></a>Support Vector Machines {#ml_intro_svm}</h1><p>Originally, support vector machines (SVM) was a technique for building an optimal binary (2-class)<br>classifier. Later the technique was extended to regression and clustering problems. SVM is a partial<br>case of kernel-based methods. It maps feature vectors into a higher-dimensional space using a kernel<br>function and builds an optimal linear discriminating function in this space or an optimal hyper-<br>plane that fits into the training data. In case of SVM, the kernel is not defined explicitly.<br>Instead, a distance between any 2 points in the hyper-space needs to be defined.</p>
<p>The solution is optimal, which means that the margin between the separating hyper-plane and the<br>nearest feature vectors from both classes (in case of 2-class classifier) is maximal. The feature<br>vectors that are the closest to the hyper-plane are called <em>support vectors</em>, which means that the<br>position of other vectors does not affect the hyper-plane (the decision function).</p>
<p>SVM implementation in OpenCV is based on @cite LibSVM</p>
<p>@sa cv::ml::SVM</p>
<h2 id="Prediction-with-SVM-ml-intro-svm-predict"><a href="#Prediction-with-SVM-ml-intro-svm-predict" class="headerlink" title="Prediction with SVM {#ml_intro_svm_predict}"></a>Prediction with SVM {#ml_intro_svm_predict}</h2><p>StatModel::predict(samples, results, flags) should be used. Pass flags&#x3D;StatModel::RAW_OUTPUT to get<br>the raw response from SVM (in the case of regression, 1-class or 2-class classification problem).</p>
<h1 id="Decision-Trees-ml-intro-trees"><a href="#Decision-Trees-ml-intro-trees" class="headerlink" title="Decision Trees {#ml_intro_trees}"></a>Decision Trees {#ml_intro_trees}</h1><p>The ML classes discussed in this section implement Classification and Regression Tree algorithms<br>described in @cite Breiman84 .</p>
<p>The class cv::ml::DTrees represents a single decision tree or a collection of decision trees. It’s<br>also a base class for RTrees and Boost.</p>
<p>A decision tree is a binary tree (tree where each non-leaf node has two child nodes). It can be used<br>either for classification or for regression. For classification, each tree leaf is marked with a<br>class label; multiple leaves may have the same label. For regression, a constant is also assigned to<br>each tree leaf, so the approximation function is piecewise constant.</p>
<p>@sa cv::ml::DTrees</p>
<h2 id="Predicting-with-Decision-Trees-ml-intro-trees-predict"><a href="#Predicting-with-Decision-Trees-ml-intro-trees-predict" class="headerlink" title="Predicting with Decision Trees {#ml_intro_trees_predict}"></a>Predicting with Decision Trees {#ml_intro_trees_predict}</h2><p>To reach a leaf node and to obtain a response for the input feature vector, the prediction procedure<br>starts with the root node. From each non-leaf node the procedure goes to the left (selects the left<br>child node as the next observed node) or to the right based on the value of a certain variable whose<br>index is stored in the observed node. The following variables are possible:</p>
<ul>
<li><p><strong>Ordered variables.</strong> The variable value is compared with a threshold that is also stored in<br>the node. If the value is less than the threshold, the procedure goes to the left. Otherwise, it<br>goes to the right. For example, if the weight is less than 1 kilogram, the procedure goes to the<br>left, else to the right.</p>
</li>
<li><p><strong>Categorical variables.</strong> A discrete variable value is tested to see whether it belongs to a<br>certain subset of values (also stored in the node) from a limited set of values the variable<br>could take. If it does, the procedure goes to the left. Otherwise, it goes to the right. For<br>example, if the color is green or red, go to the left, else to the right.</p>
</li>
</ul>
<p>So, in each node, a pair of entities (variable_index , <code>decision_rule (threshold/subset)</code> ) is used.<br>This pair is called a <em>split</em> (split on the variable variable_index ). Once a leaf node is reached,<br>the value assigned to this node is used as the output of the prediction procedure.</p>
<p>Sometimes, certain features of the input vector are missed (for example, in the darkness it is<br>difficult to determine the object color), and the prediction procedure may get stuck in the certain<br>node (in the mentioned example, if the node is split by color). To avoid such situations, decision<br>trees use so-called <em>surrogate splits</em>. That is, in addition to the best “primary” split, every tree<br>node may also be split to one or more other variables with nearly the same results.</p>
<h2 id="Training-Decision-Trees-ml-intro-trees-train"><a href="#Training-Decision-Trees-ml-intro-trees-train" class="headerlink" title="Training Decision Trees {#ml_intro_trees_train}"></a>Training Decision Trees {#ml_intro_trees_train}</h2><p>The tree is built recursively, starting from the root node. All training data (feature vectors and<br>responses) is used to split the root node. In each node the optimum decision rule (the best<br>“primary” split) is found based on some criteria. In machine learning, gini “purity” criteria are<br>used for classification, and sum of squared errors is used for regression. Then, if necessary, the<br>surrogate splits are found. They resemble the results of the primary split on the training data. All<br>the data is divided using the primary and the surrogate splits (like it is done in the prediction<br>procedure) between the left and the right child node. Then, the procedure recursively splits both<br>left and right nodes. At each node the recursive procedure may stop (that is, stop splitting the<br>node further) in one of the following cases:</p>
<ul>
<li>Depth of the constructed tree branch has reached the specified maximum value.</li>
<li>Number of training samples in the node is less than the specified threshold when it is not<br>statistically representative to split the node further.</li>
<li>All the samples in the node belong to the same class or, in case of regression, the variation is<br>too small.</li>
<li>The best found split does not give any noticeable improvement compared to a random choice.</li>
</ul>
<p>When the tree is built, it may be pruned using a cross-validation procedure, if necessary. That is,<br>some branches of the tree that may lead to the model overfitting are cut off. Normally, this<br>procedure is only applied to standalone decision trees. Usually tree ensembles build trees that are<br>small enough and use their own protection schemes against overfitting.</p>
<h2 id="Variable-Importance-ml-intro-trees-var"><a href="#Variable-Importance-ml-intro-trees-var" class="headerlink" title="Variable Importance {#ml_intro_trees_var}"></a>Variable Importance {#ml_intro_trees_var}</h2><p>Besides the prediction that is an obvious use of decision trees, the tree can be also used for<br>various data analyses. One of the key properties of the constructed decision tree algorithms is an<br>ability to compute the importance (relative decisive power) of each variable. For example, in a spam<br>filter that uses a set of words occurred in the message as a feature vector, the variable importance<br>rating can be used to determine the most “spam-indicating” words and thus help keep the dictionary<br>size reasonable.</p>
<p>Importance of each variable is computed over all the splits on this variable in the tree, primary<br>and surrogate ones. Thus, to compute variable importance correctly, the surrogate splits must be<br>enabled in the training parameters, even if there is no missing data.</p>
<h1 id="Boosting-ml-intro-boost"><a href="#Boosting-ml-intro-boost" class="headerlink" title="Boosting {#ml_intro_boost}"></a>Boosting {#ml_intro_boost}</h1><p>A common machine learning task is supervised learning. In supervised learning, the goal is to learn<br>the functional relationship \f$F: y &#x3D; F(x)\f$ between the input \f$x\f$ and the output \f$y\f$ .<br>Predicting the qualitative output is called <em>classification</em>, while predicting the quantitative<br>output is called <em>regression</em>.</p>
<p>Boosting is a powerful learning concept that provides a solution to the supervised classification<br>learning task. It combines the performance of many “weak” classifiers to produce a powerful<br>committee @cite HTF01 . A weak classifier is only required to be better than chance, and thus can be<br>very simple and computationally inexpensive. However, many of them smartly combine results to a<br>strong classifier that often outperforms most “monolithic” strong classifiers such as SVMs and<br>Neural Networks.</p>
<p>Decision trees are the most popular weak classifiers used in boosting schemes. Often the simplest<br>decision trees with only a single split node per tree (called stumps ) are sufficient.</p>
<p>The boosted model is based on \f$N\f$ training examples \f${(x_i,y_i)}1N\f$ with \f$x_i \in{R^K}\f$<br>and \f$y_i \in{-1, +1}\f$ . \f$x_i\f$ is a \f$K\f$ -component vector. Each component encodes a<br>feature relevant to the learning task at hand. The desired two-class output is encoded as -1 and +1.</p>
<p>Different variants of boosting are known as Discrete Adaboost, Real AdaBoost, LogitBoost, and Gentle<br>AdaBoost @cite FHT98 . All of them are very similar in their overall structure. Therefore, this<br>chapter focuses only on the standard two-class Discrete AdaBoost algorithm, outlined below.<br>Initially the same weight is assigned to each sample (step 2). Then, a weak classifier<br>\f$f_{m(x)}\f$ is trained on the weighted training data (step 3a). Its weighted training error and<br>scaling factor \f$c_m\f$ is computed (step 3b). The weights are increased for training samples that<br>have been misclassified (step 3c). All weights are then normalized, and the process of finding the<br>next weak classifier continues for another \f$M\f$ -1 times. The final classifier \f$F(x)\f$ is the<br>sign of the weighted sum over the individual weak classifiers (step 4).</p>
<p><strong>Two-class Discrete AdaBoost Algorithm</strong></p>
<ul>
<li><p>Set \f$N\f$ examples \f${(x_i,y_i)}1N\f$ with \f$x_i \in{R^K}, y_i \in{-1, +1}\f$ .</p>
</li>
<li><p>Assign weights as \f$w_i &#x3D; 1&#x2F;N, i &#x3D; 1,…,N\f$ .</p>
</li>
<li><p>Repeat for \f$m &#x3D; 1,2,…,M\f$ :</p>
<ul>
<li><p>Fit the classifier \f$f_m(x) \in{-1,1}\f$, using weights \f$w_i\f$ on the training data.</p>
</li>
<li><p>Compute \f$err_m &#x3D; E_w [1_{(y \neq f_m(x))}], c_m &#x3D; log((1 - err_m)&#x2F;err_m)\f$ .</p>
</li>
<li><p>Set \f$w_i \Leftarrow w_i exp[c_m 1_{(y_i \neq f_m(x_i))}], i &#x3D; 1,2,…,N,\f$ and<br>renormalize so that \f$\Sigma i w_i &#x3D; 1\f$ .</p>
</li>
</ul>
</li>
<li><p>Classify new samples <em>x</em> using the formula: \f$\textrm{sign} (\Sigma m &#x3D; 1M c_m f_m(x))\f$ .</p>
</li>
</ul>
<p>@note Similar to the classical boosting methods, the current implementation supports two-class<br>classifiers only. For M &gt; 2 classes, there is the <strong>AdaBoost.MH</strong> algorithm (described in<br>@cite FHT98) that reduces the problem to the two-class problem, yet with a much larger training set.</p>
<p>To reduce computation time for boosted models without substantially losing accuracy, the influence<br>trimming technique can be employed. As the training algorithm proceeds and the number of trees in<br>the ensemble is increased, a larger number of the training samples are classified correctly and with<br>increasing confidence, thereby those samples receive smaller weights on the subsequent iterations.<br>Examples with a very low relative weight have a small impact on the weak classifier training. Thus,<br>such examples may be excluded during the weak classifier training without having much effect on the<br>induced classifier. This process is controlled with the weight_trim_rate parameter. Only examples<br>with the summary fraction weight_trim_rate of the total weight mass are used in the weak classifier<br>training. Note that the weights for <strong>all</strong> training examples are recomputed at each training<br>iteration. Examples deleted at a particular iteration may be used again for learning some of the<br>weak classifiers further @cite FHT98</p>
<p>@sa cv::ml::Boost</p>
<h2 id="Prediction-with-Boost-ml-intro-boost-predict"><a href="#Prediction-with-Boost-ml-intro-boost-predict" class="headerlink" title="Prediction with Boost {#ml_intro_boost_predict}"></a>Prediction with Boost {#ml_intro_boost_predict}</h2><p>StatModel::predict(samples, results, flags) should be used. Pass flags&#x3D;StatModel::RAW_OUTPUT to get<br>the raw sum from Boost classifier.</p>
<h1 id="Random-Trees-ml-intro-rtrees"><a href="#Random-Trees-ml-intro-rtrees" class="headerlink" title="Random Trees {#ml_intro_rtrees}"></a>Random Trees {#ml_intro_rtrees}</h1><p>Random trees have been introduced by Leo Breiman and Adele Cutler:<br><a href="http://www.stat.berkeley.edu/users/breiman/RandomForests/">http://www.stat.berkeley.edu/users/breiman/RandomForests/</a> . The algorithm can deal with both<br>classification and regression problems. Random trees is a collection (ensemble) of tree predictors<br>that is called <em>forest</em> further in this section (the term has been also introduced by L. Breiman).<br>The classification works as follows: the random trees classifier takes the input feature vector,<br>classifies it with every tree in the forest, and outputs the class label that received the majority<br>of “votes”. In case of a regression, the classifier response is the average of the responses over<br>all the trees in the forest.</p>
<p>All the trees are trained with the same parameters but on different training sets. These sets are<br>generated from the original training set using the bootstrap procedure: for each training set, you<br>randomly select the same number of vectors as in the original set ( &#x3D;N ). The vectors are chosen<br>with replacement. That is, some vectors will occur more than once and some will be absent. At each<br>node of each trained tree, not all the variables are used to find the best split, but a random<br>subset of them. With each node a new subset is generated. However, its size is fixed for all the<br>nodes and all the trees. It is a training parameter set to \f$\sqrt{number_of_variables}\f$ by<br>default. None of the built trees are pruned.</p>
<p>In random trees there is no need for any accuracy estimation procedures, such as cross-validation or<br>bootstrap, or a separate test set to get an estimate of the training error. The error is estimated<br>internally during the training. When the training set for the current tree is drawn by sampling with<br>replacement, some vectors are left out (so-called <em>oob (out-of-bag) data</em> ). The size of oob data is<br>about N&#x2F;3 . The classification error is estimated by using this oob-data as follows:</p>
<ul>
<li><p>Get a prediction for each vector, which is oob relative to the i-th tree, using the very i-th<br>tree.</p>
</li>
<li><p>After all the trees have been trained, for each vector that has ever been oob, find the<br>class-<em>winner</em> for it (the class that has got the majority of votes in the trees where<br>the vector was oob) and compare it to the ground-truth response.</p>
</li>
<li><p>Compute the classification error estimate as a ratio of the number of misclassified oob vectors<br>to all the vectors in the original data. In case of regression, the oob-error is computed as the<br>squared error for oob vectors difference divided by the total number of vectors.</p>
</li>
</ul>
<p>For the random trees usage example, please, see letter_recog.cpp sample in OpenCV distribution.</p>
<p>@sa cv::ml::RTrees</p>
<p><strong>References:</strong></p>
<ul>
<li><em>Machine Learning</em>, Wald I, July 2002.<br><a href="http://stat-www.berkeley.edu/users/breiman/wald2002-1.pdf">http://stat-www.berkeley.edu/users/breiman/wald2002-1.pdf</a></li>
<li><em>Looking Inside the Black Box</em>, Wald II, July 2002.<br><a href="http://stat-www.berkeley.edu/users/breiman/wald2002-2.pdf">http://stat-www.berkeley.edu/users/breiman/wald2002-2.pdf</a></li>
<li><em>Software for the Masses</em>, Wald III, July 2002.<br><a href="http://stat-www.berkeley.edu/users/breiman/wald2002-3.pdf">http://stat-www.berkeley.edu/users/breiman/wald2002-3.pdf</a></li>
<li>And other articles from the web site<br><a href="http://www.stat.berkeley.edu/users/breiman/RandomForests/cc_home.htm">http://www.stat.berkeley.edu/users/breiman/RandomForests/cc_home.htm</a></li>
</ul>
<h1 id="Expectation-Maximization-ml-intro-em"><a href="#Expectation-Maximization-ml-intro-em" class="headerlink" title="Expectation Maximization {#ml_intro_em}"></a>Expectation Maximization {#ml_intro_em}</h1><p>The Expectation Maximization(EM) algorithm estimates the parameters of the multivariate probability<br>density function in the form of a Gaussian mixture distribution with a specified number of mixtures.</p>
<p>Consider the set of the N feature vectors { \f$x_1, x_2,…,x_{N}\f$ } from a d-dimensional Euclidean<br>space drawn from a Gaussian mixture:</p>
<p>\f[p(x;a_k,S_k, \pi _k) &#x3D;  \sum _{k&#x3D;1}^{m} \pi _kp_k(x),  \quad \pi _k  \geq 0,  \quad \sum _{k&#x3D;1}^{m} \pi _k&#x3D;1,\f]</p>
<p>\f[p_k(x)&#x3D; \varphi (x;a_k,S_k)&#x3D; \frac{1}{(2\pi)^{d&#x2F;2}\mid{S_k}\mid^{1&#x2F;2}} exp \left { - \frac{1}{2} (x-a_k)^TS_k^{-1}(x-a_k) \right } ,\f]</p>
<p>where \f$m\f$ is the number of mixtures, \f$p_k\f$ is the normal distribution density with the mean<br>\f$a_k\f$ and covariance matrix \f$S_k\f$, \f$\pi_k\f$ is the weight of the k-th mixture. Given the<br>number of mixtures \f$M\f$ and the samples \f$x_i\f$, \f$i&#x3D;1..N\f$ the algorithm finds the maximum-<br>likelihood estimates (MLE) of all the mixture parameters, that is, \f$a_k\f$, \f$S_k\f$ and<br>\f$\pi_k\f$ :</p>
<p>\f[L(x, \theta )&#x3D;logp(x, \theta )&#x3D; \sum _{i&#x3D;1}^{N}log \left ( \sum _{k&#x3D;1}^{m} \pi _kp_k(x) \right ) \to \max _{ \theta \in \Theta },\f]</p>
<p>\f[\Theta &#x3D; \left { (a_k,S_k, \pi _k): a_k  \in \mathbbm{R} ^d,S_k&#x3D;S_k^T&gt;0,S_k  \in \mathbbm{R} ^{d  \times d}, \pi _k \geq 0, \sum _{k&#x3D;1}^{m} \pi _k&#x3D;1 \right } .\f]</p>
<p>The EM algorithm is an iterative procedure. Each iteration includes two steps. At the first step<br>(Expectation step or E-step), you find a probability \f$p_{i,k}\f$ (denoted \f$\alpha_{i,k}\f$ in<br>the formula below) of sample i to belong to mixture k using the currently available mixture<br>parameter estimates:</p>
<p>\f[\alpha <em>{ki} &#x3D;  \frac{\pi_k\varphi(x;a_k,S_k)}{\sum\limits</em>{j&#x3D;1}^{m}\pi_j\varphi(x;a_j,S_j)} .\f]</p>
<p>At the second step (Maximization step or M-step), the mixture parameter estimates are refined using<br>the computed probabilities:</p>
<p>\f[\pi <em>k&#x3D; \frac{1}{N} \sum <em>{i&#x3D;1}^{N} \alpha <em>{ki},  \quad a_k&#x3D; \frac{\sum\limits</em>{i&#x3D;1}^{N}\alpha</em>{ki}x_i}{\sum\limits</em>{i&#x3D;1}^{N}\alpha_{ki}} ,  \quad S_k&#x3D; \frac{\sum\limits_{i&#x3D;1}^{N}\alpha_{ki}(x_i-a_k)(x_i-a_k)^T}{\sum\limits_{i&#x3D;1}^{N}\alpha_{ki}}\f]</p>
<p>Alternatively, the algorithm may start with the M-step when the initial values for \f$p_{i,k}\f$ can<br>be provided. Another alternative when \f$p_{i,k}\f$ are unknown is to use a simpler clustering<br>algorithm to pre-cluster the input samples and thus obtain initial \f$p_{i,k}\f$ . Often (including<br>machine learning) the k-means algorithm is used for that purpose.</p>
<p>One of the main problems of the EM algorithm is a large number of parameters to estimate. The<br>majority of the parameters reside in covariance matrices, which are \f$d \times d\f$ elements each<br>where \f$d\f$ is the feature space dimensionality. However, in many practical problems, the<br>covariance matrices are close to diagonal or even to \f$\mu_k*I\f$ , where \f$I\f$ is an identity<br>matrix and \f$\mu_k\f$ is a mixture-dependent “scale” parameter. So, a robust computation scheme<br>could start with harder constraints on the covariance matrices and then use the estimated parameters<br>as an input for a less constrained optimization problem (often a diagonal covariance matrix is<br>already a good enough approximation).</p>
<p>@sa cv::ml::EM</p>
<p>References:</p>
<ul>
<li>Bilmes98 J. A. Bilmes. <em>A Gentle Tutorial of the EM Algorithm and its Application to Parameter<br>Estimation for Gaussian Mixture and Hidden Markov Models</em>. Technical Report TR-97-021,<br>International Computer Science Institute and Computer Science Division, University of California<br>at Berkeley, April 1998.</li>
</ul>
<h1 id="Neural-Networks-ml-intro-ann"><a href="#Neural-Networks-ml-intro-ann" class="headerlink" title="Neural Networks {#ml_intro_ann}"></a>Neural Networks {#ml_intro_ann}</h1><p>ML implements feed-forward artificial neural networks or, more particularly, multi-layer perceptrons<br>(MLP), the most commonly used type of neural networks. MLP consists of the input layer, output<br>layer, and one or more hidden layers. Each layer of MLP includes one or more neurons directionally<br>linked with the neurons from the previous and the next layer. The example below represents a 3-layer<br>perceptron with three inputs, two outputs, and the hidden layer including five neurons:</p>
<p><img src="/pics/mlp.png" alt="image"></p>
<p>All the neurons in MLP are similar. Each of them has several input links (it takes the output values<br>from several neurons in the previous layer as input) and several output links (it passes the<br>response to several neurons in the next layer). The values retrieved from the previous layer are<br>summed up with certain weights, individual for each neuron, plus the bias term. The sum is<br>transformed using the activation function \f$f\f$ that may be also different for different neurons.</p>
<p><img src="/pics/neuron_model.png" alt="image"></p>
<p>In other words, given the outputs \f$x_j\f$ of the layer \f$n\f$ , the outputs \f$y_i\f$ of the<br>layer \f$n+1\f$ are computed as:</p>
<p>\f[u_i &#x3D;  \sum <em>j (w^{n+1}</em>{i,j}*x_j) + w^{n+1}_{i,bias}\f]</p>
<p>\f[y_i &#x3D; f(u_i)\f]</p>
<p>Different activation functions may be used. ML implements three standard functions:</p>
<ul>
<li><p>Identity function ( cv::ml::ANN_MLP::IDENTITY ): \f$f(x)&#x3D;x\f$</p>
</li>
<li><p>Symmetrical sigmoid ( cv::ml::ANN_MLP::SIGMOID_SYM ): \f$f(x)&#x3D;\beta*(1-e^{-\alpha<br>x})&#x2F;(1+e^{-\alpha x}\f$ ), which is the default choice for MLP. The standard sigmoid with<br>\f$\beta &#x3D;1, \alpha &#x3D;1\f$ is shown below:</p>
<p><img src="/pics/sigmoid_bipolar.png" alt="image"></p>
</li>
<li><p>Gaussian function ( cv::ml::ANN_MLP::GAUSSIAN ): \f$f(x)&#x3D;\beta e^{-\alpha x*x}\f$ , which is not<br>completely supported at the moment.</p>
</li>
</ul>
<p>In ML, all the neurons have the same activation functions, with the same free parameters (<br>\f$\alpha, \beta\f$ ) that are specified by user and are not altered by the training algorithms.</p>
<p>So, the whole trained network works as follows:</p>
<ol>
<li>Take the feature vector as input. The vector size is equal to the size of the input layer.</li>
<li>Pass values as input to the first hidden layer.</li>
<li>Compute outputs of the hidden layer using the weights and the activation functions.</li>
<li>Pass outputs further downstream until you compute the output layer.</li>
</ol>
<p>So, to compute the network, you need to know all the weights \f$w^{n+1)}_{i,j}\f$ . The weights are<br>computed by the training algorithm. The algorithm takes a training set, multiple input vectors with<br>the corresponding output vectors, and iteratively adjusts the weights to enable the network to give<br>the desired response to the provided input vectors.</p>
<p>The larger the network size (the number of hidden layers and their sizes) is, the more the potential<br>network flexibility is. The error on the training set could be made arbitrarily small. But at the<br>same time the learned network also “learns” the noise present in the training set, so the error on<br>the test set usually starts increasing after the network size reaches a limit. Besides, the larger<br>networks are trained much longer than the smaller ones, so it is reasonable to pre-process the data,<br>using cv::PCA or similar technique, and train a smaller network on only essential features.</p>
<p>Another MLP feature is an inability to handle categorical data as is. However, there is a<br>workaround. If a certain feature in the input or output (in case of n -class classifier for<br>\f$n&gt;2\f$ ) layer is categorical and can take \f$M&gt;2\f$ different values, it makes sense to<br>represent it as a binary tuple of M elements, where the i -th element is 1 if and only if the<br>feature is equal to the i -th value out of M possible. It increases the size of the input&#x2F;output<br>layer but speeds up the training algorithm convergence and at the same time enables “fuzzy” values<br>of such variables, that is, a tuple of probabilities instead of a fixed value.</p>
<p>ML implements two algorithms for training MLP’s. The first algorithm is a classical random<br>sequential back-propagation algorithm. The second (default) one is a batch RPROP algorithm.</p>
<p>@sa cv::ml::ANN_MLP</p>
<h1 id="Logistic-Regression-ml-intro-lr"><a href="#Logistic-Regression-ml-intro-lr" class="headerlink" title="Logistic Regression {#ml_intro_lr}"></a>Logistic Regression {#ml_intro_lr}</h1><p>ML implements logistic regression, which is a probabilistic classification technique. Logistic<br>Regression is a binary classification algorithm which is closely related to Support Vector Machines<br>(SVM). Like SVM, Logistic Regression can be extended to work on multi-class classification problems<br>like digit recognition (i.e. recognizing digits like 0,1 2, 3,… from the given images). This<br>version of Logistic Regression supports both binary and multi-class classifications (for multi-class<br>it creates a multiple 2-class classifiers). In order to train the logistic regression classifier,<br>Batch Gradient Descent and Mini-Batch Gradient Descent algorithms are used (see<br><a href="http://en.wikipedia.org/wiki/Gradient_descent_optimization">http://en.wikipedia.org/wiki/Gradient_descent_optimization</a>). Logistic Regression is a<br>discriminative classifier (see <a href="http://www.cs.cmu.edu/~tom/NewChapters.html">http://www.cs.cmu.edu/~tom/NewChapters.html</a> for more details).<br>Logistic Regression is implemented as a C++ class in LogisticRegression.</p>
<p>In Logistic Regression, we try to optimize the training parameter \f$\theta\f$ such that the<br>hypothesis \f$0 \leq h_\theta(x) \leq 1\f$ is achieved. We have \f$h_\theta(x) &#x3D; g(h_\theta(x))\f$<br>and \f$g(z) &#x3D; \frac{1}{1+e^{-z}}\f$ as the logistic or sigmoid function. The term “Logistic” in<br>Logistic Regression refers to this function. For given data of a binary classification problem of<br>classes 0 and 1, one can determine that the given data instance belongs to class 1 if \f$h_\theta(x)<br>\geq 0.5\f$ or class 0 if \f$h_\theta(x) &lt; 0.5\f$ .</p>
<p>In Logistic Regression, choosing the right parameters is of utmost importance for reducing the<br>training error and ensuring high training accuracy:</p>
<ul>
<li><p>The learning rate can be set with @ref cv::ml::LogisticRegression::setLearningRate “setLearningRate”<br>method. It determines how fast we approach the solution. It is a positive real number.</p>
</li>
<li><p>Optimization algorithms like Batch Gradient Descent and Mini-Batch Gradient Descent are supported<br>in LogisticRegression. It is important that we mention the number of iterations these optimization<br>algorithms have to run. The number of iterations can be set with @ref<br>cv::ml::LogisticRegression::setIterations “setIterations”. This parameter can be thought<br>as number of steps taken and learning rate specifies if it is a long step or a short step. This<br>and previous parameter define how fast we arrive at a possible solution.</p>
</li>
<li><p>In order to compensate for overfitting regularization is performed, which can be enabled with<br>@ref cv::ml::LogisticRegression::setRegularization “setRegularization”. One can specify what<br>kind of regularization has to be performed by passing one of @ref<br>cv::ml::LogisticRegression::RegKinds “regularization kinds” to this method.</p>
</li>
<li><p>Logistic regression implementation provides a choice of 2 training methods with Batch Gradient<br>Descent or the MiniBatch Gradient Descent. To specify this, call @ref<br>cv::ml::LogisticRegression::setTrainMethod “setTrainMethod” with either @ref<br>cv::ml::LogisticRegression::BATCH “LogisticRegression::BATCH” or @ref<br>cv::ml::LogisticRegression::MINI_BATCH “LogisticRegression::MINI_BATCH”. If training method is<br>set to @ref cv::ml::LogisticRegression::MINI_BATCH “MINI_BATCH”, the size of the mini batch has<br>to be to a positive integer set with @ref cv::ml::LogisticRegression::setMiniBatchSize<br>“setMiniBatchSize”.</p>
</li>
</ul>
<p>A sample set of training parameters for the Logistic Regression classifier can be initialized as follows:<br>@snippet samples&#x2F;cpp&#x2F;logistic_regression.cpp init</p>
<p>@sa cv::ml::LogisticRegression</p>
