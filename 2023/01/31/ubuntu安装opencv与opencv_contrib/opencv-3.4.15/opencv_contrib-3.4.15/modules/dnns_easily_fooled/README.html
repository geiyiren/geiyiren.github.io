<h1 id="Fooling-Code"><a href="#Fooling-Code" class="headerlink" title="Fooling Code"></a>Fooling Code</h1><p>This is the code base used to reproduce the “fooling” images in the paper:<br><a href="http://anhnguyen.me/">Nguyen A</a>, <a href="http://yosinski.com/">Yosinski J</a>, <a href="http://jeffclune.com/">Clune J</a>. <a href="http://arxiv.org/abs/1412.1897">“Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images”</a>. In Computer Vision and Pattern Recognition (CVPR ‘15), IEEE, 2015.</p>
<p><strong>If you use this software in an academic article, please cite:</strong></p>
<pre><code>@inproceedings{nguyen2015deep,
  title={Deep Neural Networks are Easily Fooled: High Confidence Predictions for Unrecognizable Images},
  author={Nguyen, Anh and Yosinski, Jason and Clune, Jeff},
  booktitle={Computer Vision and Pattern Recognition (CVPR), 2015 IEEE Conference on},
  year={2015},
  organization={IEEE}
}
</code></pre>
<p>For more information regarding the paper, please visit <a href="http://www.evolvingai.org/fooling">www.evolvingai.org/fooling</a></p>
<h2 id="Requirements"><a href="#Requirements" class="headerlink" title="Requirements"></a>Requirements</h2><p>This is an installation process that requires two main software packages (included in this package):</p>
<ol>
<li>Caffe: <a href="http://caffe.berkeleyvision.org/">http://caffe.berkeleyvision.org</a></li>
</ol>
<ul>
<li>Our libraries installed to work with Caffe<ul>
<li>Cuda 6.0</li>
<li>Boost 1.52</li>
<li>g++ 4.6</li>
</ul>
</li>
<li>Use the provided scripts to download the correct version of Caffe for your experiments.<ul>
<li><code>./download_caffe_evolutionary_algorithm.sh</code> Caffe version for EA experiments</li>
<li><code>./download_caffe_gradient_ascent.sh</code> Caffe version for gradient ascent experiments</li>
</ul>
</li>
</ul>
<ol start="2">
<li>Sferes: <a href="https://github.com/jbmouret/sferes2">https://github.com/jbmouret/sferes2</a></li>
</ol>
<ul>
<li>Our libraries installed to work with Sferes<ul>
<li>OpenCV 2.4.10</li>
<li>Boost 1.52</li>
<li>g++ 4.9 (a C++ compiler compatible with C++11 standard)</li>
</ul>
</li>
<li>Use the provided script <code>./download_sferes.sh</code> to download the correct version of Sferes.</li>
</ul>
<p>Note: These are patched versions of the two frameworks with our additional work necessary to produce the images as in the paper. They are not the same as their master branches.</p>
<h2 id="Installation"><a href="#Installation" class="headerlink" title="Installation"></a>Installation</h2><p>Please see the <a href="https://github.com/anguyen8/opencv_contrib/blob/master/modules/dnns_easily_fooled/Installation_Guide.pdf">Installation_Guide</a> for more details.</p>
<h2 id="Usage"><a href="#Usage" class="headerlink" title="Usage"></a>Usage</h2><ul>
<li>An MNIST experiment (Fig. 4, 5 in the paper) can be run directly on a local machine (4-core) within a reasonable amount of time (around ~5 minutes or less for 200 generations).</li>
<li>An ImageNet experiment needs to be run on a cluster environment. It took us ~4 days x 128 cores to run 5000 generations and produce 1000 images (Fig. 8 in the paper).</li>
<li><a href="https://github.com/Evolving-AI-Lab/fooling/wiki/How-to-test-the-evolutionary-framework-quickly">How to configure an experiment to test the evolutionary framework quickly</a></li>
<li>To reproduce the gradient ascent fooling images (Figures 13, S3, S4, S5, S6, and S7 from the paper), see the <a href="https://github.com/anguyen8/opencv_contrib/tree/master/modules/dnns_easily_fooled/caffe/ascent">documentation in the caffe&#x2F;ascent directory</a>. You’ll need to download the correct Caffe version for this experiment using <code>./download_caffe_gradient_ascent.sh</code> script.</li>
</ul>
<h2 id="Troubleshooting"><a href="#Troubleshooting" class="headerlink" title="Troubleshooting"></a>Troubleshooting</h2><ol>
<li><p>If Sferes (Waf) can’t find your CUDA and Caffe dynamic libraries</p>
<blockquote>
<p>Add obj.libpath to the wscript for exp&#x2F;images to find libcudart and libcaffe or you can use LD_LIBRARY_PATH (for Linux).</p>
</blockquote>
</li>
<li><p>Is there a way to monitor the progress of the experiments?</p>
<blockquote>
<p>There is a flag for printing out results (fitness + images) every N generations.<br>You can adjust the dump_period setting <a href="https://github.com/Evolving-AI-Lab/fooling/blob/master/sferes/exp/images/dl_map_elites_images.cpp#L159">here</a>.</p>
</blockquote>
</li>
<li><p>Where do I get the pre-trained Caffe models?</p>
<blockquote>
<p>For AlexNet, please download on Caffe’s Model Zoo.<br>For LeNet, you can grab it <a href="https://github.com/anguyen8/opencv_contrib/tree/master/modules/dnns_easily_fooled/model/lenet">here</a>.</p>
</blockquote>
</li>
<li><p>How do I run the experiments on my local machine without MPI?</p>
<blockquote>
<p>You can enable MPI or non-MPI mode by commenting&#x2F;uncommenting a line <a href="https://github.com/Evolving-AI-Lab/fooling/blob/master/sferes/exp/images/dl_map_elites_images_mnist.cpp#L190-L191">here</a>. It can be simple eval::Eval (single-core), eval::Mpi (distributed for clusters).</p>
</blockquote>
</li>
</ol>
