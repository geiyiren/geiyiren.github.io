<p><img src="/images/2.jpg"></p>
<h1 id="Training-face-landmark-detector-tutorial-face-training-face-landmark-detector"><a href="#Training-face-landmark-detector-tutorial-face-training-face-landmark-detector" class="headerlink" title="Training face landmark detector{#tutorial_face_training_face_landmark_detector}"></a>Training face landmark detector{#tutorial_face_training_face_landmark_detector}</h1><p>This application helps to train your own face landmark detector. You can train your own face landmark detection by just providing the paths for<br>directory containing the images and files containing their corresponding face landmarks. As this landmark detector was originally trained on<br><a href="http://www.ifp.illinois.edu/~vuongle2/helen/">HELEN dataset</a>, the training follows the format of data provided in HELEN dataset.</p>
<p>The dataset consists of .txt files whose first line contains the image name which then follows the annotations.<br>The format of the file containing annotations should be of following format :</p>
<blockquote>
<pre><code>  /directory/images/abc.jpg
  123.45,345.65
  321.67,543.89
  .... , ....
  .... , ....
</code></pre>
<p>The above format is similar to HELEN dataset which is used for training the model.</p>
</blockquote>
<pre><code>// Command to be typed for running the sample
./sample_train_landmark_detector -annotations=/home/sukhad/Downloads/code/trainset/ -config=config.xml -face_cascade=lbpcascadefrontalface.xml -model=trained_model.dat -width=460 -height=460
</code></pre>
<h3 id="Description-of-command-parameters"><a href="#Description-of-command-parameters" class="headerlink" title="Description of command parameters"></a>Description of command parameters</h3><blockquote>
<ul>
<li><strong>annotations</strong> a : (REQUIRED) Path to annotations txt file [example - &#x2F;data&#x2F;annotations.txt]</li>
<li><strong>config</strong> c : (REQUIRED) Path to configuration xml file containing parameters for training.[ example - &#x2F;data&#x2F;config.xml]</li>
<li><strong>model</strong> m :  (REQUIRED) Path to configuration xml file containing parameters for training.[ example - &#x2F;data&#x2F;model.dat]</li>
<li><strong>width</strong> w : (OPTIONAL)  The width which you want all images to get to scale the annotations. Large images are slow to process [default &#x3D; 460]</li>
<li><strong>height</strong> h : (OPTIONAL) The height which you want all images to get to scale the annotations. Large images are slow to process [default &#x3D; 460]</li>
<li><strong>face_cascade</strong> f (REQUIRED) Path to the face cascade xml file which you want to use as a detector.</li>
</ul>
</blockquote>
<h3 id="Description-of-training-parameters"><a href="#Description-of-training-parameters" class="headerlink" title="Description of training parameters"></a>Description of training parameters</h3><p>The configuration file described above which is used while training contains the training parameters which are required for training.</p>
<p><strong>The description of parameters is as follows :</strong></p>
<ol>
<li><strong>Cascade depth :</strong> This stores the depth of cascade of regressors used for training.</li>
<li><strong>Tree depth :</strong> This stores the depth of trees created as weak learners during gradient boosting.</li>
<li><strong>Number of trees per cascade level :</strong> This stores number of trees required per cascade level.</li>
<li><strong>Learning rate :</strong> This stores the learning rate for gradient boosting.This is required to prevent overfitting using shrinkage.</li>
<li><strong>Oversampling amount :</strong> This stores the oversampling amount for the samples.</li>
<li><strong>Number of test coordinates :</strong> This stores number of test coordinates to be generated as samples to decide for making the split.</li>
<li><strong>Lambda :</strong> This stores the value used for calculating the probabilty which helps to select closer pixels for making the split.</li>
<li><strong>Number of test splits :</strong> This stores the number of test splits to be generated before making the best split.</li>
</ol>
<p>To get more detailed description about the training parameters you can refer to the <a href="https://pdfs.semanticscholar.org/d78b/6a5b0dcaa81b1faea5fb0000045a62513567.pdf">Research paper</a>.</p>
<h3 id="Understanding-code"><a href="#Understanding-code" class="headerlink" title="Understanding code"></a>Understanding code</h3><p><img src="/images/3.jpg"></p>
<p>Jumping directly to the code :</p>
<pre><code class="c++">CascadeClassifier face_cascade;
bool myDetector( InputArray image, OutputArray ROIs );

bool myDetector( InputArray image, OutputArray ROIs ){
    Mat gray;
    std::vector&lt;Rect&gt; faces;
    if(image.channels()&gt;1){
        cvtColor(image.getMat(),gray,COLOR_BGR2GRAY);
    }
    else{
        gray = image.getMat().clone();
    }
    equalizeHist( gray, gray );
    face_cascade.detectMultiScale( gray, faces, 1.1, 3,0, Size(30, 30) );
    Mat(faces).copyTo(ROIs);
    return true;
}
</code></pre>
<p>The facemark API provides the functionality to the user to use their own face detector to be used in training.The above code creartes a sample face detector. The above function would be passed to a function pointer in the facemark API.</p>
<pre><code class="c++">vector&lt;String&gt; filenames;
glob(directory,filenames);
</code></pre>
<p>The above code creates a vector filenames for storing the names of the .txt files.<br>It gets the filenames of the files in the directory.</p>
<pre><code class="c++">Mat img = imread(image);
face_cascade.load(cascade_name);
FacemarkKazemi::Params params;
params.configfile = configfile_name;
Ptr&lt;Facemark&gt; facemark = FacemarkKazemi::create(params);
facemark-&gt;setFaceDetector(myDetector);
</code></pre>
<p>The above code creates a pointer of the face landmark detection class. The face detector created above has to be passed<br>as function pointer to the facemark pointer created for detecting faces while training the model.</p>
<pre><code class="c++">vector&lt;String&gt; imagenames;
vector&lt; vector&lt;Point2f&gt; &gt; trainlandmarks,Trainlandmarks;
vector&lt;Mat&gt; trainimages;
loadTrainingData(filenames,trainlandmarks,imagenames);
for(unsigned long i=0;i&lt;300;i++){
string imgname = imagenames[i].substr(0, imagenames[i].size()-1);
string img = directory + string(imgname) + &quot;.jpg&quot;;
Mat src = imread(img);
if(src.empty()){
    cerr&lt;&lt;string(&quot;Image &quot;+img+&quot; not found\n.&quot;)&lt;&lt;endl;
    continue;
}
trainimages.push_back(src);
Trainlandmarks.push_back(trainlandmarks[i]);
}
</code></pre>
<p>The above code creates std::vectors to store the images and their corresponding landmarks.<br>The above code calls a function loadTrainingData to load the landmarks and the images into their respective vectors.</p>
<p>If the dataset you downloaded is of the following format :</p>
<pre><code>version: 1
n_points:  68
{
 115.167660 220.807529
 116.164839 245.721357
 120.208690 270.389841
  ...
}
This is the example of the dataset available at https://ibug.doc.ic.ac.uk/resources/facial-point-annotations/
</code></pre>
<p>Then skip the above code for loading training data and use the following code. This sample is provided as sampleTrainLandmarkDetector2.cpp<br>in the face module in opencv contrib.</p>
<pre><code class="c++">std::vector&lt;String&gt; images;
std::vector&lt;std::vector&lt;Point2f&gt; &gt; facePoints;
loadTrainingData(imagesList, annotations, images, facePoints, 0.0);
</code></pre>
<p>In the above code imagelist and annotations are the file of following format :</p>
<pre><code>example of contents for images.txt:
../trainset/image_0001.png
../trainset/image_0002.png
example of contents for annotation.txt:
../trainset/image_0001.pts
../trainset/image_0002.pts
</code></pre>
<p>These symbolize the names of images and their corresponding annotations.</p>
<p>The above code scales images and landmarks as training on images of smaller size takes less time.<br>This is because processing larger images requires more time. After scaling data it calculates mean<br>shape of the data which is used as initial shape while training.</p>
<p>Finally call the following function to perform training :</p>
<pre><code class="c++">facemark-&gt;training(Trainimages,Trainlandmarks,configfile_name,scale,modelfile_name);
</code></pre>
<p>In the above function scale is passed to scale all images and the corresponding landmarks so that the size of all<br>images can be reduced as it takes greater time to process large images.<br>This call to the train function trains the model and stores the trained model file with the given<br>filename specified.As the training starts successfully you will see something like this :<br><img src="/images/train1.png"></p>
<p><strong>The error rate on trained images depends on the number of images used for training used as follows :</strong></p>
<p><img src="/images/train.png"></p>
<p><strong>The error rate on test images depends on the number of images used for training used as follows :</strong></p>
<p><img src="/images/test.png"></p>
