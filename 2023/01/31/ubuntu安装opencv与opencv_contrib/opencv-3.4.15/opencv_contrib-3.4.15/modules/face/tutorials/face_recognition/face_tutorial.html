<h1 id="Face-Recognition-with-OpenCV-tutorial-face-main"><a href="#Face-Recognition-with-OpenCV-tutorial-face-main" class="headerlink" title="Face Recognition with OpenCV {#tutorial_face_main}"></a>Face Recognition with OpenCV {#tutorial_face_main}</h1><p>[TOC]</p>
<h1 id="Introduction-tutorial-face-intro"><a href="#Introduction-tutorial-face-intro" class="headerlink" title="Introduction {#tutorial_face_intro}"></a>Introduction {#tutorial_face_intro}</h1><p><a href="http://opencv.org/">OpenCV (Open Source Computer Vision)</a> is a popular computer vision library<br>started by <a href="http://www.intel.com/">Intel</a> in 1999. The cross-platform library sets its focus on<br>real-time image processing and includes patent-free implementations of the latest computer vision<br>algorithms. In 2008 <a href="http://www.willowgarage.com/">Willow Garage</a> took over support and OpenCV 2.3.1<br>now comes with a programming interface to C, C++, <a href="http://www.python.org/">Python</a> and<br><a href="http://www.android.com/">Android</a>. OpenCV is released under a BSD license so it is used in academic<br>projects and commercial products alike.</p>
<p>OpenCV 2.4 now comes with the very new FaceRecognizer class for face recognition, so you can start<br>experimenting with face recognition right away. This document is the guide I’ve wished for, when I<br>was working myself into face recognition. It shows you how to perform face recognition with<br>FaceRecognizer in OpenCV (with full source code listings) and gives you an introduction into the<br>algorithms behind. I’ll also show how to create the visualizations you can find in many<br>publications, because a lot of people asked for.</p>
<p>The currently available algorithms are:</p>
<ul>
<li>Eigenfaces (see EigenFaceRecognizer::create)</li>
<li>Fisherfaces (see FisherFaceRecognizer::create)</li>
<li>Local Binary Patterns Histograms (see LBPHFaceRecognizer::create)</li>
</ul>
<p>You don’t need to copy and paste the source code examples from this page, because they are available<br>in the src folder coming with this documentation. If you have built OpenCV with the samples turned<br>on, chances are good you have them compiled already! Although it might be interesting for very<br>advanced users, I’ve decided to leave the implementation details out as I am afraid they confuse new<br>users.</p>
<p>All code in this document is released under the <a href="http://www.opensource.org/licenses/bsd-license">BSD<br>license</a>, so feel free to use it for your projects.</p>
<h2 id="Face-Recognition-tutorial-face-facerec"><a href="#Face-Recognition-tutorial-face-facerec" class="headerlink" title="Face Recognition {#tutorial_face_facerec}"></a>Face Recognition {#tutorial_face_facerec}</h2><p>Face recognition is an easy task for humans. Experiments in @cite Tu06 have shown, that even one to<br>three day old babies are able to distinguish between known faces. So how hard could it be for a<br>computer? It turns out we know little about human recognition to date. Are inner features (eyes,<br>nose, mouth) or outer features (head shape, hairline) used for a successful face recognition? How do<br>we analyze an image and how does the brain encode it? It was shown by <a href="http://en.wikipedia.org/wiki/David_H._Hubel">David<br>Hubel</a> and <a href="http://en.wikipedia.org/wiki/Torsten_Wiesel">Torsten<br>Wiesel</a>, that our brain has specialized nerve cells<br>responding to specific local features of a scene, such as lines, edges, angles or movement. Since we<br>don’t see the world as scattered pieces, our visual cortex must somehow combine the different<br>sources of information into useful patterns. Automatic face recognition is all about extracting<br>those meaningful features from an image, putting them into a useful representation and performing<br>some kind of classification on them.</p>
<p>Face recognition based on the geometric features of a face is probably the most intuitive approach<br>to face recognition. One of the first automated face recognition systems was described in<br>@cite Kanade73 : marker points (position of eyes, ears, nose, …) were used to build a feature vector<br>(distance between the points, angle between them, …). The recognition was performed by calculating<br>the euclidean distance between feature vectors of a probe and reference image. Such a method is<br>robust against changes in illumination by its nature, but has a huge drawback: the accurate<br>registration of the marker points is complicated, even with state of the art algorithms. Some of the<br>latest work on geometric face recognition was carried out in @cite Bru92 . A 22-dimensional feature<br>vector was used and experiments on large datasets have shown, that geometrical features alone may not<br>carry enough information for face recognition.</p>
<p>The Eigenfaces method described in @cite TP91 took a holistic approach to face recognition: A facial<br>image is a point from a high-dimensional image space and a lower-dimensional representation is<br>found, where classification becomes easy. The lower-dimensional subspace is found with Principal<br>Component Analysis, which identifies the axes with maximum variance. While this kind of<br>transformation is optimal from a reconstruction standpoint, it doesn’t take any class labels into<br>account. Imagine a situation where the variance is generated from external sources, let it be light.<br>The axes with maximum variance do not necessarily contain any discriminative information at all,<br>hence a classification becomes impossible. So a class-specific projection with a Linear Discriminant<br>Analysis was applied to face recognition in @cite BHK97 . The basic idea is to minimize the variance<br>within a class, while maximizing the variance between the classes at the same time.</p>
<p>Recently various methods for a local feature extraction emerged. To avoid the high-dimensionality of<br>the input data only local regions of an image are described, the extracted features are (hopefully)<br>more robust against partial occlusion, illumation and small sample size. Algorithms used for a local<br>feature extraction are Gabor Wavelets (@cite Wiskott97), Discrete Cosinus Transform (@cite Messer06) and<br>Local Binary Patterns (@cite AHP04). It’s still an open research question what’s the best way to<br>preserve spatial information when applying a local feature extraction, because spatial information<br>is potentially useful information.</p>
<h2 id="Face-Database-tutorial-face-facedb"><a href="#Face-Database-tutorial-face-facedb" class="headerlink" title="Face Database  {#tutorial_face_facedb}"></a>Face Database  {#tutorial_face_facedb}</h2><p>Let’s get some data to experiment with first. I don’t want to do a toy example here. We are doing<br>face recognition, so you’ll need some face images! You can either create your own dataset or start<br>with one of the available face databases,<br><a href="http://face-rec.org/databases"><a href="http://face-rec.org/databases/">http://face-rec.org/databases/</a></a> gives you an up-to-date overview.<br>Three interesting databases are (parts of the description are quoted from<br><a href="http://face-rec.org/"><a href="http://face-rec.org/">http://face-rec.org</a></a>):</p>
<ul>
<li><p><a href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html">AT&amp;T Facedatabase</a> The AT&amp;T<br>Facedatabase, sometimes also referred to as <em>ORL Database of Faces</em>, contains ten different<br>images of each of 40 distinct subjects. For some subjects, the images were taken at different<br>times, varying the lighting, facial expressions (open &#x2F; closed eyes, smiling &#x2F; not smiling) and<br>facial details (glasses &#x2F; no glasses). All the images were taken against a dark homogeneous<br>background with the subjects in an upright, frontal position (with tolerance for some side<br>movement).</p>
</li>
<li><p><a href="http://vision.ucsd.edu/content/yale-face-database">Yale Facedatabase A</a>, also known as<br>Yalefaces. The AT&amp;T Facedatabase is good for initial tests, but it’s a fairly easy database. The<br>Eigenfaces method already has a 97% recognition rate on it, so you won’t see any great<br>improvements with other algorithms. The Yale Facedatabase A (also known as Yalefaces) is a more<br>appropriate dataset for initial experiments, because the recognition problem is harder. The<br>database consists of 15 people (14 male, 1 female) each with 11 grayscale images sized<br>\f$320 \times 243\f$ pixel. There are changes in the light conditions (center light, left light,<br>right light), facial expressions (happy, normal, sad, sleepy, surprised, wink) and glasses<br>(glasses, no-glasses).</p>
<p>The original images are not cropped and aligned. Please look into the @ref face_appendix for a<br>Python script, that does the job for you.</p>
</li>
<li><p><a href="http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html">Extended Yale Facedatabase B</a> The<br>Extended Yale Facedatabase B contains 2414 images of 38 different people in its cropped version.<br>The focus of this database is set on extracting features that are robust to illumination, the<br>images have almost no variation in emotion&#x2F;occlusion&#x2F;… . I personally think, that this dataset<br>is too large for the experiments I perform in this document. You better use the <a href="http://www.cl.cam.ac.uk/research/dtg/attarchive/facedatabase.html">AT&amp;T<br>Facedatabase</a> for intial<br>testing. A first version of the Yale Facedatabase B was used in @cite BHK97 to see how the<br>Eigenfaces and Fisherfaces method perform under heavy illumination changes. @cite Lee05 used the<br>same setup to take 16128 images of 28 people. The Extended Yale Facedatabase B is the merge of<br>the two databases, which is now known as Extended Yalefacedatabase B.</p>
</li>
</ul>
<h3 id="Preparing-the-data-tutorial-face-prepare"><a href="#Preparing-the-data-tutorial-face-prepare" class="headerlink" title="Preparing the data  {#tutorial_face_prepare}"></a>Preparing the data  {#tutorial_face_prepare}</h3><p>Once we have acquired some data, we’ll need to read it in our program. In the demo applications I<br>have decided to read the images from a very simple CSV file. Why? Because it’s the simplest<br>platform-independent approach I can think of. However, if you know a simpler solution please ping me<br>about it. Basically all the CSV file needs to contain are lines composed of a filename followed by a<br>; followed by the label (as <em>integer number</em>), making up a line like this:</p>
<p>@code{.csv}<br>&#x2F;path&#x2F;to&#x2F;image.ext;0<br>@endcode</p>
<p>Let’s dissect the line. &#x2F;path&#x2F;to&#x2F;image.ext is the path to an image, probably something like this if<br>you are in Windows: C:&#x2F;faces&#x2F;person0&#x2F;image0.jpg. Then there is the separator ; and finally we assign<br>the label 0 to the image. Think of the label as the subject (the person) this image belongs to, so<br>same subjects (persons) should have the same label.</p>
<p>Download the AT&amp;T Facedatabase from AT&amp;T Facedatabase and the corresponding CSV file from at.txt,<br>which looks like this (file is without … of course):</p>
<p>@code{.csv}<br>.&#x2F;at&#x2F;s1&#x2F;1.pgm;0<br>.&#x2F;at&#x2F;s1&#x2F;2.pgm;0<br>…<br>.&#x2F;at&#x2F;s2&#x2F;1.pgm;1<br>.&#x2F;at&#x2F;s2&#x2F;2.pgm;1<br>…<br>.&#x2F;at&#x2F;s40&#x2F;1.pgm;39<br>.&#x2F;at&#x2F;s40&#x2F;2.pgm;39<br>@endcode</p>
<p>Imagine I have extracted the files to D:&#x2F;data&#x2F;at and have downloaded the CSV file to D:&#x2F;data&#x2F;at.txt.<br>Then you would simply need to Search &amp; Replace .&#x2F; with D:&#x2F;data&#x2F;. You can do that in an editor of<br>your choice, every sufficiently advanced editor can do this. Once you have a CSV file with valid<br>filenames and labels, you can run any of the demos by passing the path to the CSV file as parameter:</p>
<p>@code{.sh}<br>facerec_demo.exe D:&#x2F;data&#x2F;at.txt<br>@endcode</p>
<p>Please, see @ref tutorial_face_appendix_csv for details on creating CSV file.</p>
<h2 id="Eigenfaces-tutorial-face-eigenfaces"><a href="#Eigenfaces-tutorial-face-eigenfaces" class="headerlink" title="Eigenfaces  {#tutorial_face_eigenfaces}"></a>Eigenfaces  {#tutorial_face_eigenfaces}</h2><p>The problem with the image representation we are given is its high dimensionality. Two-dimensional<br>\f$p \times q\f$ grayscale images span a \f$m &#x3D; pq\f$-dimensional vector space, so an image with<br>\f$100 \times 100\f$ pixels lies in a \f$10,000\f$-dimensional image space already. The question is: Are all<br>dimensions equally useful for us? We can only make a decision if there’s any variance in data, so<br>what we are looking for are the components that account for most of the information. The Principal<br>Component Analysis (PCA) was independently proposed by <a href="http://en.wikipedia.org/wiki/Karl_Pearson">Karl<br>Pearson</a> (1901) and <a href="http://en.wikipedia.org/wiki/Harold_Hotelling">Harold<br>Hotelling</a> (1933) to turn a set of possibly<br>correlated variables into a smaller set of uncorrelated variables. The idea is, that a<br>high-dimensional dataset is often described by correlated variables and therefore only a few<br>meaningful dimensions account for most of the information. The PCA method finds the directions with<br>the greatest variance in the data, called principal components.</p>
<h3 id="Algorithmic-Description-of-Eigenfaces-method-tutorial-face-eigenfaces-algo"><a href="#Algorithmic-Description-of-Eigenfaces-method-tutorial-face-eigenfaces-algo" class="headerlink" title="Algorithmic Description of Eigenfaces method  {#tutorial_face_eigenfaces_algo}"></a>Algorithmic Description of Eigenfaces method  {#tutorial_face_eigenfaces_algo}</h3><p>Let \f$X &#x3D; { x_{1}, x_{2}, \ldots, x_{n} }\f$ be a random vector with observations \f$x_i \in R^{d}\f$.</p>
<ol>
<li>Compute the mean \f$\mu\f$</li>
</ol>
<p>\f[\mu &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} x_{i}\f]</p>
<ol start="2">
<li>Compute the the Covariance Matrix S</li>
</ol>
<p>\f[S &#x3D; \frac{1}{n} \sum_{i&#x3D;1}^{n} (x_{i} - \mu) (x_{i} - \mu)^{T}&#96;\f]</p>
<ol start="3">
<li>Compute the eigenvalues \f$\lambda_{i}\f$ and eigenvectors \f$v_{i}\f$ of \f$S\f$</li>
</ol>
<p>\f[S v_{i} &#x3D; \lambda_{i} v_{i}, i&#x3D;1,2,\ldots,n\f]</p>
<ol start="4">
<li>Order the eigenvectors descending by their eigenvalue. The \f$k\f$ principal components are the<br>eigenvectors corresponding to the \f$k\f$ largest eigenvalues.</li>
</ol>
<p>The \f$k\f$ principal components of the observed vector \f$x\f$ are then given by:</p>
<p>\f[y &#x3D; W^{T} (x - \mu)\f]</p>
<p>where \f$W &#x3D; (v_{1}, v_{2}, \ldots, v_{k})\f$.</p>
<p>The reconstruction from the PCA basis is given by:</p>
<p>\f[x &#x3D; W y + \mu\f]</p>
<p>where \f$W &#x3D; (v_{1}, v_{2}, \ldots, v_{k})\f$.</p>
<p>The Eigenfaces method then performs face recognition by:</p>
<ul>
<li>Projecting all training samples into the PCA subspace.</li>
<li>Projecting the query image into the PCA subspace.</li>
<li>Finding the nearest neighbor between the projected training images and the projected query<br>image.</li>
</ul>
<p>Still there’s one problem left to solve. Imagine we are given \f$400\f$ images sized \f$100 \times 100\f$<br>pixel. The Principal Component Analysis solves the covariance matrix \f$S &#x3D; X X^{T}\f$, where<br>\f${size}(X) &#x3D; 10000 \times 400\f$ in our example. You would end up with a \f$10000 \times 10000\f$ matrix,<br>roughly \f$0.8 GB\f$. Solving this problem isn’t feasible, so we’ll need to apply a trick. From your<br>linear algebra lessons you know that a \f$M \times N\f$ matrix with \f$M &gt; N\f$ can only have \f$N - 1\f$<br>non-zero eigenvalues. So it’s possible to take the eigenvalue decomposition \f$S &#x3D; X^{T} X\f$ of size<br>\f$N \times N\f$ instead:</p>
<p>\f[X^{T} X v_{i} &#x3D; \lambda_{i} v{i}\f]</p>
<p>and get the original eigenvectors of \f$S &#x3D; X X^{T}\f$ with a left multiplication of the data matrix:</p>
<p>\f[X X^{T} (X v_{i}) &#x3D; \lambda_{i} (X v_{i})\f]</p>
<p>The resulting eigenvectors are orthogonal, to get orthonormal eigenvectors they need to be<br>normalized to unit length. I don’t want to turn this into a publication, so please look into<br>@cite Duda01 for the derivation and proof of the equations.</p>
<h3 id="Eigenfaces-in-OpenCV-tutorial-face-eigenfaces-use"><a href="#Eigenfaces-in-OpenCV-tutorial-face-eigenfaces-use" class="headerlink" title="Eigenfaces in OpenCV  {#tutorial_face_eigenfaces_use}"></a>Eigenfaces in OpenCV  {#tutorial_face_eigenfaces_use}</h3><p>For the first source code example, I’ll go through it with you. I am first giving you the whole<br>source code listing, and after this we’ll look at the most important lines in detail. Please note:<br>every source code listing is commented in detail, so you should have no problems following it.</p>
<p>The source code for this demo application is also available in the src folder coming with this<br>documentation:</p>
<p>@include face&#x2F;samples&#x2F;facerec_eigenfaces.cpp</p>
<p>I’ve used the jet colormap, so you can see how the grayscale values are distributed within the<br>specific Eigenfaces. You can see, that the Eigenfaces do not only encode facial features, but also<br>the illumination in the images (see the left light in Eigenface #4, right light in Eigenfaces #5):</p>
<p><img src="/img/eigenfaces_opencv.png" alt="image"></p>
<p>We’ve already seen, that we can reconstruct a face from its lower dimensional approximation. So<br>let’s see how many Eigenfaces are needed for a good reconstruction. I’ll do a subplot with<br>\f$10,30,\ldots,310\f$ Eigenfaces:</p>
<p>@code{.cpp}<br>&#x2F;&#x2F; Display or save the image reconstruction at some predefined steps:<br>for(int num_components &#x3D; 10; num_components &lt; 300; num_components+&#x3D;15) {<br>    &#x2F;&#x2F; slice the eigenvectors from the model<br>    Mat evs &#x3D; Mat(W, Range::all(), Range(0, num_components));<br>    Mat projection &#x3D; LDA::subspaceProject(evs, mean, images[0].reshape(1,1));<br>    Mat reconstruction &#x3D; LDA::subspaceReconstruct(evs, mean, projection);<br>    &#x2F;&#x2F; Normalize the result:<br>    reconstruction &#x3D; norm_0_255(reconstruction.reshape(1, images[0].rows));<br>    &#x2F;&#x2F; Display or save:<br>    if(argc &#x3D;&#x3D; 2) {<br>        imshow(format(“eigenface_reconstruction_%d”, num_components), reconstruction);<br>    } else {<br>        imwrite(format(“%s&#x2F;eigenface_reconstruction_%d.png”, output_folder.c_str(), num_components), reconstruction);<br>    }<br>}<br>@endcode</p>
<p>10 Eigenvectors are obviously not sufficient for a good image reconstruction, 50 Eigenvectors may<br>already be sufficient to encode important facial features. You’ll get a good reconstruction with<br>approximately 300 Eigenvectors for the AT&amp;T Facedatabase. There are rule of thumbs how many<br>Eigenfaces you should choose for a successful face recognition, but it heavily depends on the input<br>data. @cite Zhao03 is the perfect point to start researching for this:</p>
<p><img src="/img/eigenface_reconstruction_opencv.png" alt="image"></p>
<h2 id="Fisherfaces-tutorial-face-fisherfaces"><a href="#Fisherfaces-tutorial-face-fisherfaces" class="headerlink" title="Fisherfaces  {#tutorial_face_fisherfaces}"></a>Fisherfaces  {#tutorial_face_fisherfaces}</h2><p>The Principal Component Analysis (PCA), which is the core of the Eigenfaces method, finds a linear<br>combination of features that maximizes the total variance in data. While this is clearly a powerful<br>way to represent data, it doesn’t consider any classes and so a lot of discriminative information<br><em>may</em> be lost when throwing components away. Imagine a situation where the variance in your data is<br>generated by an external source, let it be the light. The components identified by a PCA do not<br>necessarily contain any discriminative information at all, so the projected samples are smeared<br>together and a classification becomes impossible (see<br><a href="http://www.bytefish.de/wiki/pca_lda_with_gnu_octave"><a href="http://www.bytefish.de/wiki/pca_lda_with_gnu_octave">http://www.bytefish.de/wiki/pca_lda_with_gnu_octave</a></a><br>for an example).</p>
<p>The Linear Discriminant Analysis performs a class-specific dimensionality reduction and was invented<br>by the great statistician <a href="http://en.wikipedia.org/wiki/Ronald_Fisher">Sir R. A. Fisher</a>. He<br>successfully used it for classifying flowers in his 1936 paper <em>The use of multiple measurements in<br>taxonomic problems</em> @cite Fisher36 . In order to find the combination of features that separates best<br>between classes the Linear Discriminant Analysis maximizes the ratio of between-classes to<br>within-classes scatter, instead of maximizing the overall scatter. The idea is simple: same classes<br>should cluster tightly together, while different classes are as far away as possible from each other<br>in the lower-dimensional representation. This was also recognized by<br><a href="http://www.cs.columbia.edu/~belhumeur/">Belhumeur</a>, <a href="http://www.ece.ucsb.edu/~hespanha/">Hespanha</a><br>and <a href="http://cseweb.ucsd.edu/~kriegman/">Kriegman</a> and so they applied a Discriminant Analysis to<br>face recognition in @cite BHK97 .</p>
<h3 id="Algorithmic-Description-of-Fisherfaces-method-tutorial-face-fisherfaces-algo"><a href="#Algorithmic-Description-of-Fisherfaces-method-tutorial-face-fisherfaces-algo" class="headerlink" title="Algorithmic Description of Fisherfaces method {#tutorial_face_fisherfaces_algo}"></a>Algorithmic Description of Fisherfaces method {#tutorial_face_fisherfaces_algo}</h3><p>Let \f$X\f$ be a random vector with samples drawn from \f$c\f$ classes:</p>
<p>\f[\begin{align*}<br>    X &amp; &#x3D; &amp; {X_1,X_2,\ldots,X_c} \<br>    X_i &amp; &#x3D; &amp; {x_1, x_2, \ldots, x_n}<br>\end{align*}\f]</p>
<p>The scatter matrices \f$S_{B}\f$ and S_{W} are calculated as:</p>
<p>\f[\begin{align*}<br>    S_{B} &amp; &#x3D; &amp; \sum_{i&#x3D;1}^{c} N_{i} (\mu_i - \mu)(\mu_i - \mu)^{T} \<br>    S_{W} &amp; &#x3D; &amp; \sum_{i&#x3D;1}^{c} \sum_{x_{j} \in X_{i}} (x_j - \mu_i)(x_j - \mu_i)^{T}<br>\end{align*}\f]</p>
<p>, where \f$\mu\f$ is the total mean:</p>
<p>\f[\mu &#x3D; \frac{1}{N} \sum_{i&#x3D;1}^{N} x_i\f]</p>
<p>And \f$\mu_i\f$ is the mean of class \f$i \in {1,\ldots,c}\f$:</p>
<p>\f[\mu_i &#x3D; \frac{1}{|X_i|} \sum_{x_j \in X_i} x_j\f]</p>
<p>Fisher’s classic algorithm now looks for a projection \f$W\f$, that maximizes the class separability<br>criterion:</p>
<p>\f[W_{opt} &#x3D; \operatorname{arg,max}_{W} \frac{|W^T S_B W|}{|W^T S_W W|}\f]</p>
<p>Following @cite BHK97, a solution for this optimization problem is given by solving the General<br>Eigenvalue Problem:</p>
<p>\f[\begin{align*}<br>    S_{B} v_{i} &amp; &#x3D; &amp; \lambda_{i} S_w v_{i} \nonumber \<br>    S_{W}^{-1} S_{B} v_{i} &amp; &#x3D; &amp; \lambda_{i} v_{i}<br>\end{align*}\f]</p>
<p>There’s one problem left to solve: The rank of \f$S_{W}\f$ is at most \f$(N-c)\f$, with \f$N\f$ samples and \f$c\f$<br>classes. In pattern recognition problems the number of samples \f$N\f$ is almost always samller than the<br>dimension of the input data (the number of pixels), so the scatter matrix \f$S_{W}\f$ becomes singular<br>(see @cite RJ91). In @cite BHK97 this was solved by performing a Principal Component Analysis on the<br>data and projecting the samples into the \f$(N-c)\f$-dimensional space. A Linear Discriminant Analysis<br>was then performed on the reduced data, because \f$S_{W}\f$ isn’t singular anymore.</p>
<p>The optimization problem can then be rewritten as:</p>
<p>\f[\begin{align*}<br>    W_{pca} &amp; &#x3D; &amp; \operatorname{arg,max}<em>{W} |W^T S_T W| \<br>    W</em>{fld} &amp; &#x3D; &amp; \operatorname{arg,max}<em>{W} \frac{|W^T W</em>{pca}^T S_{B} W_{pca} W|}{|W^T W_{pca}^T S_{W} W_{pca} W|}<br>\end{align*}\f]</p>
<p>The transformation matrix \f$W\f$, that projects a sample into the \f$(c-1)\f$-dimensional space is then<br>given by:</p>
<p>\f[W &#x3D; W_{fld}^{T} W_{pca}^{T}\f]</p>
<h3 id="Fisherfaces-in-OpenCV-tutorial-face-fisherfaces-use"><a href="#Fisherfaces-in-OpenCV-tutorial-face-fisherfaces-use" class="headerlink" title="Fisherfaces in OpenCV {#tutorial_face_fisherfaces_use}"></a>Fisherfaces in OpenCV {#tutorial_face_fisherfaces_use}</h3><p>The source code for this demo application is also available in the src folder coming with this<br>documentation:</p>
<p>@include face&#x2F;samples&#x2F;facerec_fisherfaces.cpp</p>
<p>For this example I am going to use the Yale Facedatabase A, just because the plots are nicer. Each<br>Fisherface has the same length as an original image, thus it can be displayed as an image. The demo<br>shows (or saves) the first, at most 16 Fisherfaces:</p>
<p><img src="/img/fisherfaces_opencv.png" alt="image"></p>
<p>The Fisherfaces method learns a class-specific transformation matrix, so the they do not capture<br>illumination as obviously as the Eigenfaces method. The Discriminant Analysis instead finds the<br>facial features to discriminate between the persons. It’s important to mention, that the performance<br>of the Fisherfaces heavily depends on the input data as well. Practically said: if you learn the<br>Fisherfaces for well-illuminated pictures only and you try to recognize faces in bad-illuminated<br>scenes, then method is likely to find the wrong components (just because those features may not be<br>predominant on bad illuminated images). This is somewhat logical, since the method had no chance to<br>learn the illumination.</p>
<p>The Fisherfaces allow a reconstruction of the projected image, just like the Eigenfaces did. But<br>since we only identified the features to distinguish between subjects, you can’t expect a nice<br>reconstruction of the original image. For the Fisherfaces method we’ll project the sample image onto<br>each of the Fisherfaces instead. So you’ll have a nice visualization, which feature each of the<br>Fisherfaces describes:</p>
<p>@code{.cpp}<br>&#x2F;&#x2F; Display or save the image reconstruction at some predefined steps:<br>for(int num_component &#x3D; 0; num_component &lt; min(16, W.cols); num_component++) {<br>    &#x2F;&#x2F; Slice the Fisherface from the model:<br>    Mat ev &#x3D; W.col(num_component);<br>    Mat projection &#x3D; LDA::subspaceProject(ev, mean, images[0].reshape(1,1));<br>    Mat reconstruction &#x3D; LDA::subspaceReconstruct(ev, mean, projection);<br>    &#x2F;&#x2F; Normalize the result:<br>    reconstruction &#x3D; norm_0_255(reconstruction.reshape(1, images[0].rows));<br>    &#x2F;&#x2F; Display or save:<br>    if(argc &#x3D;&#x3D; 2) {<br>        imshow(format(“fisherface_reconstruction_%d”, num_component), reconstruction);<br>    } else {<br>        imwrite(format(“%s&#x2F;fisherface_reconstruction_%d.png”, output_folder.c_str(), num_component), reconstruction);<br>    }<br>}<br>@endcode</p>
<p>The differences may be subtle for the human eyes, but you should be able to see some differences:</p>
<p><img src="/img/fisherface_reconstruction_opencv.png" alt="image"></p>
<h2 id="Local-Binary-Patterns-Histograms-tutorial-face-lbph"><a href="#Local-Binary-Patterns-Histograms-tutorial-face-lbph" class="headerlink" title="Local Binary Patterns Histograms {#tutorial_face_lbph}"></a>Local Binary Patterns Histograms {#tutorial_face_lbph}</h2><p>Eigenfaces and Fisherfaces take a somewhat holistic approach to face recognition. You treat your<br>data as a vector somewhere in a high-dimensional image space. We all know high-dimensionality is<br>bad, so a lower-dimensional subspace is identified, where (probably) useful information is<br>preserved. The Eigenfaces approach maximizes the total scatter, which can lead to problems if the<br>variance is generated by an external source, because components with a maximum variance over all<br>classes aren’t necessarily useful for classification (see<br><a href="http://www.bytefish.de/wiki/pca_lda_with_gnu_octave"><a href="http://www.bytefish.de/wiki/pca_lda_with_gnu_octave">http://www.bytefish.de/wiki/pca_lda_with_gnu_octave</a></a>).<br>So to preserve some discriminative information we applied a Linear Discriminant Analysis and<br>optimized as described in the Fisherfaces method. The Fisherfaces method worked great… at least<br>for the constrained scenario we’ve assumed in our model.</p>
<p>Now real life isn’t perfect. You simply can’t guarantee perfect light settings in your images or 10<br>different images of a person. So what if there’s only one image for each person? Our covariance<br>estimates for the subspace <em>may</em> be horribly wrong, so will the recognition. Remember the Eigenfaces<br>method had a 96% recognition rate on the AT&amp;T Facedatabase? How many images do we actually need to<br>get such useful estimates? Here are the Rank-1 recognition rates of the Eigenfaces and Fisherfaces<br>method on the AT&amp;T Facedatabase, which is a fairly easy image database:</p>
<p><img src="/img/at_database_small_sample_size.png" alt="image"></p>
<p>So in order to get good recognition rates you’ll need at least 8(+-1) images for each person and the<br>Fisherfaces method doesn’t really help here. The above experiment is a 10-fold cross validated<br>result carried out with the facerec framework at:<br><a href="https://github.com/bytefish/facerec"><a href="https://github.com/bytefish/facerec">https://github.com/bytefish/facerec</a></a>. This is not a<br>publication, so I won’t back these figures with a deep mathematical analysis. Please have a look<br>into @cite KM01 for a detailed analysis of both methods, when it comes to small training datasets.</p>
<p>So some research concentrated on extracting local features from images. The idea is to not look at<br>the whole image as a high-dimensional vector, but describe only local features of an object. The<br>features you extract this way will have a low-dimensionality implicitly. A fine idea! But you’ll<br>soon observe the image representation we are given doesn’t only suffer from illumination variations.<br>Think of things like scale, translation or rotation in images - your local description has to be at<br>least a bit robust against those things. Just like SIFT, the Local Binary Patterns methodology has<br>its roots in 2D texture analysis. The basic idea of Local Binary Patterns is to summarize the local<br>structure in an image by comparing each pixel with its neighborhood. Take a pixel as center and<br>threshold its neighbors against. If the intensity of the center pixel is greater-equal its neighbor,<br>then denote it with 1 and 0 if not. You’ll end up with a binary number for each pixel, just like<br>11001111. So with 8 surrounding pixels you’ll end up with 2^8 possible combinations, called <em>Local<br>Binary Patterns</em> or sometimes referred to as <em>LBP codes</em>. The first LBP operator described in<br>literature actually used a fixed 3 x 3 neighborhood just like this:</p>
<p><img src="/img/lbp/lbp.png" alt="image"></p>
<h3 id="Algorithmic-Description-of-LBPH-method-tutorial-face-lbph-algo"><a href="#Algorithmic-Description-of-LBPH-method-tutorial-face-lbph-algo" class="headerlink" title="Algorithmic Description of LBPH method {#tutorial_face_lbph_algo}"></a>Algorithmic Description of LBPH method {#tutorial_face_lbph_algo}</h3><p>A more formal description of the LBP operator can be given as:</p>
<p>\f[LBP(x_c, y_c) &#x3D; \sum_{p&#x3D;0}^{P-1} 2^p s(i_p - i_c)\f]</p>
<p>, with \f$(x_c, y_c)\f$ as central pixel with intensity \f$i_c\f$; and \f$i_n\f$ being the intensity of the the<br>neighbor pixel. \f$s\f$ is the sign function defined as:</p>
<p>\f[\begin{equation}<br>s(x) &#x3D;<br>\begin{cases}<br>1 &amp; \text{if (x \geq 0)}\<br>0 &amp; \text{else}<br>\end{cases}<br>\end{equation}\f]</p>
<p>This description enables you to capture very fine grained details in images. In fact the authors<br>were able to compete with state of the art results for texture classification. Soon after the<br>operator was published it was noted, that a fixed neighborhood fails to encode details differing in<br>scale. So the operator was extended to use a variable neighborhood in @cite AHP04 . The idea is to<br>align an abritrary number of neighbors on a circle with a variable radius, which enables to capture<br>the following neighborhoods:</p>
<p><img src="/img/lbp/patterns.png" alt="image"></p>
<p>For a given Point \f$(x_c,y_c)\f$ the position of the neighbor \f$(x_p,y_p), p \in P\f$ can be calculated<br>by:</p>
<p>\f[\begin{align*}<br>x_{p} &amp; &#x3D; &amp; x_c + R \cos({\frac{2\pi p}{P}})\<br>y_{p} &amp; &#x3D; &amp; y_c - R \sin({\frac{2\pi p}{P}})<br>\end{align*}\f]</p>
<p>Where \f$R\f$ is the radius of the circle and \f$P\f$ is the number of sample points.</p>
<p>The operator is an extension to the original LBP codes, so it’s sometimes called <em>Extended LBP</em><br>(also referred to as <em>Circular LBP</em>) . If a points coordinate on the circle doesn’t correspond to<br>image coordinates, the point get’s interpolated. Computer science has a bunch of clever<br>interpolation schemes, the OpenCV implementation does a bilinear interpolation:</p>
<p>\f[\begin{align*}<br>f(x,y) \approx \begin{bmatrix}<br>    1-x &amp; x \end{bmatrix} \begin{bmatrix}<br>    f(0,0) &amp; f(0,1) \<br>    f(1,0) &amp; f(1,1) \end{bmatrix} \begin{bmatrix}<br>    1-y \<br>    y \end{bmatrix}.<br>\end{align*}\f]</p>
<p>By definition the LBP operator is robust against monotonic gray scale transformations. We can easily<br>verify this by looking at the LBP image of an artificially modified image (so you see what an LBP<br>image looks like!):</p>
<p><img src="/img/lbp/lbp_yale.jpg" alt="image"></p>
<p>So what’s left to do is how to incorporate the spatial information in the face recognition model.<br>The representation proposed by Ahonen et. al @cite AHP04 is to divide the LBP image into \f$m\f$ local<br>regions and extract a histogram from each. The spatially enhanced feature vector is then obtained by<br>concatenating the local histograms (<strong>not merging them</strong>). These histograms are called <em>Local Binary<br>Patterns Histograms</em>.</p>
<h3 id="Local-Binary-Patterns-Histograms-in-OpenCV-tutorial-face-lbph-use"><a href="#Local-Binary-Patterns-Histograms-in-OpenCV-tutorial-face-lbph-use" class="headerlink" title="Local Binary Patterns Histograms in OpenCV {#tutorial_face_lbph_use}"></a>Local Binary Patterns Histograms in OpenCV {#tutorial_face_lbph_use}</h3><p>The source code for this demo application is also available in the src folder coming with this<br>documentation:</p>
<p>@include face&#x2F;samples&#x2F;facerec_lbph.cpp</p>
<h2 id="Conclusion-tutorial-face-conclusion"><a href="#Conclusion-tutorial-face-conclusion" class="headerlink" title="Conclusion {#tutorial_face_conclusion}"></a>Conclusion {#tutorial_face_conclusion}</h2><p>You’ve learned how to use the new FaceRecognizer in real applications. After reading the document<br>you also know how the algorithms work, so now it’s time for you to experiment with the available<br>algorithms. Use them, improve them and let the OpenCV community participate!</p>
<h2 id="Credits-tutorial-face-credits"><a href="#Credits-tutorial-face-credits" class="headerlink" title="Credits {#tutorial_face_credits}"></a>Credits {#tutorial_face_credits}</h2><p>This document wouldn’t be possible without the kind permission to use the face images of the <em>AT&amp;T<br>Database of Faces</em> and the <em>Yale Facedatabase A&#x2F;B</em>.</p>
<h3 id="The-Database-of-Faces-tutorial-face-credits-db"><a href="#The-Database-of-Faces-tutorial-face-credits-db" class="headerlink" title="The Database of Faces {#tutorial_face_credits_db}"></a>The Database of Faces {#tutorial_face_credits_db}</h3><p><strong>Important: when using these images, please give credit to “AT&amp;T Laboratories, Cambridge.”</strong></p>
<p>The Database of Faces, formerly <em>The ORL Database of Faces</em>, contains a set of face images taken<br>between April 1992 and April 1994. The database was used in the context of a face recognition<br>project carried out in collaboration with the Speech, Vision and Robotics Group of the Cambridge<br>University Engineering Department.</p>
<p>There are ten different images of each of 40 distinct subjects. For some subjects, the images were<br>taken at different times, varying the lighting, facial expressions (open &#x2F; closed eyes, smiling &#x2F;<br>not smiling) and facial details (glasses &#x2F; no glasses). All the images were taken against a dark<br>homogeneous background with the subjects in an upright, frontal position (with tolerance for some<br>side movement).</p>
<p>The files are in PGM format. The size of each image is 92x112 pixels, with 256 grey levels per<br>pixel. The images are organised in 40 directories (one for each subject), which have names of the<br>form sX, where X indicates the subject number (between 1 and 40). In each of these directories,<br>there are ten different images of that subject, which have names of the form Y.pgm, where Y is the<br>image number for that subject (between 1 and 10).</p>
<p>A copy of the database can be retrieved from:<br><a href="http://www.cl.cam.ac.uk/research/dtg/attarchive/pub/data/att_faces.zip"><a href="http://www.cl.cam.ac.uk/research/dtg/attarchive/pub/data/att_faces.zip">http://www.cl.cam.ac.uk/research/dtg/attarchive/pub/data/att_faces.zip</a></a>.</p>
<h3 id="Yale-Facedatabase-A-tutorial-face-credits-yalea"><a href="#Yale-Facedatabase-A-tutorial-face-credits-yalea" class="headerlink" title="Yale Facedatabase A {#tutorial_face_credits_yalea}"></a>Yale Facedatabase A {#tutorial_face_credits_yalea}</h3><p><em>With the permission of the authors I am allowed to show a small number of images (say subject 1 and<br>all the variations) and all images such as Fisherfaces and Eigenfaces from either Yale Facedatabase<br>A or the Yale Facedatabase B.</em></p>
<p>The Yale Face Database A (size 6.4MB) contains 165 grayscale images in GIF format of 15 individuals.<br>There are 11 images per subject, one per different facial expression or configuration: center-light,<br>w&#x2F;glasses, happy, left-light, w&#x2F;no glasses, normal, right-light, sad, sleepy, surprised, and wink.<br>(Source:<br><a href="http://cvc.yale.edu/projects/yalefaces/yalefaces.html"><a href="http://cvc.yale.edu/projects/yalefaces/yalefaces.html">http://cvc.yale.edu/projects/yalefaces/yalefaces.html</a></a>)</p>
<h3 id="Yale-Facedatabase-B-tutorial-face-credits-yaleb"><a href="#Yale-Facedatabase-B-tutorial-face-credits-yaleb" class="headerlink" title="Yale Facedatabase B {#tutorial_face_credits_yaleb}"></a>Yale Facedatabase B {#tutorial_face_credits_yaleb}</h3><p><em>With the permission of the authors I am allowed to show a small number of images (say subject 1 and<br>all the variations) and all images such as Fisherfaces and Eigenfaces from either Yale Facedatabase<br>A or the Yale Facedatabase B.</em></p>
<p>The extended Yale Face Database B contains 16128 images of 28 human subjects under 9 poses and 64<br>illumination conditions. The data format of this database is the same as the Yale Face Database B.<br>Please refer to the homepage of the Yale Face Database B (or one copy of this page) for more<br>detailed information of the data format.</p>
<p>You are free to use the extended Yale Face Database B for research purposes. All publications which<br>use this database should acknowledge the use of “the Exteded Yale Face Database B” and reference<br>Athinodoros Georghiades, Peter Belhumeur, and David Kriegman’s paper, “From Few to Many:<br>Illumination Cone Models for Face Recognition under Variable Lighting and Pose”, PAMI, 2001,<br><a href="http://vision.ucsd.edu/~leekc/ExtYaleDatabase/athosref.html">[bibtex]</a>.</p>
<p>The extended database as opposed to the original Yale Face Database B with 10 subjects was first<br>reported by Kuang-Chih Lee, Jeffrey Ho, and David Kriegman in “Acquiring Linear Subspaces for Face<br>Recognition under Variable Lighting, PAMI, May, 2005<br><a href="http://vision.ucsd.edu/~leekc/papers/9pltsIEEE.pdf">[pdf]</a>.” All test image data used in the<br>experiments are manually aligned, cropped, and then re-sized to 168x192 images. If you publish your<br>experimental results with the cropped images, please reference the PAMI2005 paper as well. (Source:<br><a href="http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html"><a href="http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html">http://vision.ucsd.edu/~leekc/ExtYaleDatabase/ExtYaleB.html</a></a>)</p>
<h2 id="Appendix-face-appendix"><a href="#Appendix-face-appendix" class="headerlink" title="Appendix {#face_appendix}"></a>Appendix {#face_appendix}</h2><h3 id="Creating-the-CSV-File-tutorial-face-appendix-csv"><a href="#Creating-the-CSV-File-tutorial-face-appendix-csv" class="headerlink" title="Creating the CSV File {#tutorial_face_appendix_csv}"></a>Creating the CSV File {#tutorial_face_appendix_csv}</h3><p>You don’t really want to create the CSV file by hand. I have prepared you a little Python script<br><code>create_csv.py</code> (you find it at <code>src/create_csv.py</code> coming with this tutorial) that automatically<br>creates you a CSV file. If you have your images in hierarchie like this<br>(<code>/basepath/&lt;subject&gt;/&lt;image.ext&gt;</code>):</p>
<p>@code{.sh}<br>philipp@mango:~&#x2F;facerec&#x2F;data&#x2F;at$ tree<br>.<br>|– s1<br>|   |– 1.pgm<br>|   |– …<br>|   |– 10.pgm<br>|– s2<br>|   |– 1.pgm<br>|   |– …<br>|   |– 10.pgm<br>…<br>|– s40<br>|   |– 1.pgm<br>|   |– …<br>|   |– 10.pgm<br>@endcode</p>
<p>Then simply call <code>create_csv.py at</code> , here ‘at’ being the basepath to the folder, just like this and you could save the<br>output:</p>
<p>@code{.sh}<br>philipp@mango:~&#x2F;facerec&#x2F;data$ python create_csv.py at<br>at&#x2F;s13&#x2F;2.pgm;0<br>at&#x2F;s13&#x2F;7.pgm;0<br>at&#x2F;s13&#x2F;6.pgm;0<br>at&#x2F;s13&#x2F;9.pgm;0<br>at&#x2F;s13&#x2F;5.pgm;0<br>at&#x2F;s13&#x2F;3.pgm;0<br>at&#x2F;s13&#x2F;4.pgm;0<br>at&#x2F;s13&#x2F;10.pgm;0<br>at&#x2F;s13&#x2F;8.pgm;0<br>at&#x2F;s13&#x2F;1.pgm;0<br>at&#x2F;s17&#x2F;2.pgm;1<br>at&#x2F;s17&#x2F;7.pgm;1<br>at&#x2F;s17&#x2F;6.pgm;1<br>at&#x2F;s17&#x2F;9.pgm;1<br>at&#x2F;s17&#x2F;5.pgm;1<br>at&#x2F;s17&#x2F;3.pgm;1<br>[…]<br>@endcode</p>
<p>Here is the script, if you can’t find it:</p>
<p>@verbinclude face&#x2F;samples&#x2F;etc&#x2F;create_csv.py</p>
<h3 id="Aligning-Face-Images-tutorial-face-appendix-align"><a href="#Aligning-Face-Images-tutorial-face-appendix-align" class="headerlink" title="Aligning Face Images {#tutorial_face_appendix_align}"></a>Aligning Face Images {#tutorial_face_appendix_align}</h3><p>An accurate alignment of your image data is especially important in tasks like emotion detection,<br>were you need as much detail as possible. Believe me… You don’t want to do this by hand. So I’ve<br>prepared you a tiny Python script. The code is really easy to use. To scale, rotate and crop the<br>face image you just need to call <em>CropFace(image, eye_left, eye_right, offset_pct, dest_sz)</em>,<br>where:</p>
<ul>
<li><em>eye_left</em> is the position of the left eye</li>
<li><em>eye_right</em> is the position of the right eye</li>
<li><em>offset_pct</em> is the percent of the image you want to keep next to the eyes (horizontal,<br>vertical direction)</li>
<li><em>dest_sz</em> is the size of the output image</li>
</ul>
<p>If you are using the same <em>offset_pct</em> and <em>dest_sz</em> for your images, they are all aligned at the<br>eyes.</p>
<p>@verbinclude face&#x2F;samples&#x2F;etc&#x2F;crop_face.py</p>
<p>Imagine we are given <a href="http://en.wikipedia.org/wiki/File:Arnold_Schwarzenegger_edit%28ws%29.jpg">this photo of Arnold<br>Schwarzenegger</a>, which is<br>under a Public Domain license. The (x,y)-position of the eyes is approximately <em>(252,364)</em> for the<br>left and <em>(420,366)</em> for the right eye. Now you only need to define the horizontal offset, vertical<br>offset and the size your scaled, rotated &amp; cropped face should have.</p>
<p>Here are some examples:</p>
<table>
<thead>
<tr>
<th>Configuration</th>
<th>Cropped, Scaled, Rotated Face</th>
</tr>
</thead>
<tbody><tr>
<td>0.1 (10%), 0.1 (10%), (200,200)</td>
<td><img src="/tutorials/gender_classification/arnie_10_10_200_200.jpg"></td>
</tr>
<tr>
<td>0.2 (20%), 0.2 (20%), (200,200)</td>
<td><img src="/tutorials/gender_classification/arnie_20_20_200_200.jpg"></td>
</tr>
<tr>
<td>0.3 (30%), 0.3 (30%), (200,200)</td>
<td><img src="/tutorials/gender_classification/arnie_30_30_200_200.jpg"></td>
</tr>
<tr>
<td>0.2 (20%), 0.2 (20%), (70,70)</td>
<td><img src="/tutorials/gender_classification/arnie_20_20_70_70.jpg"></td>
</tr>
</tbody></table>
<h3 id="CSV-for-the-AT-amp-T-Facedatabase-tutorial-face-appendix-attcsv"><a href="#CSV-for-the-AT-amp-T-Facedatabase-tutorial-face-appendix-attcsv" class="headerlink" title="CSV for the AT&amp;T Facedatabase {#tutorial_face_appendix_attcsv}"></a>CSV for the AT&amp;T Facedatabase {#tutorial_face_appendix_attcsv}</h3><p>@verbinclude face&#x2F;samples&#x2F;etc&#x2F;at.txt</p>
