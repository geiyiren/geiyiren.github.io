<h1 id="Using-the-FacemarkAAM-tutorial-facemark-aam"><a href="#Using-the-FacemarkAAM-tutorial-facemark-aam" class="headerlink" title="Using the FacemarkAAM {#tutorial_facemark_aam}"></a>Using the FacemarkAAM {#tutorial_facemark_aam}</h1><h2 id="Goals"><a href="#Goals" class="headerlink" title="Goals"></a>Goals</h2><p>In this tutorial you will learn how to:</p>
<ul>
<li>creating the instance of FacemarkAAM</li>
<li>training the AAM model</li>
<li>Fitting using FacemarkAAM</li>
</ul>
<h2 id="Preparation"><a href="#Preparation" class="headerlink" title="Preparation"></a>Preparation</h2><p>Before you continue with this tutorial, you should download the dataset of facial landmarks detection.<br>We suggest you to download the LFPW dataset which can be retrieved at <a href="https://ibug.doc.ic.ac.uk/download/annotations/lfpw.zip">https://ibug.doc.ic.ac.uk/download/annotations/lfpw.zip</a>.</p>
<p>Make sure that the annotation format is supported by the API, the contents in annotation file should look like the following snippet:<br>@code<br>version: 1<br>n_points:  68<br>{<br>212.716603 499.771793<br>230.232816 566.290071<br>â€¦<br>}<br>@endcode</p>
<p>The next thing to do is to make 2 text files containing the list of image files and annotation files respectively. Make sure that the order or image and annotation in both files are matched. Furthermore, it is advised to use absolute path instead of relative path.<br>Example to make the file list in Linux machine<br>@code<br>ls $PWD&#x2F;trainset&#x2F;<em>.jpg &gt; images_train.txt<br>ls $PWD&#x2F;trainset&#x2F;</em>.pts &gt; annotation_train.txt<br>@endcode</p>
<p>example of content in the images_train.txt<br>@code<br>&#x2F;home&#x2F;user&#x2F;lfpw&#x2F;trainset&#x2F;100032540_1.jpg<br>&#x2F;home&#x2F;user&#x2F;lfpw&#x2F;trainset&#x2F;100040721_1.jpg<br>&#x2F;home&#x2F;user&#x2F;lfpw&#x2F;trainset&#x2F;100040721_2.jpg<br>&#x2F;home&#x2F;user&#x2F;lfpw&#x2F;trainset&#x2F;1002681492_1.jpg<br>@endcode</p>
<p>example of content in the annotation_train.txt<br>@code<br>&#x2F;home&#x2F;user&#x2F;lfpw&#x2F;trainset&#x2F;100032540_1.pts<br>&#x2F;home&#x2F;user&#x2F;lfpw&#x2F;trainset&#x2F;100040721_1.pts<br>&#x2F;home&#x2F;user&#x2F;lfpw&#x2F;trainset&#x2F;100040721_2.pts<br>&#x2F;home&#x2F;user&#x2F;lfpw&#x2F;trainset&#x2F;1002681492_1.pts<br>@endcode</p>
<p>Optionally, you can create the similar files for the testset.</p>
<p>In this tutorial, the pre-trained model will not be provided due to its large file size (~500MB). By following this tutorial, you will be able to train obtain your own trained model within few minutes.</p>
<h2 id="Working-with-the-AAM-Algorithm"><a href="#Working-with-the-AAM-Algorithm" class="headerlink" title="Working with the AAM Algorithm"></a>Working with the AAM Algorithm</h2><p>The full working code is available in the face&#x2F;samples&#x2F;facemark_demo_aam.cpp file. In this tutorial, the explanation of some important parts are covered.</p>
<p>-# <B>Creating the instance of AAM algorithm</B></p>
<p>   @snippet face&#x2F;samples&#x2F;facemark_demo_aam.cpp instance_creation<br>   Firstly, an instance of parameter for the AAM algorithm is created. In this case, we will modify the default list of the scaling factor. By default, the scaling factor used is 1.0 (no scaling). Here we add two more scaling factor which will make the instance trains two more model at scale 2 and 4 (2 time smaller and 4 time smaller, faster faster fitting time). However, you should make sure that this scaling factor is not too big since it will make the image scaled into a very small one. Thus it will lost all of its important information for the landmark detection purpose.</p>
<p>   Alternatively, you can override the default scaling in similar way to this example:<br>   @code<br>   std::vector<float>scales;<br>   scales.push_back(1.5);<br>   scales.push_back(2.4);</p>
<p>   FacemarkAAM::Params params;<br>   params.scales &#x3D; scales;<br>   @endcode</p>
<p>-# <B>Loading the dataset</B></p>
<p>   @snippet face&#x2F;samples&#x2F;facemark_demo_aam.cpp load_dataset<br>   List of the dataset are loaded into the program. We will put the samples from dataset one by one in the next step.</p>
<p>-# <B>Adding the samples to the trainer</B></p>
<p>   @snippet face&#x2F;samples&#x2F;facemark_demo_aam.cpp add_samples<br>   The image from the dataset list are loaded one by one as well as its corresponding annotation data. Then the pair of sample is added to the trainer.</p>
<p>-# <B>Training process</B></p>
<p>   @snippet face&#x2F;samples&#x2F;facemark_demo_aam.cpp training<br>   The training process is called using a single line of code. Make sure that all the required training samples are already added to the trainer.</p>
<p>-# <B>Preparation for fitting</B></p>
<p>   First of all, you need to load the list of test files.<br>   @snippet face&#x2F;samples&#x2F;facemark_demo_aam.cpp load_test_images</p>
<p>   Since the AAM needs initialization parameters (rotation, translation, and scaling), you need to declare the required variable to store these information which will be obtained using a custom function. Since the implementation of getInitialFitting() function in this example is not optimal, you can create your own function.</p>
<p>   The initialization is obtained by comparing the base shape of the trained model with the current face image. In this case, the rotation is obtained by comparing the angle of line formed by two eyes in the input face image with the same line in the base shape. Meanwhile, the scaling is obtained by comparing the length of line between eyes in the input image compared to the base shape.</p>
<p>-# <B>Fitting process</B></p>
<p>   The fitting process is started by detecting the face in a given image.<br>   @snippet face&#x2F;samples&#x2F;facemark_demo_aam.cpp detect_face</p>
<p>   If at least one face is found, then the next step is computing the initialization parameters. In this case, since the getInitialFitting() function is not optimal, it may not find pair of eyes from a given face. Therefore, we will filter out the face without initialization parameters and in this case, each element in the <code>conf</code> vector represent the initialization parameter for each filtered face.<br>   @snippet face&#x2F;samples&#x2F;facemark_demo_aam.cpp get_initialization</p>
<p>   For the fitting parameter stored in the <code>conf</code> vector, the last parameter represent the ID of scaling factor that will be used in the fitting process. In this example the fitting will use the biggest scaling factor (4) which is expected to have the fastest computation time compared to the other scales. If the ID if bigger than the available trained scale in the model, the the model with the biggest scale ID is used.</p>
<p>   The fitting process is quite simple, you just need to put the corresponding image, vector of <code>cv::Rect</code> representing the ROIs of all faces in the given image, container of the landmark points represented by <code>landmarks</code> variable, and the configuration variables.<br>   @snippet face&#x2F;samples&#x2F;facemark_demo_aam.cpp fitting_process</p>
<p>   After the fitting process is finished, you can visualize the result using the <code>drawFacemarks</code> function.</p>
