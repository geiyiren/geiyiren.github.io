<h1 id="OpenCV-pixel-intensity-based-registration-module"><a href="#OpenCV-pixel-intensity-based-registration-module" class="headerlink" title="OpenCV pixel-intensity based registration module"></a>OpenCV pixel-intensity based registration module</h1><p>Author and maintainer: Alfonso Sanchez-Beato<br>                       alfonsosanchezbeato____gmail.com</p>
<p>These classes implement a module for OpenCV for parametric image registration.<br>The implemented method is direct alignment, that is, it uses directly the pixel<br>values for calculating the registration between a pair of images, as opposed to<br>feature-based registration.  The implementation follows essentially the<br>corresponding part of the paper “Image Alignment and Stitching: A Tutorial”,<br>from Richard Szeliski.</p>
<p>Feature based methods have some advantages over pixel based methods when we are<br>trying to register pictures that have been shoot under different lighting<br>conditions or exposition times, or when the images overlap only partially. On<br>the other hand, the main advantage of pixel-based methods when compared to<br>feature based methods is their better precision for some pictures (those shoot<br>under similar lighting conditions and that have a significative overlap), due to<br>the fact that we are using all the information available in the image, which<br>allows us to achieve subpixel accuracy. This is particularly important for<br>certain applications like multi-frame denoising or super-resolution.</p>
<p>In fact, pixel and feature registration methods can complement each other: an<br>application could first obtain a coarse registration using features and then<br>refine the registration using a pixel based method on the overlapping area of<br>the images. The code developed allows this use case.</p>
<p>The module implements classes derived from the abstract classes cv::reg::Map or<br>cv::reg::Mapper.  The former models a coordinate transformation between two<br>reference frames, while the later encapsulates a way of invoking a method that<br>calculates a Map between two images.  Although the objective has been to<br>implement pixel based methods, the module could be extended to support other<br>methods that can calculate transformations between images (feature methods,<br>optical flow, etc.).</p>
<p>Each class derived from Map implements a motion model, as follows:</p>
<ul>
<li><p>MapShift: Models a simple translation</p>
</li>
<li><p>MapAffine: Models an affine transformation</p>
</li>
<li><p>MapProject: Models a projective transformation<br>MapProject can also be used to model affine motion or translations, but some<br>operations on it are more costly, and that is the reason for defining the other<br>two classes.</p>
</li>
</ul>
<p>The classes derived from Mapper are</p>
<ul>
<li><p>MapperGradShift: Gradient based alignment for calculating translations. It<br>produces a MapShift (two parameters that correspond to the shift vector).</p>
</li>
<li><p>MapperGradEuclid: Gradient based alignment for euclidean motions, that is,<br>rotations and translations. It calculates three parameters (angle and shift<br>vector), although the result is stored in a MapAffine object for convenience.</p>
</li>
<li><p>MapperGradSimilar: Gradient based alignment for calculating similarities,<br>which adds scaling to the euclidean motion. It calculates four parameters (two<br>for the anti-symmetric matrix and two for the shift vector), although the result<br>is stored in a MapAffine object for convenience.</p>
</li>
<li><p>MapperGradAffine: Gradient based alignment for an affine motion model. The<br>number of parameters is six and the result is stored in a MapAffine object. </p>
</li>
<li><p>MapperGradProj: Gradient based alignment for calculating projective<br>transformations. The number of parameters is eight and the result is stored in a<br>MapProject object.</p>
</li>
<li><p>MapperPyramid: It implements hyerarchical motion estimation using a Gaussian<br>pyramid. Its constructor accepts as argument any other object that implements<br>the Mapper interface, and it is that mapper the one called by MapperPyramid for<br>each scale of the pyramid.</p>
</li>
</ul>
<p>If the motion between the images is not very small, the normal way of using<br>these classes is to create a MapperGrad* object and use it as input to create a<br>MapperPyramid, which in turn is called to perform the calculation. However, if<br>the motion between the images is small enough, we can use directly the<br>MapperGrad* classes. Another possibility is to use first a feature based method<br>to perform a coarse registration and then do a refinement through MapperPyramid<br>or directly a MapperGrad* object. The “calculate” method of the mappers accepts<br>an initial estimation of the motion as input.</p>
<p>When deciding which MapperGrad to use we must take into account that mappers<br>with more parameters can handle more complex motions, but involve more<br>calculations and are therefore slower. Also, if we are confident on the motion<br>model that is followed by the sequence, increasing the number of parameters<br>beyond what we need will decrease the accuracy: it is better to use the least<br>number of degrees of freedom that we can.</p>
<p>In the file map_test.cpp some examples on how to use this module can be seen.<br>There is a test function for each MapperGrad*. A motion is simulated on an input<br>image and then we register the moved image using a MapperPyramid created with<br>the right MapperGrad*. The difference images of the pictures before and after<br>registering are displayed, and the ground truth parameters and the calculated<br>ones are printed. Additionally, two images from a real video are registered<br>using first SURF features and then MapperGradProj+MapperPyramid. The difference<br>between the images and the difference of the registered images using the two<br>methods are displayed. It can be seen in the differences shown that using a<br>pixel based difference we can achieve more accuracy.</p>
