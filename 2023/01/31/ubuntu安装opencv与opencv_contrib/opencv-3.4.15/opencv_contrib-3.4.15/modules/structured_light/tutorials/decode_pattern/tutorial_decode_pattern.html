<h1 id="Decode-Gray-code-pattern-tutorial-tutorial-decode-graycode-pattern"><a href="#Decode-Gray-code-pattern-tutorial-tutorial-decode-graycode-pattern" class="headerlink" title="Decode Gray code pattern tutorial {#tutorial_decode_graycode_pattern}"></a>Decode Gray code pattern tutorial {#tutorial_decode_graycode_pattern}</h1><h2 id="Goal"><a href="#Goal" class="headerlink" title="Goal"></a>Goal</h2><p>In this tutorial you will learn how to use the <em>GrayCodePattern</em> class to:</p>
<ul>
<li>Decode a previously acquired Gray code pattern.</li>
<li>Generate a disparity map.</li>
<li>Generate a pointcloud.</li>
</ul>
<h2 id="Code"><a href="#Code" class="headerlink" title="Code"></a>Code</h2><p>@include structured_light&#x2F;samples&#x2F;pointcloud.cpp</p>
<h2 id="Explanation"><a href="#Explanation" class="headerlink" title="Explanation"></a>Explanation</h2><p>First of all the needed parameters must be passed to the program.<br>The first is the name list of previously acquired pattern images, stored in a .yaml file organized as below:</p>
<p>@code{.cpp}<br>%YAML:1.0<br>cam1:</p>
<ul>
<li>“&#x2F;data&#x2F;pattern_cam1_im1.png”</li>
<li>“&#x2F;data&#x2F;pattern_cam1_im2.png”<br>  …………..</li>
<li>“&#x2F;data&#x2F;pattern_cam1_im42.png”</li>
<li>“&#x2F;data&#x2F;pattern_cam1_im43.png”</li>
<li>“&#x2F;data&#x2F;pattern_cam1_im44.png”<br>cam2:</li>
<li>“&#x2F;data&#x2F;pattern_cam2_im1.png”</li>
<li>“&#x2F;data&#x2F;pattern_cam2_im2.png”<br>  …………..</li>
<li>“&#x2F;data&#x2F;pattern_cam2_im42.png”</li>
<li>“&#x2F;data&#x2F;pattern_cam2_im43.png”</li>
<li>“&#x2F;data&#x2F;pattern_cam2_im44.png”<br>@endcode</li>
</ul>
<p>For example, the dataset used for this tutorial has been acquired using a projector with a resolution of 1280x800, so 42 pattern images (from number 1 to 42) + 1 white (number 43) and 1 black (number 44) were captured with both the two cameras.</p>
<p>Then the cameras calibration parameters, stored in another .yml file, together with the width and the height of the projector used to project the pattern, and, optionally, the values of white and black tresholds, must be passed to the tutorial program.</p>
<p>In this way, <em>GrayCodePattern</em> class parameters can be set up with the width and the height of the projector used during the pattern acquisition and a pointer to a GrayCodePattern object can be created:</p>
<p>@code{.cpp}<br>  structured_light::GrayCodePattern::Params params;<br>     ….<br>  params.width &#x3D; parser.get<int>( 2 );<br>  params.height &#x3D; parser.get<int>( 3 );<br>     ….<br>  &#x2F;&#x2F; Set up GraycodePattern with params<br>  Ptr&lt;structured_light::GrayCodePattern&gt; graycode &#x3D; structured_light::GrayCodePattern::create( params );<br>@endcode</p>
<p>If the white and black thresholds are passed as parameters (these thresholds influence the number of decoded pixels), their values can be set, otherwise the algorithm will use the default values.</p>
<p>@code{.cpp}<br>  size_t white_thresh &#x3D; 0;<br>  size_t black_thresh &#x3D; 0;<br>  if( argc &#x3D;&#x3D; 7 )<br>  {<br>    &#x2F;&#x2F; If passed, setting the white and black threshold, otherwise using default values<br>    white_thresh &#x3D; parser.get<size_t>( 4 );<br>    black_thresh &#x3D; parser.get<size_t>( 5 );<br>    graycode-&gt;setWhiteThreshold( white_thresh );<br>    graycode-&gt;setBlackThreshold( black_thresh );<br>  }<br>@endcode</p>
<p>At this point, to use the <em>decode</em> method of <em>GrayCodePattern</em> class, the acquired pattern images must be stored in a vector of vector of Mat.<br>The external vector has a size of two because two are the cameras: the first vector stores the pattern images captured from the left camera, the second those acquired from the right one. The number of pattern images is obviously the same for both cameras and can be retrieved using the getNumberOfPatternImages() method:</p>
<p>@code{.cpp}<br>  size_t numberOfPatternImages &#x3D; graycode-&gt;getNumberOfPatternImages();<br>  vector&lt;vector<Mat> &gt; captured_pattern;<br>  captured_pattern.resize( 2 );<br>  captured_pattern[0].resize( numberOfPatternImages );<br>  captured_pattern[1].resize( numberOfPatternImages );</p>
<p>  …..</p>
<p>  for( size_t i &#x3D; 0; i &lt; numberOfPatternImages; i++ )<br>  {<br>    captured_pattern[0][i] &#x3D; imread( imagelist[i], IMREAD_GRAYSCALE );<br>    captured_pattern[1][i] &#x3D; imread( imagelist[i + numberOfPatternImages + 2], IMREAD_GRAYSCALE );<br>     ……<br>  }<br>@endcode</p>
<p>As regards the black and white images, they must be stored in two different vectors of Mat:</p>
<p>@code{.cpp}<br>  vector<Mat> blackImages;<br>  vector<Mat> whiteImages;<br>  blackImages.resize( 2 );<br>  whiteImages.resize( 2 );<br>  &#x2F;&#x2F; Loading images (all white + all black) needed for shadows computation<br>  cvtColor( color, whiteImages[0], COLOR_RGB2GRAY );<br>  whiteImages[1] &#x3D; imread( imagelist[2 * numberOfPatternImages + 2], IMREAD_GRAYSCALE );<br>  blackImages[0] &#x3D; imread( imagelist[numberOfPatternImages + 1], IMREAD_GRAYSCALE );<br>  blackImages[1] &#x3D; imread( imagelist[2 * numberOfPatternImages + 2 + 1], IMREAD_GRAYSCALE );<br>@endcode</p>
<p>It is important to underline that all the images, the pattern ones, black and white, must be loaded as grayscale images and rectified before being passed to decode method:</p>
<p>@code{.cpp}<br>  &#x2F;&#x2F; Stereo rectify<br>  cout &lt;&lt; “Rectifying images…” &lt;&lt; endl;<br>  Mat R1, R2, P1, P2, Q;<br>  Rect validRoi[2];<br>  stereoRectify( cam1intrinsics, cam1distCoeffs, cam2intrinsics, cam2distCoeffs, imagesSize, R, T, R1, R2, P1, P2, Q, 0,<br>                -1, imagesSize, &amp;validRoi[0], &amp;validRoi[1] );<br>  Mat map1x, map1y, map2x, map2y;<br>  initUndistortRectifyMap( cam1intrinsics, cam1distCoeffs, R1, P1, imagesSize, CV_32FC1, map1x, map1y );<br>  initUndistortRectifyMap( cam2intrinsics, cam2distCoeffs, R2, P2, imagesSize, CV_32FC1, map2x, map2y );<br>         ……..<br>  for( size_t i &#x3D; 0; i &lt; numberOfPatternImages; i++ )<br>  {<br>         ……..<br>    remap( captured_pattern[1][i], captured_pattern[1][i], map1x, map1y, INTER_NEAREST, BORDER_CONSTANT, Scalar() );<br>    remap( captured_pattern[0][i], captured_pattern[0][i], map2x, map2y, INTER_NEAREST, BORDER_CONSTANT, Scalar() );<br>  }<br>         ……..<br>  remap( color, color, map2x, map2y, INTER_NEAREST, BORDER_CONSTANT, Scalar() );<br>  remap( whiteImages[0], whiteImages[0], map2x, map2y, INTER_NEAREST, BORDER_CONSTANT, Scalar() );<br>  remap( whiteImages[1], whiteImages[1], map1x, map1y, INTER_NEAREST, BORDER_CONSTANT, Scalar() );<br>  remap( blackImages[0], blackImages[0], map2x, map2y, INTER_NEAREST, BORDER_CONSTANT, Scalar() );<br>  remap( blackImages[1], blackImages[1], map1x, map1y, INTER_NEAREST, BORDER_CONSTANT, Scalar() );<br>@endcode</p>
<p>In this way the <em>decode</em> method can be called to decode the pattern and to generate the corresponding disparity map, computed on the first camera (left):<br>@code{.cpp}<br>  Mat disparityMap;<br>  bool decoded &#x3D; graycode-&gt;decode(captured_pattern, disparityMap, blackImages, whiteImages,<br>                                  structured_light::DECODE_3D_UNDERWORLD);<br>@endcode</p>
<p>To better visualize the result, a colormap is applied to the computed disparity:<br>@code{.cpp}<br>    double min;<br>    double max;<br>    minMaxIdx(disparityMap, &amp;min, &amp;max);<br>    Mat cm_disp, scaledDisparityMap;<br>    cout &lt;&lt; “disp min “ &lt;&lt; min &lt;&lt; endl &lt;&lt; “disp max “ &lt;&lt; max &lt;&lt; endl;<br>    convertScaleAbs( disparityMap, scaledDisparityMap, 255 &#x2F; ( max - min ) );<br>    applyColorMap( scaledDisparityMap, cm_disp, COLORMAP_JET );<br>    &#x2F;&#x2F; Show the result<br>    resize( cm_disp, cm_disp, Size( 640, 480 ) );<br>    imshow( “cm disparity m”, cm_disp )<br>@endcode</p>
<p><img src="/pics/cm_disparity.png"></p>
<p>At this point the point cloud can be generated using the reprojectImageTo3D method, taking care to convert the computed disparity in a CV_32FC1 Mat (decode method computes a CV_64FC1 disparity map):<br>@code{.cpp}<br>  Mat pointcloud;<br>  disparityMap.convertTo( disparityMap, CV_32FC1 );<br>  reprojectImageTo3D( disparityMap, pointcloud, Q, true, -1 );<br>@endcode</p>
<p>Then a mask to remove the unwanted background is computed:<br>@code{.cpp}<br>  Mat dst, thresholded_disp;<br>  threshold( scaledDisparityMap, thresholded_disp, 0, 255, THRESH_OTSU + THRESH_BINARY );<br>  resize( thresholded_disp, dst, Size( 640, 480 ) );<br>  imshow( “threshold disp otsu”, dst );<br>@endcode<br><img src="/pics/threshold_disp.png"></p>
<p>The white image of cam1 was previously loaded also as a color image, in order to map the color of the object on its reconstructed pointcloud:<br>@code{.cpp}<br>  Mat color &#x3D; imread( imagelist[numberOfPatternImages], IMREAD_COLOR );<br>@endcode</p>
<p>The background renoval mask is thus applied to the point cloud and to the color image:<br>@code{.cpp}<br>  Mat pointcloud_tresh, color_tresh;<br>  pointcloud.copyTo(pointcloud_tresh, thresholded_disp);<br>  color.copyTo(color_tresh, thresholded_disp);<br>@endcode</p>
<p>Finally the computed point cloud of the scanned object can be visualized on viz:<br>@code{.cpp}<br>  viz::Viz3d myWindow( “Point cloud with color”);<br>  myWindow.setBackgroundMeshLab();<br>  myWindow.showWidget( “coosys”, viz::WCoordinateSystem());<br>  myWindow.showWidget( “pointcloud”, viz::WCloud( pointcloud_tresh, color_tresh ) );<br>  myWindow.showWidget( “text2d”, viz::WText( “Point cloud”, Point(20, 20), 20, viz::Color::green() ) );<br>  myWindow.spin();<br>@endcode</p>
<p><img src="/pics/plane_viz.png"></p>
