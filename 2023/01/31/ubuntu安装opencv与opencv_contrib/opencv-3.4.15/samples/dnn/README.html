<h1 id="OpenCV-deep-learning-module-samples"><a href="#OpenCV-deep-learning-module-samples" class="headerlink" title="OpenCV deep learning module samples"></a>OpenCV deep learning module samples</h1><h2 id="Model-Zoo"><a href="#Model-Zoo" class="headerlink" title="Model Zoo"></a>Model Zoo</h2><p>Check <a href="https://github.com/opencv/opencv/wiki/Deep-Learning-in-OpenCV">a wiki</a> for a list of tested models.</p>
<p>If OpenCV is built with <a href="https://github.com/opencv/opencv/wiki/Intel%27s-Deep-Learning-Inference-Engine-backend">Intel’s Inference Engine support</a> you can use <a href="https://github.com/opencv/open_model_zoo">Intel’s pre-trained</a> models.</p>
<p>There are different preprocessing parameters such mean subtraction or scale factors for different models.<br>You may check the most popular models and their parameters at <a href="https://github.com/opencv/opencv/blob/master/samples/dnn/models.yml">models.yml</a> configuration file. It might be also used for aliasing samples parameters. In example,</p>
<pre><code class="bash">python object_detection.py opencv_fd --model /path/to/caffemodel --config /path/to/prototxt
</code></pre>
<p>Check <code>-h</code> option to know which values are used by default:</p>
<pre><code class="bash">python object_detection.py opencv_fd -h
</code></pre>
<h3 id="Sample-models"><a href="#Sample-models" class="headerlink" title="Sample models"></a>Sample models</h3><p>You can download sample models using <code>download_models.py</code>. For example, the following command will download network weights for OpenCV Face Detector model and store them in FaceDetector folder:</p>
<pre><code class="bash">python download_models.py --save_dir FaceDetector opencv_fd
</code></pre>
<p>You can use default configuration files adopted for OpenCV from <a href="https://github.com/opencv/opencv_extra/tree/master/testdata/dnn">here</a>.</p>
<p>You also can use the script to download necessary files from your code. Assume you have the following code inside <code>your_script.py</code>:</p>
<pre><code class="python">from download_models import downloadFile

filepath1 = downloadFile(&quot;https://drive.google.com/uc?export=download&amp;id=0B3gersZ2cHIxRm5PMWRoTkdHdHc&quot;, None, filename=&quot;MobileNetSSD_deploy.caffemodel&quot;, save_dir=&quot;save_dir_1&quot;)
filepath2 = downloadFile(&quot;https://drive.google.com/uc?export=download&amp;id=0B3gersZ2cHIxRm5PMWRoTkdHdHc&quot;, &quot;994d30a8afaa9e754d17d2373b2d62a7dfbaaf7a&quot;, filename=&quot;MobileNetSSD_deploy.caffemodel&quot;)
print(filepath1)
print(filepath2)
# Your code
</code></pre>
<p>By running the following commands, you will get <strong>MobileNetSSD_deploy.caffemodel</strong> file:</p>
<pre><code class="bash">export OPENCV_DOWNLOAD_DATA_PATH=download_folder
python your_script.py
</code></pre>
<p><strong>Note</strong> that you can provide a directory using <strong>save_dir</strong> parameter or via <strong>OPENCV_SAVE_DIR</strong> environment variable.</p>
<h4 id="Face-detection"><a href="#Face-detection" class="headerlink" title="Face detection"></a>Face detection</h4><p><a href="https://github.com/opencv/opencv/tree/3.4/samples/dnn/face_detector">An origin model</a><br>with single precision floating point weights has been quantized using <a href="https://www.tensorflow.org/">TensorFlow framework</a>.<br>To achieve the best accuracy run the model on BGR images resized to <code>300x300</code> applying mean subtraction<br>of values <code>(104, 177, 123)</code> for each blue, green and red channels correspondingly.</p>
<p>The following are accuracy metrics obtained using <a href="http://cocodataset.org/#detections-eval">COCO object detection evaluation<br>tool</a> on <a href="http://vis-www.cs.umass.edu/fddb/">FDDB dataset</a><br>(see <a href="https://github.com/opencv/opencv/blob/3.4/modules/dnn/misc/face_detector_accuracy.py">script</a>)<br>applying resize to <code>300x300</code> and keeping an origin images’ sizes.</p>
<pre><code>AP - Average Precision                            | FP32/FP16 | UINT8          | FP32/FP16 | UINT8          |
AR - Average Recall                               | 300x300   | 300x300        | any size  | any size       |
--------------------------------------------------|-----------|----------------|-----------|----------------|
AP @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] | 0.408     | 0.408          | 0.378     | 0.328 (-0.050) |
AP @[ IoU=0.50      | area=   all | maxDets=100 ] | 0.849     | 0.849          | 0.797     | 0.790 (-0.007) |
AP @[ IoU=0.75      | area=   all | maxDets=100 ] | 0.251     | 0.251          | 0.208     | 0.140 (-0.068) |
AP @[ IoU=0.50:0.95 | area= small | maxDets=100 ] | 0.050     | 0.051 (+0.001) | 0.107     | 0.070 (-0.037) |
AP @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] | 0.381     | 0.379 (-0.002) | 0.380     | 0.368 (-0.012) |
AP @[ IoU=0.50:0.95 | area= large | maxDets=100 ] | 0.455     | 0.455          | 0.412     | 0.337 (-0.075) |
AR @[ IoU=0.50:0.95 | area=   all | maxDets=  1 ] | 0.299     | 0.299          | 0.279     | 0.246 (-0.033) |
AR @[ IoU=0.50:0.95 | area=   all | maxDets= 10 ] | 0.482     | 0.482          | 0.476     | 0.436 (-0.040) |
AR @[ IoU=0.50:0.95 | area=   all | maxDets=100 ] | 0.496     | 0.496          | 0.491     | 0.451 (-0.040) |
AR @[ IoU=0.50:0.95 | area= small | maxDets=100 ] | 0.189     | 0.193 (+0.004) | 0.284     | 0.232 (-0.052) |
AR @[ IoU=0.50:0.95 | area=medium | maxDets=100 ] | 0.481     | 0.480 (-0.001) | 0.470     | 0.458 (-0.012) |
AR @[ IoU=0.50:0.95 | area= large | maxDets=100 ] | 0.528     | 0.528          | 0.520     | 0.462 (-0.058) |
</code></pre>
<h2 id="References"><a href="#References" class="headerlink" title="References"></a>References</h2><ul>
<li><a href="https://github.com/opencv/opencv/samples/dnn/download_models.py">Models downloading script</a></li>
<li><a href="https://github.com/opencv/opencv_extra/tree/master/testdata/dnn">Configuration files adopted for OpenCV</a></li>
<li><a href="https://github.com/opencv/opencv/wiki/TensorFlow-Object-Detection-API">How to import models from TensorFlow Object Detection API</a></li>
<li><a href="https://github.com/opencv/opencv/tree/3.4/samples/data/dnn">Names of classes from different datasets</a></li>
</ul>
